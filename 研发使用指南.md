研发使用指南（统一约定与落地操作）

## 生产接入规范（ALB / 健康检查 / 伸缩 / 网络策略）

- ALB Ingress（长期主义）
  - `ingressClassName: chek-prod-alb`
  - 注解（公网、实例模式、证书、健康检查）：
    - `ingress.vke.volcengine.com/address-type: internet`
    - `ingress.vke.volcengine.com/target-type: instance`（Flannel 必须；VPC‑CNI 可切 `ip`）
    - `alb.ingress.kubernetes.io/certificate-ids: <CertID>`（HTTPS 必填）
    - 健康检查对齐：
      - `ingress.vke.volcengine.com/health-check-host: <host>`
      - `ingress.vke.volcengine.com/health-check-path: <healthz-path>`
      - `ingress.vke.volcengine.com/success-codes: 2xx,3xx`
  - Service 要求（实例模式）：
    - `type: NodePort`，端口名统一 `name: http`，`port: 80`，`targetPort` 指向容器端口
    - 节点安全组放通来自 ALB 安全组的 NodePort 范围
  - 证书与监听器：
    - 证书用 CertID 通过注解绑定；监听器与规则一律由 Ingress 注解驱动创建/管理（GitOps），禁止在控制台手工增改。
    - 一套 ALB 复用多域名：DNS CNAME 到同一 ALB FQDN，SNI 选证书

- 监控（Prometheus-agent）
  - 暴露 `/metrics`，使用默认 REGISTRY
  - `ServiceMonitor` 规范：
    - `metadata.labels.volcengine.vmp: "true"`
    - `spec.selector.matchLabels` 对齐 Service 标签；`endpoints[0].port: http`，`path: /metrics`

- 伸缩（KEDA）
  - `ScaledObject` 建议 CPU 利用率触发：
    - `minReplicaCount: 1`，`maxReplicaCount: 5`，`cooldownPeriod: 60`
    - 触发器：`type: cpu`，`metricType: Utilization`，`value: "70"`
  - 仅在集群已安装托管 KEDA/CRDs 时启用

- 网络策略（NetworkPolicy）
  - 后端服务默认收敛入站，仅放行：
    - 网关命名空间（`platform-<env>` 中的 `backend-gateway-saas`）
    - 监控命名空间（`monitoring`）
  - 示例（backend-miker-offline）：
    - `podSelector.matchLabels.app.kubernetes.io/name: backend-miker-offline`
    - 仅放行 TCP:8088（容器端口），来源为上述命名空间/Pod 选择器

- GitOps（Argo CD）
  - Application 开启 `automated.prune/selfHeal`，`syncOptions: [CreateNamespace=true, Replace=true]`
  - 仓库访问统一配好（HTTPS+PAT 或 SSH Deploy Key+known_hosts）
  - Ingress/Service/ServiceMonitor/ScaledObject/NetworkPolicy 全量走 Git，同步落地


一、基本原则（必须遵守）
- 工具与入口
  - 飞连：统一 SSO 入口（GitHub、Argo CD、Grafana、VECR/Harbor、YApi、Nacos 等）。
  - 飞书文档：仅承载需要协同讨论/沉淀的活文档；代码仓内 Markdown 是执行说明的单一真源。
  - 飞书机器人：构建/发布/告警/回滚/审批/高危漏洞自动通知；Webhook/Secret 存于仓库 Secrets。
  - 代码与 CI：GitHub + Actions 为主（构建/扫描/签名/推镜像/触发 GitOps）。
- 命名与目录
  - 命名空间：`platform-<env>`（平台公共） + `<product>-<env>`（业务，如 `miker-dev/staging/prod`）。
  - Helm Release/目录：`<product>-<env>`；values 以环境分目录；infra 仓库为 GitOps 真源。
  - VPC：`chek-<purpose>-vpc-<nn>`（如 `chek-k8s-vpc-01`）。子网：`subnet-<env>-<az>-<cidr-suffix>`；路由表：`rt-<env>-<purpose>`；安全组：`sg-<env>-<purpose>`。
- 镜像与 VECR
  - 仓库：`chek-images-cn-beijing.cr.volces.com/<project>/<image>`；项目建议：平台基础镜像用 `devops/`，业务按产品分项目。
  - 标签：业务统一使用 **固定环境标签 + 显式版本标签**，禁止 `:latest`；如需保留 `sha-*` 仅用于短期追踪，必须配套清理，所有标签都应记录 digest 以便追溯。
  - 拉取：镜像凭据由控制器/策略自动注入，勿在清单手写 `imagePullSecrets`。统一 Secret 名为 `vecr-regcred`。
  - 版本来源与生成（必须遵守）
    - 前端（Node）：版本取自 `package.json.version`。CI 规则：  
      - `dev` 分支：只推 `:dev` 固定标签；  
      - `staging` 分支：只推 `:staging` 固定标签；  
      - 在 `main` 上打 `vX.Y.Z` tag：构建 prod 镜像并推 `:vX.Y.Z` 与别名 `:prod` 两个标签；  
      - 其它分支/未打 tag 的 `main` 不推镜像，避免产生“不可追溯的临时版本”。
    - 后端：Java 取 `pom.xml`/Gradle 版本，Python 取 `pyproject.toml`，Go/其它推荐以 `helm/Chart.yaml.appVersion` 为准；**标签策略必须与前端保持一致**（dev/staging 固定标签，prod 由 `main` 上的 `vX.Y.Z` tag 决定，产出 `:vX.Y.Z + :prod`）。
  - VECR 标签上限（必须遵守）：
    - 每个仓库的 tag 数量有限，CI 禁止为同一服务/环境无限追加新 tag（特别是每次构建都新建 `sha-xxxx`）。  
    - 推荐策略：业务仓 CI 仅推有限集合的稳定标签（dev/staging 固定 `:dev/:staging`，prod 只推 `:vX.Y.Z` 与 `:prod`），由 Helm/infra 仓通过修改 tag/digest 控制发布版本；如确需 `sha-*` 标签，必须配套定期清理任务（Cron + VECR API），避免再次触发 `reach tag limit`。  
    - 一旦出现 `failed to fetch oauth token: unknown: bad request: reach tag limit`，只允许通过“减少标签种类 / 清理旧标签”解决，禁止切换到临时仓库或改用 `:latest` 逃避。
- 容器基线
  - 固定基础镜像版本（如 `alpine:3.20`/`busybox:1.36`）；非 root 运行；只读根文件系统；必须有探针与资源配额。
- VPC 与出网（路由/NAT）
  - NAT：staging+dev 共用，prod 独享；私有子网默认 `0.0.0.0/0 → NAT`；NO_PROXY 包含 `.svc,.svc.cluster.local,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,localhost,127.0.0.1`。
  - 安全组：放通出站 TCP 443；CoreDNS 上游需要 UDP/TCP 53（可收敛到 1.1.1.1/8.8.8.8/114.114.114.114）。
- DNS 原则
  - 统一 CoreDNS（停用 node-local-dns），Upstream 使用 1.1.1.1/8.8.8.8/114.114.114.114；必要时可切 DoH/DoT。
  - 禁止修改节点 containerd 配置（强制）：不得通过 DaemonSet/SSH 改 `/etc/containerd/*`、`sandbox_image` 或镜像加速；避免节点不一致与 DNS 级联故障。
  - 镜像拉取问题处理顺序：先排查安全组/NAT → 核验 CoreDNS →  在 Workload/Helm 层切国内镜像或同步到 VECR；不要动节点 CRI 配置。
- 安全与密钥
  - 机密通过 Secrets/SOPS/External Secrets/IRSA 管理；访问云资源统一 IRSA（OIDC 角色绑定）。

（新增）镜像与准入策略（长期主义）
- 首选引擎：Kyverno（Mutation + Validation 一体化，规则可读性好；Gatekeeper 可作为备选，不再主推）
- 自动注入镜像凭据（Mutation）
  - Kyverno ClusterPolicy：为所有命名空间（排除 `kube-system/kyverno`）自动注入 `imagePullSecrets: vecr-regcred`，避免遗漏。
  - 清单位置：`templates/k8s/policy/kyverno/vecr-imagepullsecret-mutate.yaml`
- 准入校验（Validation，默认 dry-run，灰度后收紧为 Fail）
  - 仅允许仓库前缀（按需扩展白名单）：
    - `vke-cn-beijing.cr.volces.com`
    - `chek-images-cn-beijing.cr.volces.com`
    - `docker.m.daocloud.io`
    - `k8s.m.daocloud.io`
    - `ghcr.m.daocloud.io`
    - 清单：`templates/k8s/policy/kyverno/validate-allowed-registries.yaml`
  - 禁止 `:latest` 标签。
    - 清单：`templates/k8s/policy/kyverno/validate-disallow-latest.yaml`
- 镜像与白名单
  - 列表文件：`templates/image-mirror.list`（按行维护需要同步的公共镜像，固定版本）。
  - 定时同步：`vecr-image-mirror` CronJob 使用 skopeo 将公共镜像同步到 `chek-images-cn-beijing.cr.volces.com/devops/...`。
  - 清单位置：`templates/k8s/cronjobs/vecr-image-mirror.yaml`（含脚本与列表 ConfigMap）。
- 首次补齐基础镜像（以 `eclipse-temurin:17-jre` 为例）
  - 典型症状：在 dev/staging 集群 `ops` 命名空间里跑 `vecr-mirror-*` Job，日志出现：
    - `registry-1.docker.io:443 i/o timeout`（集群出网到 Docker Hub 被防火墙/NAT 拦截）
    - 或 `lookup chek-images-cn-beijing.cr.volces.com: no such host`（CoreDNS 未配置 VECR 上游，Pod 内无法解析 VECR 域名）
  - 结论：这类问题属于 **集群侧 DNS / 出网配置不完整**，不是 skopeo/YAML 本身的错误；要按顺序排查：安全组/NAT → CoreDNS → 再看 Job 与清单。
  - 标准处置：
    - **长期方案**：优先在有完整出网的集群（dev/staging VPC 或专门的 ops 集群）修好 DNS/出口，再以 `vecr-mirror-*` Job 从 `docker.m.daocloud.io` 等白名单源同步到 VECR。
    - **同样纳入治理的应急方案**：在受控环境且有公网出网能力的执行点（例如 GitHub Actions Runner 或运维本机）执行一次性首拉：
      - `docker pull docker.m.daocloud.io/library/eclipse-temurin:17-jre`
      - `docker tag docker.m.daocloud.io/library/eclipse-temurin:17-jre chek-images-cn-beijing.cr-volces.com/devops/library/eclipse-temurin:17-jre`
      - `docker push chek-images-cn-beijing.cr-volces.com/devops/library/eclipse-temurin:17-jre`
      - 用 `docker pull` 或临时 Pod（`kubectl run ... --image=chek-images-cn-beijing.cr-volces.com/devops/library/eclipse-temurin:17-jre`）验证 VECR 拉取成功后，再让 CI/业务统一指向该 VECR 镜像。
  - 原则：**业务 Pod 与集群依然只从 VECR 拉取基础镜像**，不直接访问 Docker Hub；“首拉同步”可以在集群外完成，但必须经由受控账号、固定脚本和审计，最终落点仍是 VECR。
- 节点基线与自检
  - 预拉镜像：`node-prepull` DaemonSet 通过多个 initContainers 预拉常用基础镜像，降低冷启动失败概率。
  - 出网自检：`egress-probe` CronJob 每 10 分钟检查 `DNS(53)` 与 `HTTPS(443)` 到 `open.volcengineapi.com` 的连通性并输出日志。
  - 清单位置：`templates/k8s/daemonsets/node-prepull.yaml`、`templates/k8s/cronjobs/egress-probe.yaml`（默认部署至 `ops` 命名空间）。

落地与运维建议
- 首次启用：先以 `validationFailureAction: audit` 验证无误，逐步在非生产→生产改为 `enforce`；历史命名空间灰度开启。
- 白名单例外：按命名空间/团队维度扩充 Kyverno 中的 `allowedRegistries`，临时需求登记并限时撤销。
- CI 对齐：工作流禁止 `:latest`，构建输出 digest，部署以 digest 为准；静态扫描与 Policy Test 作为必跑检查。

二、命名空间与资源分布规范（新增｜必须遵守）
- 命名空间分层
  - 平台公共：`platform-<env>`（统一承载 `backend-gateway-saas`/SSO/YApi/Nacos/监控等共享组件）。
  - 业务域：`<product>-<env>`（如 `miker-<env>`、`saas-<env>`、`app-<env>`），仅放本产品后端与前端。
- 域名与入口归属
  - 公共域名（跨产品，如 `api.chekkk.com`）只在 `platform-<env>` 定义 Ingress，由平台网关转发到各业务 Service。
  - 产品域名（如 `miker.chekkk.com`、`app.chekkk.com`、`www.chekkk.com`）在各自业务命名空间定义。
  - 每个域名“在全集群仅一处定义”，严禁不同命名空间为同域名声明重叠路径。
- 路由规则形态（ALB）
  - 每个对外域名两条 Ingress：`*-http`（80 仅 301）与 `*-https`（443 承载业务）；`ingressClassName: chek-<env>-alb`。
  - 证书：prod 统一启用 HTTPS；绑定 `alb.ingress.kubernetes.io/certificate-ids: <CertID>` 或由 cert‑manager 自动签发后挂载到 `*-https`。
- Service 统一规范
  - 被 ALB 转发的后端 Service 使用 `type: NodePort`，端口名固定 `http`，`port: 80 → targetPort: <容器端口>`。
  - 仅内部调用的后端使用 `ClusterIP`。
- 标签与可观测
  - 统一使用 `app.kubernetes.io/{name,instance,part-of,component,version,managed-by}`。
  - 监控用 `ServiceMonitor`，加 `metadata.labels.volcengine.vmp: "true"`，`endpoints[0].port: http`。
- 网络策略
  - 默认拒绝入站；仅放行来自 `platform-<env>` 网关与 `monitoring` 命名空间的访问。
- GitOps 与变更
  - Argo CD 必开 `automated+prune+selfHeal` 与 `syncOptions=[CreateNamespace=true,Replace=true]`。
  - 所有新增/修改/删除均通过仓库 PR 完成；下线=删除清单，由 Argo 负责 prune。禁止控制台手改。
- 迁移原则（从旧分布收敛到本规范）
  1) 统一将公共域名入口迁至 `platform-<env>`，收回其它命名空间的同域名 Ingress。
  2) 将散落的 `*-redirect/ssl-redirect` Service 收敛为“80=301、443=业务”的两条 Ingress，删除多余跳转服务。
  3) 校验所有被 ALB 访问的 Service 是否满足 NodePort+端口名 `http`；不满足则修正。
  4) 更新业务命名空间的 `NetworkPolicy` 白名单，仅允平台网关与监控访问。
  5) 合并发布后观察 4xx/5xx 与健康检查，确认无回归后删除旧入口与遗留组件（如未用的 `ingress-nginx` LB）。

第三方 API Key 管理（长期主义｜适用于 backend-miker-offline/osm-gateway 等）
- 明确来源与分层（环境隔离，禁止入库与打包进镜像）
  - 每个环境单独密钥：`dev/staging/prod` 各自管理、互不复用。
  - 存储位置：优先 Secret 管理（K8s Secret/SealedSecret 或 Vault/Secrets Manager）；严禁写入代码仓与镜像层。
- 交付与注入（发布阶段注入为进程环境）
  - 集群侧：以 `Secret` 下发，应用 `Deployment` 通过 `envFrom.secretRef`/`env`+`valueFrom.secretKeyRef` 注入。
  - 自检：服务启动前检查必需环境变量（缺失则告警并退出/降级，打印结构化日志）。
- 轮换与审计（到期提醒与灰度并行）
  - 轮换节奏：为每个 Key 设置到期提醒（平台日历/飞书机器人）；流水线支持“新旧并行”一段时间（新增字段或新增条目），验证成功后再删除旧 Key。
  - 审计与告警：记录第三方调用成功/失败率、配额/429/5xx 指标，接入告警；缺失 Key 或读取失败要有事件日志。

实施（osm-gateway 最小落地）
- Secret 模板（每环境一份，命名空间建议 `platform-<env>`）：
  - 文件位置：`templates/k8s/secrets/secret-osm-gateway-keys.example.yaml`
  - 创建命令（示例，dev）：
    ```bash
    NS=platform-dev
    kubectl get ns "$NS" >/dev/null 2>&1 || kubectl create ns "$NS"
    kubectl -n "$NS" create secret generic osm-gateway-keys \
      --from-literal=PROVIDER_A_KEY='REPLACE_ME' \
      --from-literal=PROVIDER_B_KEY='REPLACE_ME'
    ```
- 部署注入（Helm/K8s 片段）：
  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  spec:
    template:
      spec:
        containers:
        - name: osm-gateway
          envFrom:
          - secretRef:
              name: osm-gateway-keys
          # 如需精确控制：
          # env:
          # - name: PROVIDER_A_KEY
          #   valueFrom:
          #     secretKeyRef:
          #       name: osm-gateway-keys
          #       key: PROVIDER_A_KEY
  ```
- 启动前自检（示例）：
  ```bash
  required=(PROVIDER_A_KEY PROVIDER_B_KEY)
  for k in "${required[@]}"; do
    if [ -z "${!k}" ]; then
      echo "{\"event\":\"missing_secret\",\"key\":\"$k\"}" 1>&2
      exit 1
    fi
  done
  ```
- 轮换策略（示例）：
  - 在 Secret 中新增 `PROVIDER_A_KEY_NEXT`，应用侧兼容读取新旧（优先 `_NEXT`）；运行 24–72 小时观测失败率与配额消耗正常后，移除旧键并将新键更名为正式键。
  - 指标：导出 `third_party_requests_total{provider,<status>}`、`third_party_errors_total{code}`、`third_party_rate_limited_total`，对 `5xx` 和 `429` 提醒；Secret 拉取失败/缺失计数独立告警。

注意
- 如采用 Vault/云 Secrets Manager：通过 External Secrets Operator 下发到 K8s Secret，应用使用方式不变；轮换由上游完成，ESO 轮询同步到集群。
- 如采用 SealedSecret：只入库密文（`SealedSecret` 对象），解密仅在集群控制面；密钥值仍不得明文入库。
- 微服务复用
  - 优先“平台单份部署 + NetworkPolicy 白名单”（命名空间 `platform-<env>`），通过 ClusterIP DNS 复用。

实施（backend-miker-offline｜高德 AMap 最小落地）
- Secret 模板与命名：
  - 文件位置：`templates/k8s/secrets/secret-backend-miker-offline-keys.example.yaml`
  - Secret 名：`backend-miker-offline-keys`，命名空间使用 `miker-<env>`（如 `miker-dev/staging/prod`）。
- 创建命令（示例，dev）：
      ```bash
  NS=miker-dev
  kubectl get ns "$NS" >/dev/null 2>&1 || kubectl create ns "$NS"
  kubectl -n "$NS" create secret generic backend-miker-offline-keys \
    --from-literal=AMAP_KEY='REPLACE_ME' \
    --from-literal=AMAP_SIG_SECRET='REPLACE_IF_USED'
      ```
- 部署注入（K8s 片段）：
  ```yaml
  envFrom:
    - secretRef:
        name: backend-miker-offline-keys
  # 或按键精确注入：
  # env:
  # - name: AMAP_KEY
  #   valueFrom:
  #     secretKeyRef:
  #       name: backend-miker-offline-keys
  #       key: AMAP_KEY
      ```
- 启动前自检（示例）：
      ```bash
  required=(AMAP_KEY)
  for k in "${required[@]}"; do
    if [ -z "${!k}" ]; then
      echo "{\"event\":\"missing_secret\",\"key\":\"$k\"}" 1>&2
      exit 1
    fi
  done
      ```
- 轮换策略（AMap）：
- 在 Secret 中新增 `AMAP_KEY_NEXT`，服务端优先读取 `_NEXT`；运行观察期后删除旧键并重命名。
- 配额与失败率：监控 `429/5xx`、请求耗时，发生异常时降级为缓存/回退方案并告警。
- 发布与回滚
  - “PR → Actions 构建与安全扫描 → 推 VECR → 提交 infra 变更 → Argo CD 发布”；回滚以 GitOps 为准。
  - 新项目落地原则（统一）：优先直接 Helm 部署（GitHub Actions 直发到集群，快速上线）；稳定后再切换为 GitOps（由 CI 修改 infra 仓 PR，Argo CD 同步）。迁移时保留一段并行验证期，最终以 GitOps 为真源。
- CI 门禁对齐
  - 与 `.specify/commands/speckit.specify.md` 保持一致：Lint/TypeCheck/Test/Trivy/CodeQL/Gitleaks 为必过项；失败不发布；产出记录不可变 digest 与审计。

 - 平台账号与治理（Nacos / YApi / Ingress‑NGINX）
  - Nacos（MSE 托管优先）：
    - 选型：prod/staging/dev 优先使用火山引擎 MSE 托管版 Nacos；仅在合规或成本要求下自建。
    - 隔离：Namespace 使用 `\<product>-\<env>`（如 `miker-prod/staging/dev`）；业务用 Group（如 `miker`/`common`）；DataId 以 “应用-场景.yaml” 命名并与代码同源管理。
    - 访问与安全：仅开放 VPC；开启登录鉴权；账号分级 `nacos_rd`（只读）/`nacos_op`（发布）/`nacos_admin`（平台）；最小权限；以 `nacos-cred` Secret 下发 `address/username/password`，禁止明文。
    - 变更流程：配置必须走 Git→CI 校验（lint/模板）→审批→自动发布到 MSE；禁止控制台直接改配置。
    - 客户端：`spring.cloud.nacos.*` 指向 MSE 内网域名；心跳 5–10s；实例保护阈值按流量评估慎用；超时/重试采用官方默认或经压测调优。
    - 监控与告警：启用 MSE 自带监控和告警；关键事件（发布/回滚/异常）接入飞书机器人。
    - 备份与数据：托管优先使用 MSE 内置 MySQL；如自建，外置 RDS 并开启每日备份；重大发布前手动快照。
    - 自建 fallback（如必须）：生产 3 副本 + 外部 MySQL；非生产 1 副本或 3 小副本；镜像固定 `nacos/nacos-server:2.3.2` 并镜像到 VECR。
    - 平台非生产统一实例（dev/staging 共用）：不新装新环境，按命名隔离“开分区”，缺什么增什么。
      - 命名：`<product>-<env>`（例：`miker-dev`）。
      - 操作：在平台 MSE/Nacos 上“新建 Namespace=<product>-<env>`”，业务侧仅切换 `NACOS_NAMESPACE=<product>-<env>`；地址/账号不变。
  - YApi（组织级规范 + 复用工作流）：
    - 项目命名：`<product>-<env>`（例：`miker-dev`、`miker-staging`、`miker-prod`），接口按域/微服务分组；Token 仅用于 CI 同步。
    - 账号：`yapi_admin`（平台）、`yapi_owner_<product>`（产品负责人）、成员按最小权限加入；Token 存于仓库或组织 Secrets。
    - 分组与成员（统一四个分组，仅此四个）：`Miker`、`app`、`SaaS`、`公用微服务`；飞连 SSO 登录后，用户自动加入上述四个分组（无需手动加组）。
    - 平台非生产统一实例（dev/staging 共用）：不额外新装环境，通过项目“开分区”区分环境。
      - 项目命名仍用 `<product>-<env>`（例：`miker-dev`），在对应产品分组（如 `Miker`/`公用微服务`）中创建项目并配置 `YAPI_TOKEN_DEV|STG|PROD`。
      - CI 构建后统一复用组织级工作流导入 OpenAPI，不在业务仓手写 curl/HTTP 逻辑。
      - 实战（prod，域名 yapi.chekkk.com）
      - Ingress 与 ALB（GitOps，禁止控制台手改）：
        - `Ingress/yapi-https`：`ingressClassName: chek-prod-alb`，TLS 绑定 `yapi-chekkk-com-tls`，路由：
          - `/feishu-sso/ → Service yapi:8080`（SSO 服务）
          - `/ → Service yapi:80`（YApi）
        - `Ingress/yapi`（80 → 仅用于 HTTP 重定向）：路由 `/ → Service yapi-http-redirect:80`，该服务仅返回 `301 Location: https://$host$request_uri`。
        - Service（实例模式）：`yapi` 为 `NodePort`，端口 `80` 与 `8080`；ALB 控制器自动创建目标组（`target-type=instance`）。
      - 证书与 ACME：
        - `ClusterIssuer/letsencrypt-http` 的 `http01.ingress.class` 统一为 `chek-prod-alb`，由 ALB 控制器下发临时 Solver Ingress。
        - 若公共 DNS 不可解析某内部域，需在 CoreDNS 增加条件转发，保证 HTTP‑01 自检 200。
      - 401/403 的含义（避免误判）：
        - `401 禁止访问资源`：应用鉴权返回（未登录/无权限），与 ALB 无关；访问 `/feishu-sso/auth?...` 完成登录即可。
        - `403`（HTTPS 初期）：因未完成重定向与 TLS 绑定导致；完成上文 HTTPS 与 80→443 后解决。
      - 长期做法：
        - 只用 Ingress 注解驱动 ALB 监听/规则/目标组，不在控制台维护转发规则。
        - 80 强制跳转到 443；SSO 与主站分路径实现，无需二级 ALB。
        - 命名与可观测：Listener/TargetGroup 在文档中用别名 `frontend-app-prod-http-80 / tg-yapi-prod-<nodePort>` 描述，避免依赖控制台生成的哈希名。
      - 验证手册（外部）：
        - `curl -I https://yapi.chekkk.com` 应 `200`；
        - `curl -I 'https://yapi.chekkk.com/feishu-sso/auth?ret=https%3A%2F%2Fyapi.chekkk.com%2Fapi%2Fuser%2Flogin_by_token'` 应 `302` 到认证中心；
        - `curl -I http://yapi.chekkk.com` 应 `301 Location: https://yapi.chekkk.com/`。
  - 入口（ALB 优先）：
    - 域名：`\<product>-dev|staging.chekkk.com`、`\<product>.chekkk.com`；TLS Secret `<product>-tls`。
    - TLS Secret 命名：推荐使用 `<host>-tls`（与证书域名一一对应），亦可使用 `<product>-tls`；全篇示例优先采用 `<host>-tls`。
    - Ingress 类：各环境使用专用类 `ingressClassName: chek-<env>-alb`；在 Helm values 显式设置 `ingress.className=chek-<env>-alb`（Chart 已支持参数化）。
    - 路由：优先前缀匹配（Prefix）。若历史使用 NGINX 的正则/重写（`use-regex`/`rewrite-target`），改为后端去前缀或在网关层实现；ALB 不支持 NGINX 专有注解。
    - 金丝雀：与 Rollouts 配合，通过两套 Service 权重切换。
    - TLS 策略：仅生产环境启用（由 cert-manager/ClusterIssuer 自动签发）；dev/staging 默认 HTTP，如需临时启用在项目内覆盖 values。

  ALB 原则与解析（长期主义）
  - 共享 EIP 是正常且推荐的：同一环境多个域名（如 `app/app-staging/www`）可以指向同一外网 ALB/EIP，靠 SNI 与 Host 路由隔离；无需为每域名单独一个 EIP。
  - DNS 建议使用 CNAME：域名解析优先 CNAME 到 ALB 域名，避免直接 A 记录到 EIP（ALB IP 可能更换）。
  - 环境隔离：每个环境（dev/staging/prod）各一套公网 ALB；后端微服务仅内网访问（ClusterIP/内网 ALB），对外统一经网关域名暴露。
  - 80/443 总则：生产环境 80 仅 301 重定向到 443，业务路径只在 443；非生产默认仅开 80，如需 TLS 可在项目内覆盖开启 443，并沿用“80=301 / 443=业务”的分工；严禁在两个 Ingress 中为同域名定义相同路径。
  - Service 端口规范（强制）：端口名必须为 `http`；Ingress backend 推荐使用 `service.port.number: 80`（Admission 兼容最佳）。说明：Service 仍需 `name: http`；Ingress backend 写 `port.number: 80`，两者不冲突。
  - 最小注解集（按环境拆分）：
    - dev/staging（仅 80）：
      ```yaml
      metadata:
        annotations:
          ingress.vke.volcengine.com/address-type: internet
          ingress.vke.volcengine.com/target-type: instance
          ingress.vke.volcengine.com/backend-protocol: HTTP
          alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
      ```
    - prod（80=301，443=业务）：
      ```yaml
      # 80：仅 301
      metadata:
        annotations:
          ingress.vke.volcengine.com/address-type: internet
          ingress.vke.volcengine.com/target-type: instance
          ingress.vke.volcengine.com/backend-protocol: HTTP
          alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
      ---
      # 443：承载业务
      metadata:
        annotations:
          ingress.vke.volcengine.com/address-type: internet
          ingress.vke.volcengine.com/target-type: instance
          ingress.vke.volcengine.com/backend-protocol: HTTP
          alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
          alb.ingress.kubernetes.io/ssl-redirect: "443"
          alb.ingress.kubernetes.io/certificate-ids: <CertID>
      ```
  - CertID 说明：
    - CertID 是云证书的唯一标识，用于 ALB 绑定 HTTPS 证书；与本地 `tools/证书` 中的证书文件不同，后者不包含 CertID。
    - 获取方式：火山引擎 控制台 → 证书中心 → 证书详情 → 复制 CertID（如 `cert-xxxxxxxx`）。
    - 配置位置：优先在 Ingress 注解 `alb.ingress.kubernetes.io/certificate-ids`；也可在对应 TLS Secret 上加入平台要求的 CertID 注解（若控制器支持）。
  - 验证与回滚：
    - 创建/更新后等待 Ingress `.status.loadBalancer` 回填 ALB 地址；未回填前视为未生效。
    - 如证书不可用或配额不足，可临时降级为 HTTP（仅监听 80），待证书或配额就绪后再恢复 HTTPS。
  - 观测与伸缩：入口流量与 4xx/5xx 指标纳入 Prometheus/VMP；入口只做第 7 层路由与 TLS 终止，流量治理（鉴权/限流）由网关/后端完成。

### ALB 接管与规则管理（防止手工规则被覆盖｜强制）
- 单一真源：一旦某台 ALB 被 Ingress 控制器接管，该 ALB 的监听器“转发规则”应只由 Ingress 定义。
- 禁止混管：同一台 ALB 不允许“部分服务走 Ingress、部分手工维护规则”的混合模式。要么全部 GitOps，要么全部手工（不推荐）。推荐“全部 GitOps”。
- 环境与实例的强约束（新增，必须遵守）：
  - 环境隔离：dev / staging / prod 各自使用独立的公网 ALB 与独立的 IngressClass（例如：`chek-dev-alb`、`chek-staging-alb`、`chek-prod-alb`）。服务部署在哪个环境，就必须在该环境创建对应的 Ingress 并绑定到该环境的 IngressClass/ALB，禁止跨环境复用同一台 ALB。
  - 监听器与目标组的映射：同一环境内，若同一后端同时对外提供 HTTP:80 与 HTTPS:443，VKE ALB Ingress Controller 会分别在 80/443 监听器下各创建一条规则与一组 ServerGroup（目标组）。这属于预期与受控行为，请勿尝试在控制台“合并/复用”目标组；手工更改会被控制器回收。
  - （统一约定）所有环境固定使用独立类名：`chek-<env>-alb`；不再使用通用 `alb`。
- 上线步骤（每个对外域名/服务必须执行）：
  1) Service 统一为 `type: NodePort`，端口名 `http`，`port: 80`，`targetPort` 指向容器端口；放通“ALB 安全组 → 工作节点安全组”的 NodePort 段。
  2) Ingress 使用 `ingressClassName: chek-<env>-alb`，并用最小注解集：
     - `ingress.vke.volcengine.com/address-type: internet`
     - `ingress.vke.volcengine.com/target-type: instance`
     - `ingress.vke.volcengine.com/backend-protocol: HTTP`
     - `alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80},{"HTTPS":443}]'`
     - `alb.ingress.kubernetes.io/ssl-redirect: "443"`
     - `alb.ingress.kubernetes.io/certificate-ids: <CertID>`
     - 可选：`ingress.vke.volcengine.com/alb-instance-id: <ALB_ID>` 显式绑定环境 ALB
  3) 证书：使用证书中心的 CertID 绑定到 Ingress 注解。80 仅用于 301，443 为主通道。
  4) GitOps：所有 Ingress/Service/证书配置入仓，由 Argo CD 自动同步；禁止在控制台编辑监听/规则。
- 迁移既有“手工规则”至 GitOps（零中断建议）：
  1) 盘点当前 ALB 上的域名与后端服务映射（Host/Path → 目标组）。
  2) 在仓库补齐对应 Ingress（Host/Path 完全一致），指向正确 Service（NodePort）；先合并但不删除手工规则。
  3) 等控制器生成 `k8s_*` 目标组并命中后，再删除同名的手工 `tg-*` 与手工转发规则。
  4) 验证 HTTP→HTTPS(301) 与 HTTPS(200)，健康检查成功后关闭应急手工规则。
- 推荐落地形态（避免冲突｜HTTP 仅做 301 到 HTTPS）
  - 目标：在 80/443 下分别生成独立 TargetGroup，同时规避“同域名同路径在两份 Ingress 中”的校验冲突。
  - 原则：
    - HTTP:80 仅承担重定向，不声明任何业务路径（如 `/osm-gateway`、`/api/*`）；仅声明根路径 `/`，指向一个 `http-redirect` 服务（返回 `301 Location: https://$host$request_uri`）。
    - HTTPS:443 承载全部业务路径；不在 443 Ingress 中声明 `/` 根路径（由上游或网关页面决定）。
  - 清单示例：
    ```yaml
    # Ingress（HTTP，仅 80，用于 301 跳转）
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: api-gateway-http
      namespace: platform-prod
      annotations:
        ingress.vke.volcengine.com/address-type: internet
        ingress.vke.volcengine.com/target-type: instance
        ingress.vke.volcengine.com/backend-protocol: HTTP
        alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
        ingress.vke.volcengine.com/alb-instance-id: <ALB_ID>
    spec:
      ingressClassName: chek-prod-alb
      rules:
      - host: api.chekkk.com
        http:
          paths:
          - path: /
            pathType: Exact
            backend:
              service:
                name: http-redirect
                port:
                  number: 80
    ---
    # Ingress（HTTPS，仅 443，承载全部业务路径）
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: api-gateway-https
      namespace: platform-prod
      annotations:
        ingress.vke.volcengine.com/address-type: internet
        ingress.vke.volcengine.com/target-type: instance
        ingress.vke.volcengine.com/backend-protocol: HTTP
        alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
        alb.ingress.kubernetes.io/ssl-redirect: "443"
        alb.ingress.kubernetes.io/certificate-ids: <CertID>
        ingress.vke.volcengine.com/alb-instance-id: <ALB_ID>
    spec:
      ingressClassName: chek-prod-alb
      rules:
      - host: api.chekkk.com
        http:
          paths:
          - path: /osm-gateway
            pathType: Prefix
            backend:
              service:
                name: osm-gateway
                port:
                  number: 80
          - path: /api/osmgw/
            pathType: Prefix
            backend:
              service:
                name: backend-gateway-saas
                port:
                  number: 80
          # 其余业务路径...
    ```
  - 验证（必须）：
    - 外部：`curl -I http://api.chekkk.com/osm-gateway/healthz` 应返回 `301`，跳转到 `https://...`；`curl -I https://api.chekkk.com/osm-gateway/healthz` 应返回 `200`。
    - OpenAPI：使用 `DescribeListeners` 预签名 URL 校验
      - 80 的 Listener 下仅挂载“redirect”对应的 TargetGroup；
      - 443 的 Listener 下仅挂载业务 TargetGroup（如 `k8s_<hash>_osm-gateway_<nodePort>` 等）。
    - 落地示例（backend-app）
      - 版本与标签（backend-app 示例）：
        - `dev` 分支合并：CI 仅推 `chek-images-cn-beijing.cr.volces.com/dev/backend-app:dev`，部署到 `app-dev` 命名空间；  
        - `staging` 分支合并：CI 仅推 `.../staging/backend-app:staging`，部署到 `app-staging`；  
        - 在 `main` 上打 `vX.Y.Z` tag 并 push：CI 为 prod 构建镜像并推 `.../prod/backend-app:vX.Y.Z` 与别名 `:prod`，部署到 `app-prod`。  
      - CI/CD 入口（通用约定，包括 `backend-auth-saas`）：  
        - CI 仅在 `main / dev / staging` 分支和 `v*` tag 上触发构建与推镜像；其它临时分支只允许做编译自测，不跑正式 CI。  
        - CD 仅在 `dev / staging` 分支和 `v*` tag 上触发 Helm 部署（dev→`*-dev`、staging→`*-staging`、tag→prod）。  
        - OpenAPI / YApi 同步遵循“单工作流多 Job”模式：  
          - Job1：在仓库内导出 OpenAPI（如 `scripts/export_openapi.sh` 生成 `build/swagger/swagger.json`）；  
          - Job2：`needs` Job1，复用组织级 `yapi-sync-reusable.yml` 或 `openapi-file-sync.yml` 导入 YApi，按 `main/dev/staging` + `v*` 自动选择 `YAPI_TOKEN_DEV/STG/PROD`。  
        - `backend-auth-saas` 专用：  
          - CI：`.github/workflows/ci.yml`，只在 `main / dev / staging` 分支与 `v*` tag 触发；  
          - CD：`.github/workflows/cd.yml`，只在 `dev / staging` 与 `v*` tag 触发，分别部署到 `saas-dev / saas-staging / saas-prod`；  
          - YApi：`.github/workflows/yapi-sync.yml`，在上述分支/tag 上运行，先用 `scripts/export_openapi.sh` 本地导出，再统一导入 YApi，并默认开启 LLM 富化（可按项目需要在组织级模板中调整）。  
          - 分支保护 Required checks：`main / dev / staging` 只允许勾选 `ci` / `cd` / `yapi-sync` 三条自动流水线；`.github/workflows/ops-db-query.yml`、`deploy.yml`、`nacos-enable.yml`、`feishu-notify.yml`、`ci-security.yml` 等运维/工具型工作流一律保留为 `workflow_dispatch` 手动使用，不得配置为 Required check，避免每次 push 产生大量 0s 的配置校验失败。  
      - 网关与 ALB：
        - 对外统一通过 `backend-gateway-saas` 暴露 `/api/backend-app/**` 路由与 `/api/backend-app/openapi.json`，Ingress 全量由 `ops-bootstrap` 管理，禁止在控制台手改规则；  
        - dev/staging 环境已在 `app-dev.chekkk.com` / `app-staging.chekkk.com` 上验证 `openapi.json` 200，prod 侧需要统一以此路径为准调整网关路由与后端健康检查。
  - 注意：
    - 不要在两个 Ingress 中重复声明同一路径（例如 `/openapi` 同时出现在 80 与 443），否则 Admission 会报“路径冲突”并拒绝。
- 应急策略（仅短期）：
  - 如必须临时恢复外网访问，可在控制台把 80/443 的规则指向当前在用的 `k8s_*` 目标组；但务必在 24–48 小时内完成 Ingress 化并删掉手工规则，避免与控制器竞争。
- 常见误区：
  - 只新增“转发规则”但未放通 443 安全组/未创建 443 监听器，HTTPS 仍会超时。
  - 更新已有 Ingress 的 `loadbalancer-protocol` 等只读字段会被 Admission 拒绝；需新建一个 Ingress 资源承载 HTTPS，再逐步切换。
  - Ingress 后端端口写 `name` 在 Admission 校验失败时可能被拒；统一写 `number: 80` 最稳。
  - 目标组命名 `k8s_<hash>` 属正常，由控制器生成；手工创建的 `tg-...` 不会被控制器使用。

### 本机直连 ALB OpenAPI 常见问题与解法（签名/SSL）
- 常见问题
  - SignatureDoesNotMatch（签名不匹配）：
    - 参数名错误：必须用 `Region`，不是 `RegionId`。
    - 错误的 endpoint 或 service：ALB 用 `endpoint=alb.<region>.volcengineapi.com`，`service=alb`。
    - 参与签名的 Header 不一致：GET 不要设置 `Content-Type`；仅保留 `Host/Accept`。
    - Body 非空：GET 必须空体（统一用预签名 URL 规避）。
    - 时间漂移：本机时钟与 UTC 偏差过大（>5 分钟）会导致校验失败。
  - SSL/EOF/代理：
    - 本地 Python OpenSSL 版本或公司代理导致 `SSLEOFError/handshake EOF`。
    - 系统代理劫持：请对 `*.volcengineapi.com` 关闭代理（NO_PROXY）。
- 标准解法（推荐优先）
  - 统一使用 SDK 的预签名 URL（`SignerV4.sign_url`）+ `curl`：
    - 仅 GET，空 Body；
    - Query 中携带 `Action/Version/Region/LoadBalancerId` 与 `X-*` 签名字段；
    - Header 仅保留 `Host/Accept`；
    - 规避本地 SSL/头部差异导致的签名失败。
  - 校时与网络：
    - 确认本机 UTC 时间正确（或直接在 CI/Runner 上运行脚本）；
    - 放通 443 出站到 `alb.<region>.volcengineapi.com`；为 curl 关闭公司代理或加入 NO_PROXY。

### 验证作业模板（预签名 + curl｜本地/任意自动化）
- 目的：本地直接用“预签名 + curl”拉取 ALB 的 80/443 监听器规则，校验是否符合“HTTP 仅 301，业务路径集中在 HTTPS:443”的原则；避免依赖 GitHub Actions。
- 准备：需安装 python3、curl、jq。使用最小权限只读 AK/SK（仅 `alb:Describe*`）。
- 使用（将 AK/SK/ALB_ID 按需替换；不会回显密钥到日志）：
```bash
export REGION=cn-beijing
export ALB_ID=alb-xxxxxxxxxxxxxxxxxxxxx
export AK='你的只读AK'
export SK='你的只读SK'

python3 - <<'PY' > /tmp/alb_rules.tsv
import os, hmac, hashlib, urllib.parse, subprocess, json
from datetime import datetime

AK=os.environ['AK']; SK=os.environ['SK']; REGION=os.environ.get('REGION','cn-beijing')
ALB_ID=os.environ['ALB_ID']; EP=f'alb.{REGION}.volcengineapi.com'; SERVICE='alb'

def h(key,msg):
  if isinstance(msg,str): msg=msg.encode()
  return hmac.new(key,msg,hashlib.sha256).digest()

def sign_req(action,version,params):
  # 与 scripts/alb_clean_http_rules_presign.py 保持一致：使用 X-Date，SignedHeaders=host;x-date
  t=datetime.utcnow(); amz=t.strftime('%Y%m%dT%H%M%SZ'); dat=t.strftime('%Y%m%d')
  query={'Action':action,'Version':version}; query.update(params or {})
  qs='&'.join(['{}={}'.format(urllib.parse.quote_plus(k),urllib.parse.quote_plus(str(v))) for k,v in sorted(query.items())])
  ch='host:{}\n'.format(EP)+'x-date:{}\n'.format(amz)
  canonical='\n'.join(['GET','/',qs,ch,'host;x-date',hashlib.sha256(b'').hexdigest()])
  scope='{}/{}/{}/request'.format(dat,REGION,SERVICE)
  sts='\n'.join(['HMAC-SHA256',amz,scope,hashlib.sha256(canonical.encode()).hexdigest()])
  kDate=h(('VOLC'+SK).encode(),dat); kRegion=h(kDate,REGION); kService=h(kRegion,SERVICE); kSign=h(kService,'request')
  sig=hmac.new(kSign,sts.encode(),hashlib.sha256).hexdigest()
  auth='HMAC-SHA256 Credential={}/{}, SignedHeaders=host;x-date, Signature={}'.format(AK,scope,sig)
  url='https://{}/?{}'.format(EP,qs)
  headers={'Authorization':auth,'Host':EP,'X-Date':amz}
  return url, headers

def curl_json(req):
  url, headers = req
  cmd=['curl','-sS',url]
  for k,v in headers.items():
    cmd.extend(['-H',f'{k}: {v}'])
  out=subprocess.check_output(cmd)
  try: return json.loads(out.decode())
  except: return {}

def extract_listeners(doc):
  for path in (('Result','Listeners'),('Listeners',),('ListenerSet',)):
    cur=doc; ok=True
    for k in path:
      if isinstance(cur,dict) and k in cur: cur=cur[k]
      else: ok=False; break
    if ok and isinstance(cur,list): return cur
  return []

def find_rules_anywhere(d, acc=None):
  if acc is None: acc=[]
  if isinstance(d,dict):
    for k,v in d.items():
      if k=='Rules' and isinstance(v,list): acc.extend(v)
      else: find_rules_anywhere(v,acc)
  elif isinstance(d,list):
    for it in d: find_rules_anywhere(it,acc)
  return acc

def pick(vals):
  for v in vals:
    if v: return v
  return ''

# 1) 拿到所有监听器
ls = []
for ver in ('2020-04-01','2020-11-26','2023-01-01'):
  d=curl_json(sign_url('DescribeListeners',ver,{'LoadBalancerId':ALB_ID}))
  ls=extract_listeners(d)
  if ls: break

rows=[]
for l in ls:
  proto=str(l.get('Protocol','')).upper(); port=int(l.get('Port') or 0)
  if proto not in ('HTTP','HTTPS') or port not in (80,443): continue
  lid=l.get('ListenerId') or l.get('Id') or ''
  if not lid: continue
  # 2) 拉规则（优先 DescribeRules，再回退 DescribeListenerAttributes）
  rules=[]
  for ver in ('2020-04-01','2020-11-26','2023-01-01'):
    rules=find_rules_anywhere(curl_json(sign_url('DescribeRules',ver,{'ListenerId':lid})))
    if rules: break
  if not rules:
    for ver in ('2020-04-01','2020-11-26','2023-01-01'):
      rules=find_rules_anywhere(curl_json(sign_url('DescribeListenerAttributes',ver,{'ListenerId':lid})))
      if rules: break
  for r in rules:
    host=pick([r.get('Domain'),r.get('Host'),r.get('HostHeader')]); path=pick([r.get('Path'),r.get('Url'),r.get('PathPattern')]) or '/'
    conds=r.get('RuleConditions') or []
    if not host:
      for c in conds:
        fld=str(c.get('Field','')).lower()
        if fld in ('host','host-header','hostheader'):
          vals=(c.get('HostHeaderConfig') or {}).get('Values') or []
          if vals: host=vals[0]; break
    if not path:
      for c in conds:
        fld=str(c.get('Field','')).lower()
        if fld in ('path','path-pattern'):
          cfg=c.get('PathConfig') or c.get('PathPatternConfig') or {}
          vals=cfg.get('Values') or []
          if vals: path=vals[0]; break
    sg=pick([r.get('ServerGroupId'),r.get('BackendServerGroupId')])
    rows.append({'port':port,'host':host or '-', 'path':path, 'sg':sg or ''})

print('port\thost\tpath\tserver_group_id')
for it in rows:
  print(f\"{it['port']}\\t{it['host']}\\t{it['path']}\\t{it['sg']}\")
PY

# 结果文件：/tmp/alb_rules.tsv
column -t /tmp/alb_rules.tsv | sed -n '1,50p'
```
- 审查要点
  - Secrets 使用最小权限只读 AK/SK（仅 `alb:Describe*`），不将预签名 URL 输出到日志。
  - 同时拉取 80 与 443 的规则，直观看端口与 ServerGroup 的映射；80 应“仅有默认/跳转”规则，不承载业务路径；443 承载业务路径（如 `/`、`/feishu-sso/` 等）。
  - 通过 `server_group_id` 对比：若同一 `ServerGroupId` 同时出现在 80/443，需下线改为“HTTP 仅 301，路径集中在 443”，或拆分不同 Service/NodePort 物理隔离。

#### 不再推荐/已移除的做法
- 本地手写 Authorization 头 + curl 直调：极易因 Header/时钟/Query 差异触发 `SignatureDoesNotMatch`。
- 通过 `CreateCommand`/云助手在云端执行脚本再调用 ALB：服务域错配、权限边界不清晰，已废弃。
- GET 请求设置 `Content-Type` 或附带 body：会参与签名导致验签失败。
- 依赖本机代理访问 `*.volcengineapi.com`：容易触发 SSL EOF；统一在 CI 用预签名 URL + curl。

  - 已知问题与规避建议（Flannel + ALB/instance）
    - 统一入口：对外统一经“网关服务（NodePort）”暴露路径；后台微服务仅内网访问（ClusterIP/或由网关内路由），避免在 Ingress 中为特定后端配置“直连精确路径（Exact）”导致控制器生成多组目标组而行为不一致。
    - 路由粒度：优先使用前缀匹配（Prefix）将请求打到网关，再由网关进行 `StripPrefix/Rewrite/SetPath`；避免在 Ingress 上实现复杂重写。
    - 健康检查与超时：对齐 `host/path/success-codes`，并将空闲超时设为 ≥120s；若出现“HEAD 200 但 GET 超时”之类回传异常，先确认统一经网关的入口策略是否生效，再核对节点安全组是否放通 ALB→NodePort。
    - 目标组与安全组：Flannel 场景固定 `target-type: instance`；服务必须 `type: NodePort` 且端口名为 `http`；务必放通“ALB 安全组 → 节点安全组”的 NodePort 段（默认为 30000–32767/TCP，建议按实际收敛）。
    - 变更策略：优先通过 GitOps（修改 Ingress 与网关配置）触发 ALB 控制器自动对齐，避免在控制台手动编辑监听/目标组；必要时仅用于临时排障，完成后回滚到 Git 定义。

  - 监听/目标组命名规范（新增）
    - Listener：`<svc>-<env>-http-80`、`<svc>-<env>-https-443`
    - TargetGroup（Instance 模式/Flannel）：`tg-<svc>-<env>-<nodePort>`（例：`tg-backend-gateway-saas-prod-31263`）
    - 规则描述：`host=<domain> path=<prefix> -> tg-...`
    - 由 ALB Ingress Controller 自动创建时，名称可能为 `k8s_<hash>`；无需手工改名，但在文档/评审中以上述规范作为别名引用。
    - 安全组放通：务必保证“ALB 安全组 → 工作节点安全组”允许 NodePort 段 `30000-32767/TCP`（或最小化到实际使用端口），否则目标组不健康将导致请求回落到默认后端（常见现象为外网 404/NGINX 页）。

  <!-- VKE 托管 ALB（target-type=ip）相关内容暂不适用当前 Flannel 基线，已移除以避免混淆 -->

  Flannel 现状下的统一管理：NodePort + ALB target‑type=instance
  - 适用场景：集群为 Flannel，暂不重建为 VPC‑CNI，但仍需使用托管 ALB 统一入口。
  - 统一暴露模型：
    - 内部服务：`Service=ClusterIP`（仅集群内访问）。
    - 对外服务：`Service=NodePort` + `Ingress(alb)`，ALB 注解 `target-type: instance`。
  - Service 规范（对外）：
    - 端口命名：固定 `name: http`，对外端口 `port: 80`，容器端口 `targetPort: <容器端口>`。
    - `type: NodePort`（`nodePort` 可固定或交由 k8s 分配）。
    - 节点安全组放通统一 NodePort 范围（建议收敛如 30080–30199），ALB 安全组放通 80/443 指向节点安全组；禁止直接对公网放通 NodePort。
  - Ingress 规范（对外）：
    - `spec.ingressClassName: chek-<env>-alb`。
    - 注解最小集：
      - `ingress.vke.volcengine.com/target-type: instance`
      - `ingress.vke.volcengine.com/address-type: internet`
      - `ingress.vke.volcengine.com/backend-protocol: HTTP`
      - 可选：`ingress.vke.volcengine.com/alb-instance-id: <ALB_ID>`（显式绑定环境 ALB）。
    - 后端端口使用数值 `number: 80`（与 Admission 兼容最佳）。
  - 示例（对外服务）：
    ```yaml
    # Service（NodePort）
    apiVersion: v1
    kind: Service
    metadata:
      name: frontend-app
    spec:
      type: NodePort
      selector:
        app.kubernetes.io/name: frontend-app
      ports:
        - name: http
          port: 80
          targetPort: 3000
          nodePort: 30080  # 可省略交由集群分配

    ---
    # Ingress（ALB instance 模式）
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: frontend-app
      annotations:
        ingress.vke.volcengine.com/target-type: instance
        ingress.vke.volcengine.com/address-type: internet
        ingress.vke.volcengine.com/backend-protocol: HTTP
        # 可选绑定指定 ALB：
        # ingress.vke.volcengine.com/alb-instance-id: alb-xxxxxxxxxxxxxxxxxxxxx
    spec:
      ingressClassName: chek-<env>-alb
      rules:
        - host: app-dev.chekkk.com
          http:
            paths:
              - path: /
                pathType: Prefix
                backend:
                  service:
                    name: frontend-app
                    port:
                      number: 80
    ```
  - 策略与校验（建议）：
    - Kyverno/Gatekeeper：
      - 对外服务必须 `Service.name=http / port=80`，`Ingress.class=alb` 且注解齐全；禁止 `hostPath/privileged`；必须 `resources.requests/limits`。
    - CI：Chart Lint + Policy Test，PR 必须通过再合并。
  - 迁移到 VPC‑CNI 的路径：
    - 新集群（VPC‑CNI）就绪后，把 Service 改回 `ClusterIP`、Ingress 注解改 `target-type: ip`，其余不变；DNS CNAME 切到新 ALB，完成无损迁移。

  （新增）环境域名与 Nacos 解耦（如何区分 dev/staging/prod）
  - 职责分离：
    - 对外域名/入口 由 DNS + Ingress/ALB 决定；
    - 服务发现/配置 由 Nacos 决定；二者相互独立、解耦。
  - DNS 规范：
    - prod：`api.chekkk.com` CNAME → prod 环境 ALB 域名；
    - 非生产：`api-dev.chekkk.com`、`api-staging.chekkk.com` 各自 CNAME → 对应环境 ALB。
  - Ingress/ALB（各环境各一份）：
    - `spec.rules.host` 使用各自环境域名；
    - 后端统一接入网关（如 `backend-gateway-saas`），网关内路由到 `lb://<service>`。
  - Nacos 隔离：
    - 三个 namespace：`<product>-dev` / `<product>-staging` / `<product>-prod`；
    - 网关与后端在各自 `NACOS_NAMESPACE` 下注册/发现，同名服务跨环境互不可见；必要时再用 `group` 做业务域隔离。
  - YApi：
    - 同一实例多项目隔离：每环境一个项目 `<product>-<env>`；
    - CI 用不同 `YAPI_TOKEN_<ENV>` 同步到对应项目，运行时不依赖 YApi。

二、装好一个环境需要注意的事项（以 miker-prod 为例）
- 现状/资源
  - 目标：按环境隔离（dev / staging / prod），产品按命名空间隔离，最小权限与配额控制。
  - 集群/VPC：
    - dev：既有 VKE（VPC `chek-k8s-vpc-01`）。
    - staging：既有 VKE（VPC `default`）。
    - prod：`chek-prod-k8s`（cd3dvl7dfro4clvq3ktt0；VPC `chek-k8s-vpc-01`；子网 172.31.120.0/24、172.31.121.0/24；安全组 `sg-iiyj5u0it0jk74o8cui4yuzf`）。
  - 命名空间命名：`<product>-<env>`（例：`miker-dev/staging/prod`）。
- 资源配额：每命名空间配置 ResourceQuota/LimitRange、NetworkPolicy、IRSA（最小权限）。
- VPC 出口与 DNS（本次落地要点）
  - NAT：为 prod 新建 NAT 和 SNAT，独立 EIP；覆盖 172.31.120.0/24、172.31.121.0/24。
  - 安全组：放通 443；CoreDNS 上游需要 UDP/TCP 53；必要时收敛到三大公共 DNS。
  - DNS：卸载 node-local-dns；用 VECR 镜像重装 CoreDNS，Upstream 1.1.1.1/8.8.8.8/114.114.114.114；以 busybox `nslookup/curl` 验证。
- 入口与证书
  - 【过渡/遗留场景】新接入一律使用 ALB。仅对存量 NGINX Ingress 进行迁移与兼容，不再作为默认入口。
  - Ingress‑NGINX：每集群一套；域名示例 `miker.chekkk.com`（prod）、`miker-staging.chekkk.com`、`miker-dev.chekkk.com`。
  - 端口规范（强制）：Service 端口命名 `http`，`targetPort` 指向容器端口（推荐 3000）；Ingress backend 使用 `port.number: 80`（Admission 兼容最佳）。
  - Admission：长期 `failurePolicy=Fail`；必要时临时 `Ignore` 应急，操作完即恢复。
  - 镜像源：controller 与 certgen 镜像走 VECR/国内镜像源并固定版本（如 controller v1.10.1、certgen v1.4.1）。

- OpenAPI 脚本化创建资源（Volcengine 最小实践）
  - 认证与公共参数：统一走 `open.volcengineapi.com` 或服务专属域；签名使用 AK/SK；所有请求带 `Action`、`Version`、`Region`。
  - NAT 网关
    - 创建：服务 `natgateway`，端点 `https://natgateway.<region>.volcengineapi.com`，`Action=CreateNatGateway`，`Version=2020-04-01`。
    - 关键参数：`NatGatewayName`、`VpcId`、`SubnetId`、`Spec=Small`。
    - 绑定 EIP：优先使用 EIP 服务 `AssociateEipAddress`（服务 `eip`，`Version=2020-04-01`，参数：`AllocationId`、`InstanceType=Nat`、`InstanceId=<NAT_ID>`）。
    - 创建 SNAT：`CreateSnatEntry`（服务 `natgateway`，`Version=2020-04-01`）；常用参数：`NatGatewayId`、`SubnetId` 或 `SourceCIDR`、以及 `AllocationId` 或 `PublicIpAddresses.N`。
    - 文档参考：NAT 网关与 SNAT 规则 [文档](https://www.volcengine.com/docs/6404/198717)。
  - VKE 集群
    - 创建：服务 `vke`，端点 `https://open.volcengineapi.com`，`Action=CreateCluster`，`Version=2022-05-12`。
    - 最小可行参数（Flannel 示意）：
      - `ClusterConfig`: `VpcId`、`ClusterType=Managed`、`KubernetesVersion`、`SubnetIds`。
      - `PodsConfig.FlannelConfig.PodCidrs=["172.20.0.0/16"]`（各环境避免冲突）。
      - `ServicesConfig.ServiceCidrsv4=["10.96.0.0/12"]`。
    - 创建节点池：`Action=CreateNodePool`（同 `Version`）。
      - 核心：`SubnetIds`、`InstanceTypeIds`、`KubernetesConfig.MaxPodsPerNode`、`Security.Login`（密码或密钥对登录，按文档允许的字段形态填写）。
      - 若字段校验严格，建议在控制台校验一次参数组合（密码策略、卷类型枚举等），再回填到脚本。
    - 生成 kubeconfig：`Action=CreateKubeconfig`，建议 `Type=Private`、`IpAddressType=Ipv4`；或用 `ListKubeconfigs` 查询已有条目。
    - 文档参考：创建集群/节点池/Kubeconfig [CreateCluster](https://www.volcengine.com/docs/6460/100936) / [CreateNodePool](https://www.volcengine.com/docs/6460/115194) / [CreateKubeconfig](https://www.volcengine.com/docs/6460/163034)。
  - Kubernetes 命名空间（通过 kubeconfig + kubectl）
    - 命名规范：`<product>-<env>`（例：`miker-prod/staging/dev`）。
    - 流程：用 `CreateKubeconfig` 获取 kubeconfig → 设置 `KUBECONFIG` → 通过 Kubernetes API 创建 `Namespace`。
    - 脚本模板：`templates/scripts/create-namespace.sh`
    - 一步到位脚本（含基础配额/策略，可直接使用）：
      ```bash
      # 将 CreateKubeconfig 返回内容写入文件，然后：
      export KUBECONFIG=/tmp/kubeconfig-target.conf

      PRODUCT=miker
      ENV=prod
      NS="${PRODUCT}-${ENV}"

      kubectl get ns "$NS" >/dev/null 2>&1 || kubectl create namespace "$NS"

      cat <<'YAML' | kubectl -n "$NS" apply -f -
      apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: rq-basic
      spec:
        hard:
          requests.cpu: "4"
          requests.memory: 8Gi
          limits.cpu: "8"
          limits.memory: 16Gi
      ---
      apiVersion: v1
      kind: LimitRange
      metadata:
        name: lr-defaults
      spec:
        limits:
          - type: Container
            default:
              cpu: "500m"
              memory: 512Mi
            defaultRequest:
              cpu: "100m"
              memory: 128Mi
      ---
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: default-deny-all
      spec:
        podSelector: {}
        policyTypes: [Ingress, Egress]
      YAML
      ```
  - 超时与重试：EIP/NAT/VKE 接口偶发超时或 404/InvalidAction，请：
    - 固定稳定 `Version`；
    - 增大连接/读取超时与重试（3–5 次指数退避）；
    - 必要时切换到控制台创建，随后用 OpenAPI 查询并回填资源 ID（NAT/VKE/EIP）。
- 配置与注册
  - 在 Nacos 为三环境建 namespace：`miker-dev/staging/prod`；拷贝模板配置并差异化。
  - 敏感项不明文：放 K8s Secret 或 External Secrets；服务元数据建议包含 `openapi_url/docs_url/owner/desc`。

- 命名与标签规范（云资源）
  - 集群与节点池
    - 集群：`chek-<env>-k8s`（例：`chek-dev-k8s`、`chek-staging-k8s`）。
    - 节点池：`<env>-default-pool` 或 `<env>-<purpose>-pool`（例：`dev-default-pool`）。
  - 网络与出网
    - VPC：`chek-<purpose>-vpc-<nn>`（例：`chek-k8s-vpc-01`）。
    - 子网：`subnet-<env>-<az>-<cidr-suffix>`（例：`subnet-dev-a-110`）。
    - NAT 网关：`nat-<env>-egress-<nn>`（例：`nat-dev-egress-01`、`nat-staging-egress-01`）。
    - EIP：`eip-<product>-<env>-egress-<nn>`（例：`eip-miker-dev-egress-01`）。
    - 安全组：`sg-<env>-<purpose>`（例：`sg-dev-k8s`）。
  - 标签建议（所有资源）
    - `project=miker`、`env=<dev|staging|prod>`、`owner=platform`、`purpose=<k8s|egress|db|cache>`。
  - 域名与证书
    - 业务域：`<product>-<env>.chekkk.com`；生产 `miker.chekkk.com`。
    - TLS Secret：`<product>-tls`；ClusterIssuer：`letsencrypt-http` 或平台内置。
- 数据库与缓存
  - 优先托管（RDS MySQL/PG/SQLServer、Redis）；变更审计后续接入 Archery；凭据发放走 External Secrets；访问由 NetworkPolicy 白名单控制。
  - 迁移：Helm Hook/Flyway/Liquibase/Mongock 等以 Job 形式执行，输出落 TOS 备份。
- 对象存储 TOS
  - 长期主义：每“区域”一个桶，统一前缀，不按环境复制数据。
    - 北京集群：桶 `chek-app-assets`（cn-beijing）；前缀 `app_project_pic/` 为封面/头像等静态资源唯一真源。
    - 所有环境统一读取该前缀；仅 prod 允许写入（dev/staging 只读）。如需沙箱上传，使用独立前缀 `dev-test/`。
  - 访问与权限：
    - 端点：`https://tos-<region>.volces.com`（同区解析，配 VPC Endpoint 走私网）。
    - IRSA 最小权限（示例，prod）：
      - ListBucket 条件：`s3:prefix = app_project_pic/*`
      - 对象：`arn:aws:s3:::chek-app-assets/app_project_pic/*` 的 `GetObject/PutObject/AbortMultipartUpload`
    - dev/staging 仅 `List/Get`；禁止 `Put`（或仅允许 `dev-test/*`）。
  - 部署约定（vehicle-model-service）：
    - `TOS_ENDPOINT=https://tos-cn-beijing.volces.com`
    - `TOS_BUCKET=chek-app-assets`
    - `TOS_PREFIX=app_project_pic`
  - CDN/域名：统一一个加速域名（如 `assets.miker.chekkk.com`）指向该桶/前缀；各环境共用 URL，避免漂移与同步问题。
  - CI：如需构件归档可临时用 AK/SK；业务运行时统一 IRSA。
  - 参考：K8s/CLI/Actions 用法与常见问题见文末 TOS 小节示例。

### 图片上传与查看微服务规范（统一约定｜适用于 vehicle-model-service 等）

- **存储与前缀（TOS 层）**
  - 桶：统一使用 `chek-app-assets`（cn-beijing），所有封面/头像等静态图片共用一套桶。
  - 前缀：统一落在 `app_project_pic/` 下，按日期分层：`app_project_pic/<yyyyMMdd>/...`。
  - 多产品复用：同一图片可以被多个产品共用，不再按产品名拆分桶或一级前缀（如 `app_project_pic/miker/...`），以简化治理与缓存命中。

- **文件命名（SEO 友好 + 可运维）**
  - 允许字符：仅使用小写 `[a-z0-9-_.]`，用 `-` 分词，禁止空格与中文，避免 URL 编码与跨系统兼容问题。
  - 推荐模式（封面图）：`<brand>-<series>-<model>-<extra>-cover-<hash8>.<ext>`：
    - 示例：`byd-han-ev-2024-long-range-cover-a1b2c3d4.jpg`。
    - `brand/series/model/extra`：使用英文或稳定拼音（如 `byd`、`han`、`ev`），便于搜索收录与人工检索。
    - `hash8`：来自文件内容或 `业务ID+时间戳` 的 `sha1/md5` 前 8 位，保证在同一天前缀下冲突概率极低。
  - 完整 key 示例：`app_project_pic/20240828/byd-han-ev-2024-long-range-cover-a1b2c3d4.jpg`。

- **对外 URL 与域名（CDN 层）**
  - 域名：静态图片统一使用产品无关域名 `assets.chekkk.com`，在 DNS 中 CNAME 到火山引擎加速域名，再由 CDN 回源 `chek-app-assets` 桶。
  - URL 形态：`https://assets.chekkk.com/app_project_pic/<yyyyMMdd>/<seo-name>.<ext>`，与 TOS key 一一对应：
    - 示例：`https://assets.chekkk.com/app_project_pic/20240828/byd-han-ev-2024-long-range-cover-a1b2c3d4.jpg`。
  - 要求：前端/移动端只使用该对外 URL，不直接组合 TOS 内部域名或桶名，避免未来迁移/CDN 变更时破坏链接。

- **业务数据源写入（所有有图片上传的微服务必须遵守）**
  - 原则：**对象存储只负责“放文件”，图片的“引用关系”必须由业务数据源管理**（Mongo/DB 等），不能只在 TOS 放一张无人引用的图片。
  - 每条业务记录（例如车型）应至少包含两个字段：
    - `cover_image`：完整对外 URL，例如 `https://assets.chekkk.com/app_project_pic/20240828/byd-han-ev-2024-cover-a1b2c3d4.jpg`；
    - `cover_image_key`：TOS key，例如 `app_project_pic/20240828/byd-han-ev-2024-cover-a1b2c3d4.jpg`，用于运维迁移/巡检/批量处理。
  - vehicle-model-service 现状与目标：
    - 现状：已从 Mongo 的 `cover_image` 读取 URL 返回列表/详情；后续改造时需补充 `cover_image_key` 字段。
    - 目标：上传封面后，由服务端负责在 Mongo 中更新 `cover_image` 与 `cover_image_key`，前端只关心返回的 URL。

- **上传接口模式（统一长期做法）**
  - 新微服务或改造后的微服务，应统一采用“**预签名或后端代理上传 + 写回业务数据源**”的模式，而不是只裸返回 `{bucket,key,url}`：
    - 推荐接口示例（vehicle-model-service）：
      - `POST /api/vehicles/{id}/cover/presign`：
        - Request：`{ "contentType": "image/jpeg", "suggestName": "比亚迪 汉 EV 2024" }`
        - 服务根据车型文案与规则生成 SEO 文件名（例如上文的 `byd-han-ev-2024-cover-a1b2c3d4.jpg`），拼出 key：`app_project_pic/<yyyyMMdd>/<seo-name>.jpg`。
        - Response：`{ "key": "<tos-key>", "putUrl": "...", "getUrl": "...", "expiresIn": 600 }`。
      - 或 `POST /api/vehicles/{id}/cover`（代理上传版本，直接接收 `multipart/form-data`）。
    - 业务写回：
      - 在确认 TOS PUT 成功后，由服务端统一更新业务库中的 `cover_image` 与 `cover_image_key` 字段；
      - 列表/详情接口统一从业务库读取 `cover_image` 返回，确保“图片展示”只依赖业务真源而非硬编码路径。
  - 历史直传接口的处理：
    - 历史上存在的“未绑定具体业务记录、仅返回 `{bucket,key,url}` 的上传接口”（例如 `POST /api/vehicles/cover/upload`）仅视为过渡形态，不再推荐。
    - 后续有图片上传需求的微服务一律参照本节规范设计接口，并在上线前清理/标记废弃旧直传接口。

- **环境与权限约束**
  - 写入策略：默认仅 prod 对 `app_project_pic/*` 开启写权限（IRSA + 桶策略），dev/staging 只读：
    - 如需在 dev/staging 做上传联调，应使用单独前缀（如 `dev-test/`）或独立测试桶。
  - 鉴权与审计：
    - 图片上传接口需要通过业务鉴权（例如登录态/角色），避免匿名大量写入；
    - 建议记录上传操作日志（业务 ID、key、操作者、时间），以便事后追溯与清理。

TOS 预签名直传与下载（后端规范）
- 场景：移动端/前端直传封面/头像到 `chek-app-assets/app_project_pic/`，后端仅颁发预签名 URL；或服务间下载私有对象。
- 统一约束：
  - 仅使用虚拟域名寻址：`https://<bucket>.tos-<region>.volces.com`；禁用 path 样式（否则可能返回 InvalidPathAccess）。
  - 生产环境后端通过 IRSA→STS 获取临时凭证；dev/staging 可用 AK/SK（Secret 注入）。
  - 桶策略需放行 assumed-role 主体：`trn:sts::<accountId>:assumed-role/<roleName>/*`；Principal 必须写在 `Principal.TOS` 下；Resource 使用不带区域/账号的 TRN（`trn:tos:::<bucket>`）。
  - 建议对象段允许 `tos:GetObject,tos:PutObject,tos:PutObjectAcl,tos:AbortMultipartUpload`。

- 预签名接口约定（vehicle-model-service 示例）
  - `POST /api/vehicle-model/cover/presign`
    - Request: `{ "keyHint": "vehicle-control/<modelId>.jpg", "contentType": "image/jpeg" }`
    - Response: `{ "key": "app_project_pic/vehicle-control/<modelId>.jpg", "putUrl": "...", "getUrl": "...", "expiresIn": 600 }`
  - 后端生成规则：
    - 若传入 `keyHint` 则拼接到 `app_project_pic/` 下；否则生成 UUID 路径；统一小写后缀。
    - 预签名有效期默认 600 秒。

- 参考代码（Java/Spring Boot，S3 v4 直连 TOS）
```java
// Bean
@Bean
public S3Client s3Client(@Value("${tos.endpoint}") String endpoint,
                         @Value("${aws.region}") String region) {
  return S3Client.builder()
      .credentialsProvider(DefaultCredentialsProvider.create()) // IRSA or env
      .region(Region.of(region))
      .endpointOverride(URI.create(endpoint))
      .serviceConfiguration(S3Configuration.builder().pathStyleAccessEnabled(false).build())
      .build();
}

// Service 方法
public PresignResult presign(String key, String contentType, Duration ttl) {
  S3Presigner presigner = S3Presigner.builder()
      .credentialsProvider(DefaultCredentialsProvider.create())
      .region(Region.of(region))
      .endpointOverride(URI.create(endpoint))
      .serviceConfiguration(S3Configuration.builder().pathStyleAccessEnabled(false).build())
      .build();

  PutObjectRequest putReq = PutObjectRequest.builder()
      .bucket(bucket)
      .key(key)
      .contentType(contentType)
      .build();
  PresignedPutObjectRequest put = presigner.presignPutObject(b -> b.signatureDuration(ttl).putObjectRequest(putReq));

  GetObjectRequest getReq = GetObjectRequest.builder().bucket(bucket).key(key).build();
  PresignedGetObjectRequest get = presigner.presignGetObject(b -> b.signatureDuration(ttl).getObjectRequest(getReq));

  return new PresignResult(key, put.url().toString(), get.url().toString(), ttl.getSeconds());
}
```

- 参考代码（Node.js，aws-sdk v3）
```ts
const client = new S3({
  endpoint: process.env.TOS_ENDPOINT, // https://tos-cn-beijing.volces.com
  region: process.env.AWS_REGION,
  forcePathStyle: false,
  credentials: fromNodeProviderChain(), // IRSA/env/ecs
});
export async function presign(key: string, contentType = "image/jpeg", expiresIn = 600) {
  const put = await getSignedUrl(client, new PutObjectCommand({ Bucket: process.env.TOS_BUCKET, Key: key, ContentType: contentType }), { expiresIn });
  const get = await getSignedUrl(client, new GetObjectCommand({ Bucket: process.env.TOS_BUCKET, Key: key }), { expiresIn });
  return { key, putUrl: put, getUrl: get, expiresIn };
}
```

- 常见错误与排查
  - 403 AccessDenied（EC=0003-00000015）：主体未命中或被上层策略拦截。核对桶策略 Principal：`Principal: { "TOS": ["trn:sts::<accountId>:assumed-role/<roleName>/*"] }`；同时确认角色身份策略已允许对应动作；如仍失败，短期开启桶策略 `Principal.TOS=["*"]` 验证链路，再收紧。
  - 403 InvalidPathAccess：使用了 path 样式。改为虚拟域名样式并设置 `forcePathStyle=false`。
  - MalformedPolicy（Invalid resource/condition key）：Resource 必须 `trn:tos:::<bucket>`/`trn:tos:::<bucket>/*`，条件键使用 `tos:prefix`，仅可用于 `ListBucket`。
  - 403 Get 但 404：PUT 未成功（预签名通过但被策略拒绝）。先确认 PUT 允许并允许 `PutObjectAcl`。

桶策略示例（最小可用，先通后收紧）
```json
{
  "Statement": [
    {"Sid":"ListHeadBucket","Effect":"Allow","Principal":{"TOS":["trn:sts::2100459557:assumed-role/temp_tos_connect/*","trn:iam::2100459557:role/temp_tos_connect"]},"Action":["tos:HeadBucket","tos:ListBucket"],"Resource":["trn:tos:::chek-app-assets"]},
    {"Sid":"RWAppPics","Effect":"Allow","Principal":{"TOS":["trn:sts::2100459557:assumed-role/temp_tos_connect/*","trn:iam::2100459557:role/temp_tos_connect"]},"Action":["tos:GetObject","tos:PutObject","tos:PutObjectAcl","tos:AbortMultipartUpload"],"Resource":["trn:tos:::chek-app-assets/app_project_pic/*"]}
  ]
}
```

TOS 接入方式（微服务/CI）
- 适用对象：任意后端微服务、CI 产物上传。
- 标准接入（K8s 微服务，IRSA）：
  1) 为命名空间创建 SA 并绑定 IRSA 角色（最小权限）。
     - 模板：`templates/k8s/sa-tos.example.yaml`
     - 策略示例：`templates/k8s/role-tos-policy.example.json`（将 `<prefix>` 替换为业务前缀，如 `app_project_pic`）。
  2) 部署注入环境变量：`TOS_ENDPOINT/TOS_BUCKET/TOS_PREFIX/AWS_REGION`，非生产将 `ALLOW_UPLOAD=false`。
     - 模板 values：`templates/helm/examples/tos.values.example.yaml`
  3) 代码使用 S3 兼容 SDK（boto3/minio 等），SDK 将自动从 IRSA 获取临时凭证。
- CI 用 AK/SK（仅临时）：将 AK/SK 放仓库 Secrets，使用 `tosutil`/`aws s3` 上传，构件生成后即上传到 `s3://chek-app-assets/<product>/<env>/...`。

部署环境接入 TOS 常见坑与实战
- 节点出网受限导致 Pod 卡在 ContainerCreating：
  - 现象：无法拉取 `registry.k8s.io/pause:3.6`，报 `us-west2-docker.pkg.dev` 超时。
  - 处理原则（不修改 containerd）：
    1) 开放 443/53 出站与 NAT；
    2) 保证 CoreDNS 正常；
    3) 将公共镜像改用国内代理或同步到 VECR（见“公共镜像国内代理映射”）；
    4) 如确需替换 `pause`，在镜像仓库侧提供国内等价镜像，并在集群/工作负载层面覆盖，禁止改节点 CRI 配置。
- TOS 桶区域与端点不一致：
  - 现象：`tosutil` 报 404；上海桶用北京端点。
  - 解决：统一在北京区域新建桶 `chek-app-assets`，端点固定 `https://tos-cn-beijing.volces.com`。
- 权限过宽/过窄：
  - 建议：prod 仅允许目标前缀写入，dev/staging 只读；最小权限策略模板见 `templates/k8s/role-tos-policy.example.json`。
- VPC Endpoint 未启用：
  - 影响：经公网访问 TOS，延迟与合规风险高。
  - 解决：按 `templates/k8s/tos-endpoint-vpc-endpoint.example.md` 开启私网访问与私有 DNS。

模板索引（与位置）
- K8s
  - IRSA ServiceAccount：`templates/k8s/sa-tos.example.yaml`
  - IRSA 策略（最小权限）：`templates/k8s/role-tos-policy.example.json`
  - 节点维护（pause 镜像修复 DaemonSet）：`templates/k8s/pause-fix-daemonset.yaml`
  - TOS VPC Endpoint 步骤：`templates/k8s/tos-endpoint-vpc-endpoint.example.md`
- Helm values 示例
  - `templates/helm/examples/tos.values.example.yaml`
- 监控工作区（Prometheus/VMP）
  - 当前约定：统一一个工作区 `platform-all`（dev/staging/prod 共用），便于快速落地与集中看板/告警。
  - 何时拆分：出现独立告警/权限/留存需求，或非生产指标体量/压测明显增大时，拆分为 `platform-prod` 与 `platform-nonprod`（必要时扩展到 dev/staging/prod 三套）。
  - 绑定：基础组件（Nacos、Ingress、ARC 等）创建时绑定该工作区；告警统一路由到飞书机器人。
- GitOps 与 CI/CD
  - GitHub Actions：构建/扫描/签名/推镜像；触发 GitOps。Argo CD：从 infra 下发 Helm/Kustomize；可视化与回滚。
  - 镜像仓库：VECR 为主（Harbor 作为补充）。包仓库：Nexus（后续上线）。
  - 仓库 Secrets：`REGISTRY_*`（VECR 登录）、`NACOS_*`、`TOS_*`、必要 DB 凭据（建议改 External Secrets）。
  - 流程：`dev → staging → 生产 tag → prod`；失败自动通知飞书机器人。
  - 分支策略与保护：
    - 分支映射：`dev`→开发环境、`staging`→预发、`main` 打 tag→生产。
    - 命名规范：`feature/<issue>-<desc>`、`fix/<incident>-<desc>`、`chore/<task>`、`docs/<topic>`、`refactor/<module>`。
    - 保护规则：`main/staging` 受保护（必须评审、必须通过检查、禁止直推、要求线性历史与对话已解决）；可选保护 `dev`。
    - 必跑检查：Lint/TypeCheck/Test/Trivy/CodeQL/Gitleaks；Actions 工作流提供统一入口（可按项目渐进收紧）。
    - 自动化：开启合并后自动删除分支；90 天未更新分支标记过期并清理。
  - 直接 Helm 部署（新项目默认）：在业务仓接入 `ci-helm-deploy.yml`，按环境注入 `KUBECONFIG_<ENV>` Secrets 完成部署；待稳定后改为“CI 仅构建推镜像 + GitOps PR”。
  - **应用行为改动与 Helm 对齐（重要）**：
    - 对采用 `deploy.yml` + `helm/**` 触发发布的服务（如 `vehicle-model-service`），凡是会影响线上行为的改动（如健康检查路径 `/actuator/health`、`/api/<svc>/healthz`，端口、`ROOT_PATH`、Nacos 元数据等），**必须同时更新 `helm/` 下对应 values/Probe 等配置**，保证探针、路由与代码一致。
    - push 到 `main` 时，**只有改动路径命中 `.github/workflows/deploy.yml` 或 `helm/**` 时，`deploy.yml` 才会自动触发**；因此严禁只改 `app/**` 代码、不改 Helm，导致 prod 未实际发布或探针/路由与实现脱节。
- ARC（Actions Runner Controller）
  - 控制器镜像与 `kube-rbac-proxy` 统一切到 VECR；`kube-rbac-proxy` 使用 `--tls-cert-file/--tls-private-key-file` 并挂载证书。
  - RunnerDeployment 设 `ephemeral: false`、`RUNNER_EPHEMERAL=false`、`replicas: 1`，组织 Runner Online。
  - 常见问题与排障（本次实战）：
    - Runner 容器启动即退出 `ExitCode=0`：使用了原生 `actions/actions-runner` 镜像，缺少 ARC wrapper。改用 `summerwind/actions-runner` 并镜像到 VECR。
    - GHCR `manifest unknown`：`ghcr.io/actions-runner-controller/actions-runner:2.321.0` 标签不存在；改走 Docker Hub `summerwind/actions-runner` 并同步到 VECR。
    - 集群拉取 Docker Hub 超时：出网到 Docker Hub 受限；在 CI 拉取后推 VECR，由集群从 VECR 拉取。
    - DNS/HTTPS 超时或 `lookup api.github.com i/o timeout`：重装 CoreDNS，`dnsPolicy: None` + `dnsConfig` 指定上游（1.1.1.1/8.8.8.8/114.114.114.114），放通 53/443；用 busybox `nslookup/curl/openssl s_client` 验证。
    - `workVolumeClaimTemplate must be specified in container mode kubernetes`：如用 `containerMode: kubernetes` 需提供 `workVolumeClaimTemplate`；否则改用 `dockerdWithinRunnerContainer: true`/`dockerEnabled: true`。
    - `kube-rbac-proxy`：替换参数为 `--secure-listen-address/--tls-cert-file/--tls-private-key-file` 并挂载证书；移除多余代理环境变量。
    - 版本与参数：固定 runner 镜像版本（避免 `:latest` 漂移）；设置 `DISABLE_RUNNER_UPDATE=true`、必要时 `RUNNER_URL` 指向目标仓库/组织。
    - 在线验证：用 GitHub API `/repos/<org>/<repo>/actions/runners` 查看 `status=online busy=false`；Pod 日志应出现 `Listening for Jobs`。
- 平台一览（原文并入，便于“去哪做什么”）
  - 代码：GitHub Enterprise（chekdata 组织）。CI：GitHub Actions / Jenkins（如需）。
  - GitOps：Argo CD（按 `infra/` 管理）。API：YApi（https://yapi.chekkk.com）。
  - 镜像：VECR/Harbor。配置注册：Nacos（172.31.44.114:8848，私网）。观测：Grafana/Prometheus/Loki（待装）。

三、作为一个开发团队成员的操作流程（最短路径）

（团队共享速查｜10 分钟上手）
- 一次性准备（平台侧）
  1) Nacos：在平台实例“开分区”，新建命名空间 `<product>-<env>`（例：miker-dev）。
  2) YApi：在分组（Miker/app/SaaS/公用微服务）下“新建项目 `<product>-<env>` 或 `<service>`”。
  3) 飞连 SSO：团队成员首次登录后自动加入四个分组，无需手动加组。

- 仓库侧（每个后端微服务）
  1) Secrets：
     - 镜像：`REGISTRY_ENDPOINT/REGISTRY_USERNAME/REGISTRY_PASSWORD`
     - YApi：`YAPI_BASE=https://yapi.chekkk.com`、`YAPI_TOKEN_DEV|STG|<ENV>`
     - 如需：`NACOS_*`（自建或直连地址时），其余敏感项走 External Secrets 更佳。
  2) 工作流：复用组织 `.github` 可复用工作流
     - 自动探测版：`uses: chekdata/.github/.github/workflows/yapi-sync-autodetect.yml@main`
     - 本地生成版：`uses: chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main`
  3) Nacos：运行时仅切 `NACOS_NAMESPACE=<product>-<env>`；地址/账号不变。
  4) 联调域名：按环境使用 `<product>-<env>.chekkk.com`，后端暴露统一前缀路径，前端直接调用。

- 自检清单（过线即对外联调）
  - Actions 成功产出镜像并推 VECR；部署镜像版本与期望一致（Digest/Tag）。
  - Service targetPort=容器端口，Ingress backend 使用 `port.number: 80`；健康探针通过。
  - YApi 目标项目中接口“已导入/合并”且公开可见。
  - Nacos 命名空间中可查询到服务实例（健康=true）。

- 账号与入口
  - 用个人 GitHub 登录飞连完成授权，自动加入 `chekdata` 组织所需团队；本地常用 `gh/git/docker/kubectl(只读)`。
- 代码与分支
  - 分支：dev（开发）、staging（测试）、main（生产）。生产标签：`vX.Y.Z`/`vX.Y.Z-rc.N`；main 受保护（评审+CI+签名制品）。
- 新建/改造服务到上线
  - 用模板仓库（已内置在 `ops-bootstrap/templates/`）：
    - Actions：`templates/actions/ci-docker-vecr.yml`（构建并推 VECR）
    - Docker：`templates/docker/Dockerfile.node`、`.dockerignore`
    - Helm：`templates/helm/Chart.yaml`、`values.yaml`、`templates/{deployment,service}.yaml`
    - K8s：`templates/k8s/secret-nacos-cred.example.yaml`
  - 快速开始（示例，创建 `mysvc` 于 `miker-dev`）：
    ```bash
    cp -r ops-bootstrap/templates/helm miker.repo/charts/mysvc
    cp ops-bootstrap/templates/docker/Dockerfile.node miker.repo/Dockerfile
    cp ops-bootstrap/templates/docker/.dockerignore miker.repo/.dockerignore
    mkdir -p miker.repo/.github/workflows && \
      cp ops-bootstrap/templates/actions/ci-docker-vecr.yml miker.repo/.github/workflows/ci.yml
    # 替换占位符：<product>=miker <service>=mysvc <namespace>=miker-dev <imageRepo>=miker <port>=3000
    ```
  - Push→Actions 自动构建并推 VECR→提 PR 修改 infra 中 `<product>-<env>` 的镜像 tag→合并后 Argo CD 自动发布。
  - 配置放 Nacos，机密用 Secrets/External Secrets；访问云资源用 IRSA；对外暴露写 Ingress + Certificate。
- 日常排障
  - 503/502 快速流程：检查 Service selector → Endpoints（Ready/NotReady）→ Service targetPort 与容器端口 → Ingress backend 是否用 `name: http` → 控制器日志（无 endpoints/连接拒绝）。
  - 集群：`kubectl -n <ns> get deploy,po,svc,ingress` 和 Argo CD UI；CoreDNS/出口优先用 busybox `nslookup/curl` 验证；若命名空间有 ResourceQuota，优先在 `ingress-nginx` 或 `default` 运行临时 Pod。
  - 镜像：确认 imagePullSecrets 与 VECR 镜像存在；CI 失败看 Actions 日志与飞书卡片。

#### 前端接入与排障要点（miker-web 实战）
- 端口与路由（强制）
  - 以镜像实际监听端口为准对齐 `Deployment/Service/Ingress`（miker-web=3001）。
  - `Service` 端口命名 `http`，`targetPort` 指向容器端口；`Ingress` backend 使用 `port.number: 80`（Service 仍命名 `http`）。
- 必需环境变量
  - `USER_API_BASE` 指向上游用户服务，必须包含 `/api/user`，示例：`https://benchmark-staging.chekkk.com/api/user`。
  - 容器内可 `printenv | grep USER_API_BASE` 校验；代理路由在 `app/api/user/v1/[...path]/route.ts`。
- Ingress 设置
  - 建议超时：`nginx.ingress.kubernetes.io/proxy-read-timeout: "120"`、`nginx.ingress.kubernetes.io/proxy-send-timeout: "120"`。
  - 控制器 RBAC：如日志出现 `cannot get resource "leases"`，在 `ingress-nginx` 命名空间补 `Role/RoleBinding` 允许 `coordination.k8s.io/leases: get,create,update`。
- DNS/出网回退（仅在 CoreDNS 未就绪时临时使用）
  - `Deployment.spec.template.spec.hostAliases` 绑定上游域名到可达 IP（示例 `benchmark-staging.chekkk.com → 101.126.6.116`），并可加 `dnsConfig.options: [{name: single-request-reopen}]`。
  - 恢复后应移除 hostAliases，改为修复 CoreDNS 上游与 NAT/SNAT、开放 53/443 出站。
- 验证手册
  - 外部：`curl -I -H 'Host: miker-dev.chekkk.com' http://<LB-IP>/login`（绕过本机代理）；
  - 内部：Pod 内 `curl -I http://127.0.0.1:3001/login` 与 `curl -I http://127.0.0.1:3001/api/user/v1/kaptcha/imageCode?stream=1`；
  - 链路：`kubectl -n <ns> get endpoints <svc> -o wide` 应显示容器端口；控制器 `logs` 无 upstream/探针错误。
- 临时措施的收尾
  - 清理应急 `/_ok` 路由与 `ok-echo` 占位服务；关闭 `allow-snippet-annotations`；保持最小注解集。

#### 后端微服务接入（YApi/Nacos/CI：最短路径）
- 目标：让任意后端微服务在 push/合并后，自动同步 Swagger 到平台 YApi；运行时按命名隔离接入 Nacos。
- 平台侧准备（一次性每个服务做一次即可）
  1) 在 YApi 组“公用微服务”下创建项目，项目名推荐使用 `<service>`（示例：`vehicle-model-service`、`osm-gateway`）。
  2) 进入项目“设置→Token”获取项目 Token，并在对应业务仓库配置 Secrets：
     - `YAPI_BASE`：`https://yapi.chekkk.com`
     - `YAPI_TOKEN`（main 用）/`YAPI_TOKEN_STG`（staging）/`YAPI_TOKEN_DEV`（dev）
  3) 成员访问：通过飞连 SSO 登录 YApi 后将自动加入“公用微服务/SaaS/app/Miker”四个组，无需手动分配。

- 仓库侧接入（两种方式，二选一）
  - 自动探测版（推荐，服务容器本身能暴露 OpenAPI 时）
    1) 在业务仓库新增 `.github/workflows/yapi-sync.yml`：
       ```yaml
       name: yapi-sync
       on:
         push:
           branches: [ main, dev, staging ]
         workflow_dispatch:
         schedule:
           - cron: '0 2 * * *'
       jobs:
         sync:
           uses: chekdata/.github/.github/workflows/yapi-sync-autodetect.yml@main
           with:
             dockerfile: Dockerfile
             context: .
             # 可按服务实际端口/路径自定义（默认会尝试常见候选）
             ports: '80,3000,8080,4010,8090'
             paths: '/openapi.json,/v3/api-docs,/v2/api-docs,/swagger.json,/swagger/doc.json'
           secrets:
             YAPI_BASE: ${{ secrets.YAPI_BASE }}
             # 分支选择对应 Token（也可拆成两个 job 按分支条件调用）
             YAPI_TOKEN: ${{
               (github.ref == 'refs/heads/main' && secrets.YAPI_TOKEN) ||
               (github.ref == 'refs/heads/staging' && secrets.YAPI_TOKEN_STG) ||
               (github.ref == 'refs/heads/dev' && secrets.YAPI_TOKEN_DEV) ||
               secrets.YAPI_TOKEN
             }}
       ```
    2) 首次运行若未探测到文档，请在 `ports/paths` 中加入该服务的真实监听端口与文档路径（例如 SpringDoc 常见为 `/v3/api-docs`）。

  - 本地生成版（稳定，推荐给大部分 Java/Node/Go 服务）
    1) 在 CI 构建阶段生成 `build/swagger/swagger.json`（按技术栈选择一条）：
       - Spring Boot（springdoc-openapi）：通过 profile 或脚本导出到 `build/swagger/swagger.json`（如 `bash scripts/export_openapi.sh`）。
       - Node/Express（swagger-jsdoc）：运行脚本生成 `build/swagger/swagger.json`。
       - Go（swaggo/swag）：`swag init` 生成 `docs/swagger.json` 后复制至 `build/swagger/swagger.json`。
    2) 在业务仓库新增 `.github/workflows/yapi-sync.yml`，仅作为薄封装调用组织级复用工作流：
       ```yaml
       name: yapi-sync
       on:
         push:
           branches: [ main, dev, staging ]
         workflow_dispatch:
         schedule:
           - cron: '0 2 * * *'

       jobs:
         build-and-export-swagger:
           runs-on: ubuntu-latest
           outputs:
             swagger_path: ${{ steps.out.outputs.path }}
           steps:
             - uses: actions/checkout@v4
             - name: Generate OpenAPI (project-specific)
               run: |
                 set -e
                 bash scripts/export_openapi.sh   # 内部生成 build/swagger/swagger.json
             - id: out
               run: echo "path=build/swagger/swagger.json" >> "$GITHUB_OUTPUT"

         import-to-yapi:
           needs: build-and-export-swagger
           uses: chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main
               with:
                 swagger-path: ${{ needs.build-and-export-swagger.outputs.swagger_path }}
               secrets:
                 YAPI_BASE: ${{ secrets.YAPI_BASE }}
             YAPI_TOKEN: ${{
               (github.ref == 'refs/heads/main' && secrets.YAPI_TOKEN) ||
               (github.ref == 'refs/heads/staging' && secrets.YAPI_TOKEN_STG) ||
               (github.ref == 'refs/heads/dev' && secrets.YAPI_TOKEN_DEV) ||
               secrets.YAPI_TOKEN
             }}
       ```

- Nacos 接入（与环境隔离一致）
  - 客户端统一指向平台地址，按环境仅切 `NACOS_NAMESPACE`：
    ```
    NACOS_SERVER_ADDR=<平台 Nacos 地址>
    NACOS_NAMESPACE=<product>-<env>   # 例：miker-dev/staging/prod
    ```
  - Spring Cloud Alibaba 客户端配置 `spring.cloud.nacos.discovery.*`、`spring.cloud.nacos.config.*`；敏感凭据通过 K8s Secret 或 External Secrets 下发。
  - **注册发现命名约定**：对接网关的微服务必须使用与 Service 一致的小写名（如 `vehicle-model-service`、`backend-miker-offline`、`backend-app`、`backend-saas`、`osm-gateway`），避免 `serviceId` 与 K8s Service 名不一致导致排障困难。
  - **统一账号策略与 Secret 约定**：
    - 平台侧为每个产品线准备一组只用于注册发现的 Nacos 账号（如只读 `nacos_rd`/可写 `nacos_op`），按 namespace 授权最小权限；
    - 集群中统一使用名为 `nacos-cred` 的 Secret 挂载到各微服务 Pod，字段推荐为：
      - `NACOS_SERVER`：如 `172.31.120.5:8848`；
      - `NACOS_NAMESPACE`：对应环境的 namespaceId（如 `a15810b5-cca6-4eb2-9f6d-7a2c0d68fa76`）；
      - `NACOS_USERNAME`/`NACOS_PASSWORD`：注册用账号密码；
    - 业务 Deployment/Helm values 统一通过 `envFromSecret: nacos-cred` 或显式 `env` 引用这些字段，禁止在清单中明文写死密码；遇到 Nacos 403 报错（如 `username and pwd is blank!`），优先检查是否按本规范正确挂载了这几项环境变量。
  - **MSE 托管 Nacos 鉴权（vehicle-model-service 403 排查实战）**：
    - 部分 MSE 集群开启了登录鉴权后，对 naming 接口要求 **先登录获取 accessToken 再调用注册/心跳**，否则即便使用 `nacos/tnM*fb6P` 等全局账号也会在 `add_naming_instance` 时返回 `403 Insufficient privilege`；
    - 实战结论（vehicle-model-service）：
      - 同一组 `NACOS_SERVER/NACOS_NAMESPACE/NACOS_USERNAME/NACOS_PASSWORD` 在 Pod 内直接 `curl /nacos/v1/auth/login` 获取到 accessToken，并用  
        `POST /nacos/v1/ns/instance?serviceName=vehicle-model-service&ip=...&port=4010&namespaceId=...&accessToken=...` 可以成功注册并在 `hosts` 中看到实例；
      - 但使用 python `nacos.NacosClient.add_naming_instance(...)` 未携带 accessToken，仅依赖用户名密码时会被服务端拒绝，抛出 `NacosException("Insufficient privilege.")`，后续 `list_naming_instance` 可能出现 `code is 412`；
    - 推荐长期做法：
      - 业务侧在自定义 Nacos 封装里显式走两步：  
        1）`POST /nacos/v1/auth/login`（`username/password` 来自 `nacos-cred`），解析 `accessToken/tokenTtl` 并在进程内缓存；  
        2）调用 `/nacos/v1/ns/instance`、`/nacos/v1/ns/instance/beat` 时，将 `accessToken` 作为 query 参数一并带上，并始终传 `namespaceId=<NACOS_NAMESPACE>`；
      - vehicle-model-service 已按此模式重写了 `core/nacos_client.py`：只要配置了 `NACOS_SERVER/NACOS_NAMESPACE/NACOS_USERNAME/NACOS_PASSWORD`，启动时会自动登录并注册实例，心跳通过 HTTP 维持；失败不会阻塞服务启动；
    - 排查指引（出现 `Insufficient privilege/412` 时）：
      - 在服务 Pod 内优先用 curl 验证当前环境变量：  
        `curl -sS "http://$NACOS_SERVER/nacos/v1/ns/instance/list?serviceName=<svc>&namespaceId=$NACOS_NAMESPACE&username=$NACOS_USERNAME&password=$NACOS_PASSWORD"`；  
        若能列出服务但 `hosts` 为空，再用登录 + accessToken 模式确认服务端确实允许注册；
      - 若 curl + accessToken 能成功而 SDK 一直 403，优先怀疑 SDK 未适配 MSE 的鉴权逻辑，而不是账号/namespace 权限；此时建议：  
        1）升级官方 SDK 到支持 auth 的版本；或  
        2）像 vehicle-model-service 一样在业务代码中改用 HTTP 登录 + 注册/心跳的显式实现。

- 网关与 Nacos 路由规范（统一 `/api/<svc>/...`）
  - 外部入口统一：`https://api-<env>.chekkk.com/api/<svc>/...`，其中 `<svc>` 为微服务标识（如 `vms`、`offline`、`backend-app` 等），由网关统一路由到后端。
  - 每个对外微服务至少需要三类路由（网关侧）：
    - 业务：`/api/<svc>/**` → 后端服务（StripPrefix=2 或直接转发）；
    - 健康：`/api/<svc>/healthz` → 后端 `/healthz` 或 `/actuator/health`；
    - 文档：`/api/<svc>/openapi.json` → 后端 `/openapi.json`。
  - 安全白名单：网关 `SecurityConfiguration/SECURE_IGNORE_URLS` 必须放行 `/api/**/healthz`、`/api/**/actuator/**`、`/**/openapi.json`，避免健康检查和 OpenAPI 被 403 拦截。
  - 发现方式推荐：
    - 默认形态：**业务微服务走 Nacos**（`uri: lb://<nacos-service-name>`），由各环境 Nacos namespace 维护实例列表与权重；
    - 例外：少数基础能力（如鉴权网关 `backend-auth-saas`）可以保留“直连 K8s Service”形式（`http://<svc>.<ns>.svc.cluster.local:80`），但必须在本指南中标记为特例，避免误以为未接入 Nacos。
  - 实战约定（以现有服务为例）：
    - `vehicle-model-service`：外部统一 `/api/vms/**`，当前 prod 走 K8s Service 直连；后续如需要按实例维度做灰度/限流，建议改为 `lb://vehicle-model-service` 并保证 Nacos 注册名一致；
    - `backend-miker-offline`：业务路由 `/api/offline/**` 走 `lb://backend-miker-offline`（Nacos），OpenAPI 通过 `/openapi.json` 直连后端兜底；
    - `backend-app`：`/api/backend-app/**`、`/api/backend-app/openapi.json` 走 `lb://backend-app`；
    - `backend-saas`：应补齐 `/api/backend-saas/**`、`/api/backend-saas/healthz`、`/api/backend-saas/openapi.json` 路由，并统一走 `lb://backend-saas`；
    - `backend-auth-saas`（统一鉴权服务）：部署在 `saas-<env>` 命名空间，通过网关暴露 `/api/auth/**`：
      - 网关内部推荐使用 **直连 K8s Service FQDN** 的形式（例如 prod：`uri: http://backend-auth-saas.saas-prod.svc.cluster.local:80`），明确这是“跨命名空间调用共享服务”，**不要求在 `miker-prod` / `miker-staging` 再起一套 `backend-auth-saas`**；
      - 当前实现（backend-gateway-saas `v0.1.3` 起）：网关内部所有 token 校验均通过 Feign `AuthFeign` 直连 `http://backend-auth-saas.saas-prod.svc.cluster.local:80/api/auth`，不再依赖 `lb://backend-auth-saas` 在网关所用 Nacos namespace 中完成注册；
      - 历史问题：若在网关侧改用 `lb://backend-auth-saas` 走 Nacos，但 `backend-auth-saas` 只在 `namespace=saas-<env>` 注册、而网关连的是另一组 namespace（如 `a15810b5-...`），会导致 Ribbon 报 `Load balancer does not have available server for client: backend-auth-saas`，外部表现为需要鉴权的接口（如 `raw-params`）返回 `500 + INTERNAL_ERROR`；
      - 文档/排障时应显式注明：`miker-prod` 命名空间下看不到任何名字带 `auth` 的 Deployment/Service/Pod 属于预期状态，网关通过 Service DNS 访问的是 `saas-<env>` 中的统一鉴权服务。
    - `osm-gateway`：推荐只保留 `/api/osm-gateway/**` 这一套路径（老的 `/api/osmgw/**` 逐步废弃），业务路由与健康/OpenAPI 可按 “业务 + `/healthz` + `/openapi.json`” 模式配置。

- 验证与常见问题
  - 验证：
    - Actions 日志出现 “YApi synchronization successful.”；
    - 打开 `https://yapi.chekkk.com/project/<id>/interface/api` 能看到最新接口；
    - 首次登录 YApi 使用飞连 SSO，自动加入公共组，确保能看到“公用微服务”组项目列表。
  - 常见问题：
    - 400 参数/Token 错误：检查仓库 Secrets `YAPI_BASE/YAPI_TOKEN*` 是否配置正确，项目 Token 是否与 YApi 对应项目一致。
    - 未探测到文档：在自动探测版中补充 `ports/paths`；或改用“本地生成版”。
    - 容器未暴露文档：确认运行镜像监听端口与路径，或在构建阶段导出 JSON 再导入。
  - 字段命名 / description 最低质量线（配合 LLM 富化）：
    - 路径与操作：
      - path 使用语义化资源名，例如 `/api/vms/api/vehicles/{id}`、`/jobs/{jobId}/logs`，避免 `/doQuery`、`/getData` 这类无语义路径；
      - 每个 path+method 至少要有 `summary` 和 `description`，`summary` 一句话说明“做什么”，`description` 补充业务背景和使用场景；
    - 请求参数：
      - 字段名必须可读、能从字面猜到含义，禁止 `a1/a2/param1/param2` 等无语义命名；
      - 对外字段在 OpenAPI/Pydantic/DTO 上都应填写 `description`，说明含义、单位、取值范围，必要时给出典型示例值；
      - 枚举型字段（状态/类型等）在描述中列出主要枚举值及含义，避免下游和 LLM 自行猜测；
    - 返回数据：
      - 响应 200/4xx/5xx 至少要有 `description`，200 的 schema 中为主要字段补全 `description` 与 `example`；
      - 分页/列表返回统一使用约定结构（如 `items` + `total`），并在 description 中说明语义和排序/分页规则；
    - PR 审查：
      - 新增或修改接口时，review 除看类型正确外，**必须检查字段是否带有合理的 description/example**，空描述或 “TODO” 不允许合入；
      - 若接口仍处早期探索阶段，应在 description 中显式写明“试验性接口 / 未来可能变更”，避免下游误认为已稳定。

- backend-saas 实战补充（Dev/Staging/Prod）
  - 基础镜像治理与 CI：
    - 基础镜像 `eclipse-temurin:17-jre` 已首拉到 `chek-images-cn-beijing.cr.volces.com/devops/library/eclipse-temurin:17-jre`，`backend-saas` 的 `ci.yml` 固定使用该 VECR 镜像作为 `FROM_IMAGE`，构建阶段不再访问外网基础镜像源；
    - dev/staging 分支分别推 `dev/backend-saas:dev` 与 `dev/backend-saas:staging`，`main` 上 `vX.Y.Z` tag 推 `prod/backend-saas:vX.Y.Z + :prod`，并通过 `deploy.yml/deploy-prod.yml` 部署到 `saas-<env>`。
  - 运行时与外部入口：
    - dev/staging/prod 集群中 `backend-saas` Deployment 均为 Running，Nacos 日志显示 `register finished` 与实例列表更新，说明注册/发现链路正常；
    - 从公网入口看：`api-dev.chekkk.com` 健康探活受 ALB/网络出口限制影响仍存在超时，`api-staging.chekkk.com` 返回 301，`api.chekkk.com` 上 `/api/backend-saas/openapi.json` 为 404（因运行态未开启 springdoc Profile），这部分在本指南中视为 **基础设施/路由层配置待后续收口**，与微服务自身健康与 CI/CD 成功解耦。


- 共享微服务接入
  - 直接访问 `service.platform-<env>.svc.cluster.local`；需跨产品白名单由平台添加 NetworkPolicy。
- 飞书机器人
  - 缺陷/需求：GitHub Issues + Projects（看板/里程碑），飞书机器人同步事件（Issues/PR/Workflow run）。
  - 机器人治理：配置与路由以代码管理（infra/.github）；事件路由与降噪规则沿用原文；成员同步工作流与映射文件保持不变。

### CI 构建与飞书通知（要点与操作手册）
- 关键实践（已验证）
  - CI 构建：使用 `docker/login-action@v3`、`docker/setup-buildx-action@v3`、`docker/build-push-action@v6`；VECR 登录凭据由 `REGISTRY_ENDPOINT/USERNAME/PASSWORD` Secrets 提供；镜像标签用 `sha-<short>`。
  - Actions 策略兼容：若出现 “0 jobs/failure before steps”，优先回退到上述稳定 actions 组合，并确认仓库/组织策略允许使用这些官方 actions。
  - 网络与 DNS：VECR/DockerHub/GitHub 访问异常，优先检查 CoreDNS 上游与出网 53/443，再查看 NAT/SNAT 与安全组规则。
  - 分支保护与必需检查：`main/staging` 保护，Required Checks 覆盖 Lint/Type/Unit/Trivy/CodeQL；合并后自动删分支。

- 标准工作流命名（后端微服务｜FastAPI / Spring / Go 通用）
  - `ci.yml`（唯一主 CI，必须有）：
    - 触发：`push` 到 `main/dev/staging`、`pull_request`、`workflow_dispatch`。
    - 职责：运行单元测试/静态检查（可选），使用 Buildx 构建并推镜像到 VECR，按分支映射 `dev/staging/prod` 仓库，标签统一 `sha-<short>`（可附加 `dev-latest/stg-latest` 等别名）。
    - 约束：**禁止再新建平行的“自定义 build”工作流**，所有构建统一走 `ci.yml`，避免多条流水线同时推镜像互相覆盖。
  - `deploy.yml`（生产 Helm 发布，对接 ops-bootstrap 通用 Chart）：
    - 触发：`push` 到 `main` 且命中 `.github/workflows/deploy.yml` 或 `helm/**`，以及 `workflow_dispatch`。
    - 职责：复用 `chekdata/ops-bootstrap/.github/workflows/deploy-helm-call.yml@main`，用参数 `service / namespace / container_port / svc_port / target_port / extra_values_file` 对生产命名空间执行 `helm upgrade --install`。
    - 强制要求：凡采用 `deploy.yml + helm/**` 的服务（如 `vehicle-model-service`），任何影响线上行为的改动（健康探针、端口、`ROOT_PATH`、Nacos 元数据等）**必须同步更新 Helm values/probe**（见上文“应用行为改动与 Helm 对齐”），否则不会自动发布。
    - 发布模式约定（推荐）：
      - **模式 B（推荐，prod 必须）：业务仓 CI 只负责构建并推带版本号的镜像（如 `:vX.Y.Z`、`:prod`），真正“当前线上跑哪个版本”由 infra 仓（如 `ops-bootstrap` 的 Argo Application/Helm values）中的 `image.tag` 决定，所有 prod 发布都以 Git 记录为准，便于审计和回滚；
      - 模式 A（仅 dev/staging 可选）：可以在 dev/staging 环境启用 Argo Image Updater 监听 VECR 中 `:dev/:staging` 或 `sha-*` 的变化，自动更新对应环境的 Argo Application image.tag 以加快联调；**禁止在 prod 环境启用自动追 tag 的 Image Updater**，避免生产版本漂移。
  - `deploy-dev.yml` / `deploy-staging.yml`（按需）：
    - 触发：对应环境分支 `dev` / `staging` 的 `push` + `pull_request` + `workflow_dispatch`。
    - 职责：使用 `kubectl + helm` 或纯 `kubectl` 对 `dev/staging` 命名空间发布，典型流程为：准备镜像 tag（如 `dev-latest` 或最近一次 `sha-<short>`）、确保命名空间和镜像/TOS 等 Secrets 存在、渲染并 apply `k8s/dev/*.yaml` 或执行环境级 `helm upgrade --install`，最后 `rollout status`。
    - 实战示例：`vehicle-model-service` 的 `deploy-dev.yml` 在 dev 集群创建/更新 `vecr-auth` 与 `tos-creds` Secret，应用 `k8s/dev/deployment.yaml` 与 `service.yaml`，并在 Pod 内跑一次 TOS 只读自检。
  - `yapi-sync.yml`（YApi + LLM 富化，统一命名，必须有）：
    - 触发：`push` 到 `main/dev/staging`、`workflow_dispatch`、`schedule: 0 2 * * *`。
    - 职责：作为薄封装 job 级调用 `uses: chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main`，根据分支选择 `YAPI_TOKEN / YAPI_TOKEN_STG / YAPI_TOKEN_DEV`，并按统一规则导入 OpenAPI：
      - 运行时直连网关：`with.swagger-url=https://api.chekkk.com/api/<svc>/openapi.json`；
      - 或文件/Artifact 模式：先在仓库中生成 `openapi.json` 并上传为 artifact，再由复用工作流下载导入 YApi。
    - 注意：`secrets:` 只能写在 **job 级 `uses`** 下（例如 `jobs.sync.secrets`），**禁止** 写在 step 级 `uses` 下，否则会触发 “Invalid workflow file ... Unexpected value 'secrets'”。
  - `openapi-generate.yml`（可选，OpenAPI 文件优先导出）：
    - 职责：在 CI 中运行项目内脚本（如 FastAPI 的 `scripts/gen_openapi.py` 或 Java 的 `scripts/export_openapi.sh`）生成 `openapi.json`，并用 `peter-evans/create-pull-request` 提交更新 PR。
    - FastAPI 注意事项：脚本需设置 `PYTHONPATH=.` 后再 `import app.main`，否则容易出现 `ModuleNotFoundError: No module named 'app'`（本次 `vehicle-model-service` 已踩过）。
  - `<svc>-lint.yml`（例如 `vms-lint.yml`，推荐）：
    - 职责：运行语言级 Lint/Formatter/TypeCheck（如 `ruff + black + mypy`），作为 `main/staging` 的 Required Check。
    - 实践建议：历史问题较多时，可用独立 `chore/lint` 分支分批修复 Ruff/black 报错，避免与业务改动混在一起。
  - `<svc>-kubevalidate.yml`（例如 `vms-kubevalidate.yml`，推荐）：
    - 职责：对 `k8s/**/*.yaml` 运行 `kubeconform`，对 Dockerfile 运行 `hadolint`，对仓库 YAML 运行 `yamllint`。
    - 常见坑点：`kubeconform` 容器模式下应先在 shell 中计算文件列表：`FILES=$(git ls-files 'k8s/**/*.yaml' || true)`，再执行 `docker run -v "$PWD":"$PWD" -w "$PWD" ghcr.io/yannh/kubeconform:latest -strict -ignore-missing-schemas -summary $FILES`；不要把 `$(git ls-files ...)` 直接塞进 `args` 字符串，否则会看到 “$(git - failed validation ...” 这类错误。
  - `<svc>-codeql.yml`（例如 `vms-codeql.yml`，可选但推荐）：
    - 职责：启用 CodeQL code scanning。
    - 注意：仓库需在 GitHub “Code security and analysis” 中开启相应功能，否则工作流会提示 “Code Security must be enabled for this repository to use code scanning.”，这是配置问题而非脚本错误。
 - 通知链路（feishu-notify）
  - 触发范围：仅生产环境启用通知（`main` 分支）。`dev/staging` 默认不发送通知；如需临时开启，另行在项目下覆盖配置。
  - 复用方式：统一复用组织级可复用工作流（`chekdata/.github`），业务仓库引用示例：
    - `uses: chekdata/.github/.github/workflows/reusable-feishu-sync.yml@main`
  - 触发：`feishu-notify.yml` 可由 `on: workflow_run` 监听 `workflows: ["ci"] types: [completed]` 触发；建议同时保留 `workflow_dispatch` 以便手动验证。
  - 环境映射：`main→prod`；`dev/staging` 不推送（默认关闭）。
  - 必需 Secrets（仅生产）：`FEISHU_APP_ID/FEISHU_APP_SECRET`、`FEISHU_CHAT_ID_PROD`（群 chat_id，形如 `oc_...`，且应用必须在群里）。
  - 报文格式：使用 `msg_type=interactive`；`content` 必须是“卡片 JSON 的字符串”（而非对象）。常见错误：
    - 9499 Invalid parameter type in json: content → 将卡片对象转成字符串（如 `jq -Rs .`）。
    - 230001 invalid receive_id → 校验 chat_id 是否为有效群 ID，且应用已加入群。
  - 降级与重试：建议失败时自动降级为 `msg_type=text` 并重试 3 次（指数退避），日志打印 `code/log_id`。

  - 组织级复用与新环境确认（重要）
    - 组织级共用：飞书“通知”和“加群同步”均为组织级工作流与机器人能力的复用，不在业务仓内复制实现。
    - 新环境部署时务必确认：
      1) 是否需要开启飞书通知（默认仅 prod 开启；dev/staging 关闭）。
      2) 需要通知时，请提供目标群聊名称/用途，并创建或指明群聊；遵循下文“飞书群命名规范”。
      3) 确认群配置：将机器人加入群；如群设置“仅群主/管理员可加人”，请授予机器人 `im:chat:operate_as_owner` 或允许全员加人。
      4) 同步成员（GitHub watchers+collaborators → 飞书群）默认开启，如不需要可在业务仓将同步工作流禁用。
    - 参数与开关：通知和同步开关通过工作流 `env` 映射环境控制；仓库仅在 `main→prod` 自动通知，其它环境按需显式开启。
- 快速排障清单
  - CI 登录 VECR 失败：检查三项 `REGISTRY_*` Secrets 是否存在、可用；确认 `docker/login-action@v3` 在组织策略下被允许。
  - Buildx 下载失败：固定使用 `setup-buildx-action@v3`，避免漂移。
  - CI 触发不了：为 `ci.yml` 加 `workflow_dispatch`，网页端手动 Run Workflow 验证；若本地推送不稳定，改用 SSH key（ssh.github.com:443）或直接网页提交。
  - 未收到飞书：看 `feishu-notify` 任务日志的 Feishu API 返回码；定位 9499/230001 按上面处理；确认应用在群内。

- 后端微服务标准工作流与模板
  - 标准命名：
    - `ci.yml`：唯一主 CI（构建 jar + Buildx 推 VECR + 可选推 OpenAPI 到 YApi）。
    - `deploy-dev.yml` / `deploy-staging.yml` / `deploy-prod.yml`：分别负责 Dev/Staging/Prod 环境的 Helm 部署。
    - `yapi-sync-build.yml`：本地起应用导出 OpenAPI 并导入 YApi。
  - 模板路径（本仓）：
    - CI 模板：`templates/actions/microservice/ci.yml`
    - 部署模板：`templates/actions/microservice/deploy-dev.yml`、`templates/actions/microservice/deploy-staging.yml`、`templates/actions/microservice/deploy-prod.yml`
    - YApi 同步模板：`templates/actions/microservice/yapi-sync-build.yml`
  - 使用方法：
    - 将上述模板复制到业务仓 `.github/workflows/` 下；
    - 将其中的 `<service-name>`、命名空间（如 `miker-prod`）、域名（如 `api-dev.chekkk.com`）等占位符替换为实际值；
    - 确保 Secrets 按本指南规范配置：`REGISTRY_ENDPOINT/REGISTRY_USERNAME/REGISTRY_PASSWORD`、`KUBECONFIG_*_B64`、`YAPI_BASE/YAPI_TOKEN_DEV|STG|PROD`。

- 本次 backend-gateway-saas 部署卡点复盘（给后续微服务接入参考）
  - Harbor/VECR tag 爆表：限制 `latest/sha-*` 数量，统一只保留 `v*.*.*` + 最近 N 个 `sha-*`，并在 CI 中避免额外 `latest` 标签。
  - 多条镜像构建工作流并行：`build.yml/ci-build.yml/ci.yml` 同时构建+推镜像导致阻塞，已收敛为单一 `ci.yml`；后续微服务禁止再新建平行的“自定义 build”工作流。
  - ErrImagePull 导致 Service 无 Endpoints：CI 刚完成但镜像尚未可拉取，或 tag 拼写不一致；发布前必须确认 VECR 中存在对应 `sha-<short>`，并在 Helm values 中保持完全一致。
  - Helm 渲染失败 + 手工卸载造成“空壳 Service”：本次线上曾出现 Ingress 指向 `backend-gateway-saas` Service，但该 Service 长期无 Pod Endpoints，根因是早期调试时在 `deploy-prod.yml` 中手工执行了 `helm uninstall/kubectl delete deploy`，而 Chart 又因为 YAML 语法错误持续渲染失败，导致 Deployment 一直没被 ArgoCD 重建。规范：
    - 生产环境禁止通过额外脚本手工卸载 Deployment/Release，**唯一允许的“删资源”入口是修正 Helm/Argo 配置后让 ArgoCD 自行收敛**；
    - 遇到网关 503/空壳 Service 时，排查顺序应为：1）`kubectl get deploy,svc,endpoints <svc>` 确认 Service 是否有 Endpoints；2）`argocd get application <app>`/`describe` 查看是否存在 “Manifest generation error / YAML parse error”；3）在本地用 `helm template` 复现并修正 Chart，再 `git push` 触发 ArgoCD 自动重建；
    - 修复后的验收动作必须包含一次从公网入口的真实调用（如 `curl https://api.chekkk.com/api/vms/healthz`），确认 Ingress → Service → Pod 链路闭环，而不仅仅是看 ArgoCD 状态从 `Degraded` 变回 `Healthy`。
  - `/api/auth/openapi.json` 404 但网关本地 200：本次发现 `http://127.0.0.1:12000/api/auth/openapi.json` 在网关 Pod 内访问正常，但通过 `https://api.chekkk.com/api/auth/openapi.json` 却返回 ALB 级别的 404，原因是 Ingress/ALB 侧虽然已有 `/api/auth/ → backend-gateway-saas:80` 的规则，但早期为 `backend-auth-saas` 预留的 Gateway route 环境变量中包含了空的 `filters_0` 配置，导致该 route 未正确生效。规范：
    - **生产网关路由唯一真源：`SPRING_APPLICATION_JSON`（由 ops-bootstrap/Helm values 下发）**。禁止在 K8s 环境中再使用 `env.SPRING_CLOUD_GATEWAY_ROUTES_*` 或业务仓 `application.yml` 作为路由来源，避免“双轨配置”；历史上若存在 env 方式注入的 route，应在迁移后统一删除。
    - 对于带 context-path 的后端（如 `server.servlet.context-path=/api/auth`），默认不做 StripPrefix，直接将 `/api/auth/**` 原样转发给后端；如确需前缀改写，必须联动后端调整 context-path；
    - 任何“公网 404 / 内网 200”的差异，都应分别在 **ALB 前后各自 curl 一次**（`https://api.chekkk.com/...` vs `curl 127.0.0.1:<gateway-port>/...`），先排除网关自身问题，再看是否有其它 Ingress/ALB 规则抢占了同一 Host/Path。
    - backend-app 实战：prod 中 `backend-gateway-saas` 针对 `/api/backend-app/**` 与 `/api/backend-app/openapi.json` 的路由，已全部收口到由 ops-bootstrap 渲染的 `SPRING_APPLICATION_JSON`，ArgoCD Application 中不再注入任何 `env.SPRING_CLOUD_GATEWAY_ROUTES_*`，业务仓也不再作为 K8s 环境下的路由配置来源；历史上 staging 中为调试使用的 env 路由会在迁移完成后统一删除，避免“双轨”。
  - Nacos 403 `username and pwd is blank!`：环境变量未正确注入，已改为在 ArgoCD Application 中显式设置 `SPRING_CLOUD_NACOS_(DISCOVERY)_USERNAME/PASSWORD`。规范：生产环境必须通过 Secret 或 ArgoCD parameters 显式注入，不依赖“默认匿名”。
  - 健康检查与 OpenAPI 被网关拦截：`SecurityConfiguration` 未显式放行 `/actuator/**`、`/api/**/healthz`、`/**/openapi.json` 等路径，已统一放行。规范：所有新接入网关的微服务必须保证健康探针和 OpenAPI 路径在“白名单”中。
  - HTTP→HTTPS 301 与 Ingress 冲突：多个命名空间同时占用同一域名 80 端口（如 `backend-auth-saas` 与 `api-gateway-http-osm`），导致 404/规则冲突。规范：同一域名的 80 Ingress 只允许一个“统一重定向入口”，业务 Ingress 仅在 443 上暴露带前缀的路径。
  - 工作流与分支膨胀：历史上为调试 YApi/超时等问题创建了大量临时工作流和分支（如 `ci/yapi-timeouts-*`、`ci/yapi-fallback-*`、`ci/openapi-*`），后续已统一清理，仅保留 `ci.yml` + `deploy-*.yml` + `yapi-sync-build.yml`/`yapi-sync-incluster.yml` 以及环境分支 `main/dev/staging`。规范：**与部署相关的新实验一律在现有工作流内通过参数开关实现，临时分支用完必须在一两周内删掉**，避免未来排查时混淆“哪一条才是权威流水线”。
  - 发布路径双轨问题：一度同时存在 “GitHub Actions 直接 `deploy-prod.yml` Helm 发布” 与 “ArgoCD Application GitOps 发布” 两条生产发布路径，容易让运维误判“哪一条失败算真正失败”。当前约定：**生产环境以 ArgoCD Application `backend-gateway-saas-prod` 为单一真源**，`deploy-prod.yml` 仅作为临时诊断工具保留，其因 NetworkPolicy 所有权冲突报错属预期，不应通过删除集群已有策略来“修绿”。
  - VECR 镜像保留：backend-gateway-saas 实战中多次遇到“老 tag 清理、新 tag 还未发布完毕”带来的回滚困难，已按镜像治理章节的建议，为 `dev/prod/backend-gateway-saas` 只保留最近若干个 `sha-*` 以及必要的版本标签。规范：**删除镜像前必须先确认：ArgoCD/Deployment/Helm values 中不再引用该 tag，且不作为当前或上一个回滚目标**。
  - vehicle-model-service / backend-miker-offline 实战补充：
    - 两个微服务仓库在本轮整改后，prod 发布均以“业务仓 CI → VECR → ops-bootstrap/ArgoCD Application/Helm Chart”为单一真源：`vehicle-model-service` 删除了旧的 `k8s/prod/service.yaml`，prod Service 仅由 ops-bootstrap 通用 Chart + `helm/prod.values.yaml` 渲染；`backend-miker-offline` 创建独立 Chart 并在 ops-bootstrap 中新增 `backend-miker-offline-prod` Application，仓内原 `k8s/prod/*.yaml` 已全部删除；
    - 与之对应，两个仓中所有平行或历史部署工作流（如 `ci-build.yml`、手写 `yapi-sync-manual.yml`、自动改 `k8s/prod/deployment.yaml` 镜像的 `update-k8s-image.yml` 等）也已清理，只保留一条 `ci.yml` 主构建线 + 标准 `deploy*.yml` + `openapi-generate.yml` + `yapi-sync.yml` + Lint/Kubevalidate/CodeQL/Release/SBOM/Gitleaks 工作流，避免今后再出现“同一仓有多条互相覆盖的发布流水线”的情况。
- OSM-Gateway 实战补充（CI/CD 与清理）
  - 工作流收敛：`osm-gateway` 最终只保留 `ci.yml` / `deploy.yml` / `yapi-sync.yml` 三条自动工作流，其它如 `ci-build.yml`、`build-kaniko.yml`、`build-rsync.yml` 改为仅 `workflow_dispatch` 手动使用，避免 push main 时触发多条并行构建。推荐后续微服务复用这一模式——**自动路径只有“主 CI + 部署 + 文档同步”，工具型工作流一律改为手动**。
  - 分支治理：为调试 CI/YApi 创建的纯基础设施分支（如 `chore/use-org-workflows`、`chore/yapi-sync-*`、`infra/ci-standard`、`infra/prod-service`）在主线稳定后统一删除，只保留 `main` 和真实功能分支（`feat/*`、`fix/*`）。规范：与部署/CI 调试相关的短期分支，在对应改动合入 main 并验证稳定后，应在 1–2 周内清理。
  - Legacy 清单清理：`osm-gateway` 从手写 `k8s/prod/*.yaml` 迁移到 Helm Chart 管理后，仓库中残留的 `k8s/prod/service.yaml` 容易被误当作权威配置。实战中已删除该文件，仅保留 `helm/osm-gateway/**` 作为服务定义真源。规范：**完成 Helm 或 GitOps 迁移后，必须盘点并删除旧的 K8s 清单（尤其是同名 Service/Ingress），以免双轨配置导致后续排障困难**。
  - Helm 发布容错：在 `deploy.yml` 中，`helm upgrade --install` 偶发返回 `release: already exists`，但集群内 Release 实际已处于 `deployed` 状态。改进做法是在 Helm 命令失败时自动调用 `helm status` 检查 Release：若状态健康则视为成功，仅在 Release 不存在或状态异常时才让流水线失败。建议后续 Helm 部署工作流复用这一模式，在不牺牲安全性的前提下减少“假红灯”。

四、QA（常见问答）
- Jenkins 还需要吗？
  - 主用 GitHub Actions 做 CI；仅当需要有状态 Agent 或特殊编排时短期保留 Jenkins，逐步迁移。
- 测试/缺陷如何管理？
  - GitHub Issues + Projects（看板/里程碑/模板）+ 飞书机器人；生产失败/告警必推送。
- 数据库变更如何管控？
  - RDS 优先；上线走变更/审计（后续接入 Archery）；应用侧用迁移 Job（Flyway/Liquibase/Mongock）并将输出落 TOS 归档。
- TOS 如何打通？
  - 统一 IRSA；CI 可临时 AK/SK；端点 `https://tos-<region>.volces.com`；路径 `s3://<bucket>/<product>/<env>/...`（详见本文件 TOS 小节）。
- DNS/NAT 常见问题
  - 组织 Runner注册报 `lookup api.github.com i/o timeout`：先 busybox 验证 `nslookup/curl`；若 DNS 超时，检查 CoreDNS Pod、kube-dns Endpoints、节点出站 53；若 HTTPS 超时，检查 NAT/SNAT 与安全组 443。

附录A：平台操作步骤与账号规范（精简版）
- Nacos
  1) 创建命名空间：`<product>-<env>`。
  2) 导入配置：按目录与 `dataId` 规范提交到代码库，发布时由 CI 同步；敏感项写入 K8s Secret 或 External Secrets。
  3) 创建账号与权限：
     - 只读：`nacos_rd`（给研发组）；可写：`nacos_op`（运维/发布机器人）。
     - 最小授权目标命名空间，避免全局写入。
  4) 下发到集群：
     ```bash
     kubectl -n <product>-<env> create secret generic nacos-cred \
       --from-literal=address=http://<nacos-host>:8848 \
       --from-literal=username=nacos_rd \
       --from-literal=password='***'
     ```

- YApi
  1) 新建项目：名称 `\<product>-\<env>`；设置负责人与成员（只读/编辑）。
  2) 导入：启用 Swagger 同步或上传 JSON；生成只读 Token 供 CI 使用。
  3) 安全：Token 存于仓库 Secrets（如 `YAPI_TOKEN_<ENV>`）。

- Ingress（ALB）
  1) 证书：确保集群存在 `ClusterIssuer/letsencrypt-http`；业务侧创建 `Certificate`，Secret 命名 `<product>-tls`。
  2) Ingress 示例：
     ```yaml
     apiVersion: networking.k8s.io/v1
     kind: Ingress
     metadata:
       name: <product>-ing
       namespace: <product>-<env>
      annotations: {}
     spec:
        ingressClassName: chek-<env>-alb
       tls:
       - hosts: ["<product>-<env>.chekkk.com"]
         secretName: <product>-tls
       rules:
       - host: <product>-<env>.chekkk.com
         http:
           paths:
           - path: /
             pathType: Prefix
             backend:
               service:
                 name: <service>
                 port:
                   number: 80
     ```
  3) 金丝雀：与 Argo Rollouts 的 `canary` Service 搭配（Ingress 指向 `stable`，按权重切换 `canary`）。

（附录）原文细节未动部分
- “TOS 访问指南”完整示例（K8s/CLI/Actions/权限/常见问题）保留在文末原位置，便于复制。
- “平台一览/成员同步/机器人模板”保留，按上述新目录定位。


## 飞书企业邮箱映射（生成与同步）
- 脚本：`scripts/gen_feishu_mapping_from_xlsx.py`
- 输入：飞书后台导出的通讯录 Excel（示例：`tmp/车控CHEK-通讯录-导出.xlsx`）
- 输出：`mappings/feishu_userid_to_enterprise_email.yaml`
- 使用：
  ```bash
  python3 -m pip install openpyxl pyyaml
  python3 scripts/gen_feishu_mapping_from_xlsx.py --input tmp/车控CHEK-通讯录-导出.xlsx
  ```
- 建议频率：组织成员更新后手动生成一次；或接入自动同步后周更。

## 飞书群命名规范（alerts / deploys）
- 命名规则：`<project>-<env>-<purpose>[-<region>]`，全部小写、用连字符连接；生产环境固定按此规则建立。
  - 例如：`miker-prod-alerts-cn-bj`（生产告警群）、`miker-prod-deploys`（生产发布群）。
  - dev/staging 默认不建通知群，如需启用请按同一规则创建（如 `miker-dev-alerts`），并在仓库中显式开启通知。
- 用途枚举：`alerts`（告警）/ `deploys`（发布）/ `incidents`（事故）/ `ops`（运维）。
- 机器人治理：
  - 统一把机器人加入 `alerts` 与 `deploys` 两类群。
  - 生产 P0 告警群开启必达与@所有人限制，值班责任到人。
  - 群描述中写明“负责人/用途/关联系统”。

## Nacos 使用范围与接入规则
- 适用范围：仅用于后端微服务的注册发现与配置管理；前端（如 `miker` Web 前台）不直接接入 Nacos。
- 平台共享：采用 MSE 托管 Nacos，平台共享一个实例，按 `namespace=<project>-<env>` 做隔离（如 `miker-prod`、`benchmark-prod`）。
- 客户端指引：
  - 统一 Nacos 接入地址（推荐 CNAME，如 `nacos.platform.chekkk.com:8848`），各环境通过 `NACOS_NAMESPACE` 区分。
  - Spring Cloud Alibaba：配置 `spring.cloud.nacos.discovery.*` 与 `spring.cloud.nacos.config.*` 指向平台地址，敏感凭据通过 Secret/External Secrets 下发。
- 不适用项：
  - 业务入口域名（`miker.chekkk.com`、`benchmark.chekkk.com`）不指向 Nacos。
  - 前端运行时配置走环境变量/ConfigMap/CDN，不从 Nacos 直接读取。

### YApi 自动同步（可复用工作流）
- 目的：代码每次 push/合并后自动导出 Swagger 并同步到平台 YApi 项目（合并导入）。
- 工作流位置：组织级复用 `chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main`（可复用 workflow）。
- 调用示例：在业务仓库添加 `.github/workflows/yapi-sync.yml`，内容参考：
  ```yaml
  name: yapi-sync
  on:
    push:
      branches: [ main, dev, staging ]
    workflow_dispatch:
    schedule:
      - cron: '0 2 * * *'
  jobs:
    build-and-export-swagger:
      runs-on: ubuntu-latest
      outputs:
        swagger_path: ${{ steps.out.outputs.path }}
      steps:
        - uses: actions/checkout@v4
        - name: Generate Swagger (project-specific)
          run: |
            # 这里生成 swagger.json 到 build/swagger/swagger.json
            mkdir -p build/swagger
            echo '{"openapi":"3.0.0","info":{"title":"placeholder","version":"0.0.0"}}' > build/swagger/swagger.json
        - id: out
          run: echo "path=build/swagger/swagger.json" >> "$GITHUB_OUTPUT"

    import-to-yapi:
      needs: build-and-export-swagger
      runs-on: ubuntu-latest
      steps:
        - id: sel
          run: |
            if [[ "${{ github.ref_name }}" == "main" ]]; then
              echo "token=${{ secrets.YAPI_TOKEN }}" >> "$GITHUB_OUTPUT"
            elif [[ "${{ github.ref_name }}" == "staging" ]]; then
              echo "token=${{ secrets.YAPI_TOKEN_STG }}" >> "$GITHUB_OUTPUT"
            else
              echo "token=${{ secrets.YAPI_TOKEN_DEV }}" >> "$GITHUB_OUTPUT"
            fi
        - uses: chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main
          with:
            swagger-path: ${{ needs.build-and-export-swagger.outputs.swagger_path }}
          secrets:
            YAPI_BASE: ${{ secrets.YAPI_BASE }}
            YAPI_TOKEN: ${{ steps.sel.outputs.token }}
  ```
- Secrets 约定：
  - `YAPI_BASE`：如 `https://yapi.chekkk.com`
  - `YAPI_TOKEN`（main）/`YAPI_TOKEN_STG`（staging）/`YAPI_TOKEN_DEV`（dev）
  - 推荐做法：**按“每个服务在 YApi 的项目各自一套 Token”配置在对应业务仓库的 Secrets 中**，只有在多个服务确实共用同一个 YApi 项目时，才将该项目的 Token 提升为组织级公共 Secret 供多仓复用。
- YApi 项目命名：`<product>-<env>`，与 Nacos 命名一致（如 `miker-dev/staging/prod`）。
- 平台共用实例：Nacos/YApi 共用一套平台实例，按 namespace/project 隔离，微服务通过环境变量区分环境（`NACOS_NAMESPACE=<product>-<env>`，YApi 在 CI 里切换 Token）。

#### YApi + LLM 富化（统一规范｜默认开启）
- 目标：在导入 YApi 前，自动用 LLM 生成/完善接口 `description`，提升文档可读性与一致性。
- 组织级工作流已内置富化逻辑（保持兼容）：`yapi-sync-reusable.yml`
  - inputs：
    - `enable_llm`：字符串，默认 `"true"`（不开启请显式传 `"false"`）
    - `llm_endpoint`：可选，默认 `https://api.openai.com/v1`
    - `llm_model`：可选，默认 `gpt-4o-mini`
  - secrets：
    - 可选 `LLM_API_KEY`（未提供则自动跳过富化，直接导入原始 `swagger.json`）
- 使用示例（本地生成版对接富化，推荐）：
  ```yaml
  jobs:
    build-and-export-swagger: ...
    import-to-yapi:
      needs: build-and-export-swagger
      runs-on: ubuntu-latest
      steps:
        - id: sel
          run: |
            if [[ "${{ github.ref_name }}" == "main" ]]; then
              echo "token=${{ secrets.YAPI_TOKEN }}" >> "$GITHUB_OUTPUT"
            elif [[ "${{ github.ref_name }}" == "staging" ]]; then
              echo "token=${{ secrets.YAPI_TOKEN_STG }}" >> "$GITHUB_OUTPUT"
            else
              echo "token=${{ secrets.YAPI_TOKEN_DEV }}" >> "$GITHUB_OUTPUT"
            fi
        - uses: chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main
          with:
            swagger-path: ${{ needs.build-and-export-swagger.outputs.swagger_path }}
            enable_llm: "true"           # 默认即为 true，可省略
            # llm_endpoint: https://api.openai.com/v1
            # llm_model: gpt-4o-mini
          secrets:
            YAPI_BASE: ${{ secrets.YAPI_BASE }}
            YAPI_TOKEN: ${{ steps.sel.outputs.token }}
            LLM_API_KEY: ${{ secrets.LLM_API_KEY }}  # 可选；缺省则跳过富化
  ```
- 组织级 Secrets（统一放 Organization → Actions → Secrets）
  - `LLM_API_KEY`（可选，适合所有服务共用同一 LLM Key 的场景）；
  - **YApi Token 默认是“项目级”**：优先在各自业务仓库中配置 `YAPI_TOKEN_DEV / YAPI_TOKEN_STG / YAPI_TOKEN_PROD`。仅当多个服务明确共享同一个 YApi 项目时，才在组织级 Secrets 维护该项目的公共 Token，并在相关仓库中引用。
- 富化行为（简述）：仅对 `description` 为空或过短的 Operation 进行中文补全（约 150–220 字），不会修改路径/方法/Schema 结构；产出 `swagger_enriched.json`，导入失败时自动回退原始 `swagger.json`。
- 网关路径要求（与接入网关长期原则一致）：
  - 服务内部暴露 `/<root>/openapi.json`（FastAPI 默认 `/openapi.json`；SpringDoc `/v3/api-docs` 建议在 CI 导出为单一文件）。
  - 外部统一走网关：`/api/<svc>/openapi.json`，网关路由用 `Path=/api/<svc>/openapi.json` + `SetPath=/openapi.json`，并加入 `SECURE_IGNORE_URLS` 白名单。
  - 临时直达仅用于灰度排障：在独立 Ingress 组以 Exact 路径直达后端，避免与 biz 组 Prefix 冲突。

#### OpenAPI 内容与注释规范（必须）
- 目标：生成的 `swagger.json` 必须“写明接口的用处、用法、目的”，可读可审计，可直接给前后端/第三方对接使用。
- 每个接口（Operation）必须包含
  - `summary`：一句话说明用途（≤80 字）。
  - `description`：详细说明业务背景、输入输出约束、幂等/副作用、鉴权与限流说明、典型错误码与处理建议。
  - `tags`：按业务域归类（如 `user`, `order`）。
  - `operationId`：稳定、唯一的动词式命名（如 `createUser`）。
  - `parameters`：每个参数必须 `description` 与 `example`/`schema.example`，标注 `required` 与取值范围；路径参数必须声明。
  - `requestBody`：引用 `components.schemas.*`，补齐字段 `description`、`example`，必要时提供 `examples`（多示例）。
  - `responses`：至少包含
    - `200/2xx`：成功返回模型（`schema`）与示例（`example` 或 `examples`）。
    - 典型错误：`400/401/403/404/409/422/429/5xx` 中涉及的必须声明结构与示例（包含业务 `code/message`）。
  - `security`：写明所需鉴权（如 `BearerAuth`/`ApiKeyAuth`），匿名接口须显式标注无鉴权。
  - `deprecated`：废弃接口置 `true` 并在 `description` 给出替代方案与下线时间。
- 数据模型（Schema）必须包含
  - 字段 `description`（语义、单位、格式/正则、业务约束）、`example`（或 `examples`）；必要字段用 `required` 标注。
  - 约束：`minLength/maxLength`、`format`、`pattern`、`minimum/maximum`、`enum` 等尽量完备。
- 示例（建议）
  - 每个 Operation 至少 1 个请求、1 个响应示例；复杂场景提供多示例（`examples`）并命名（如 `success`, `validationError`）。
  - 示例不含敏感数据；含脱敏样例（如手机号 `138****0001`）。
- 版本与元信息
  - `openapi: 3.0+`；`info.title/description/version` 必填；`servers` 可按环境示例（不强依赖运行时域名）。
- 目录分层
  - `tags` 按业务域；`components.schemas` 按资源聚合（`User`, `Order`, `CommonError`）。

最小注解示例（Spring Boot｜springdoc-openapi）
```java
@Operation(
  summary = "创建用户",
  description = "用于注册新用户：校验手机号唯一、发送欢迎消息；受限流与登录态校验。"
)
@ApiResponses({
  @ApiResponse(responseCode = "200", description = "创建成功",
    content = @Content(schema = @Schema(implementation = CreateUserResp.class),
      examples = @ExampleObject(name="success", value="{\"userId\":123}"))),
  @ApiResponse(responseCode = "409", description = "手机号已存在",
    content = @Content(schema = @Schema(implementation = ErrorResp.class),
      examples = @ExampleObject(name="dup_phone", value="{\"code\":\"USER_PHONE_EXISTS\",\"message\":\"手机已注册\"}")))
})
@PostMapping("/api/user")
public CreateUserResp create(
  @Parameter(description="手机号", example="13800000000", required = true)
  @RequestParam String phone,
  @Parameter(description="昵称", example="张三") @RequestParam(required=false) String nick,
  @io.swagger.v3.oas.annotations.parameters.RequestBody(
    description="扩展信息", required = true,
    content = @Content(schema = @Schema(implementation = CreateUserReq.class),
      examples = @ExampleObject(value="{\"avatar\":\"https://.../a.png\"}")))
  @RequestBody CreateUserReq body) { ... }
```

最小注解示例（Node｜swagger-jsdoc）
```js
/**
 * @openapi
 * /api/user:
 *   post:
 *     tags: [user]
 *     summary: 创建用户
 *     description: 用于注册新用户：校验手机号唯一、发送欢迎消息；受限流与登录态校验。
 *     requestBody:
 *       required: true
 *       content:
 *         application/json:
 *           schema: { $ref: '#/components/schemas/CreateUserReq' }
 *           example: { phone: "13800000000", nick: "张三" }
 *     responses:
 *       '200':
 *         description: 创建成功
 *         content:
 *           application/json:
 *             schema: { $ref: '#/components/schemas/CreateUserResp' }
 *             example: { userId: 123 }
 *       '409':
 *         description: 冲突
 *         content:
 *           application/json:
 *             schema: { $ref: '#/components/schemas/ErrorResp' }
 *             example: { code: "USER_PHONE_EXISTS", message: "手机已注册" }
 */
```

落地与校验（建议）
- CI 在“生成 swagger.json”后增加 OpenAPI Lint（如 Spectral），至少检查以下规则：
  - Operation 缺失 `summary/description`；`parameters[].description` 缺失；Schema/字段缺 `description/example`；
  - 未声明错误响应；未声明 `security`（或明确无鉴权说明）。
- 生成产物统一存放：`build/swagger/swagger.json`，供后续上传到 YApi 的工作流直接读取。

### OpenAPI 文件优先工作流（组织级复用｜强烈推荐）
- 目标：在 CI 中稳定生成 `build/swagger/swagger.json` 并自动导入 YApi；不依赖运行服务暴露文档端口，适合网关/有安全限制/启动依赖重的服务。
- 组织级工作流：`chekdata/.github/.github/workflows/openapi-file-sync.yml@main`
- 分支到环境（默认）：`dev→dev`、`staging→staging`、`main→prod`

一）后端（Spring Boot 2.3/Spring Cloud Hoxton）最小改造
- 依赖（适配 2.3）：`springdoc-openapi-webflux-ui:1.5.12`、`springdoc-openapi-security:1.5.12`
- `openapi` Profile（禁用外部依赖、专用 4010 端口）
  - `src/main/resources/application-openapi.yml`（要点）
    - 关闭 Nacos：`spring.cloud.nacos.config.enabled=false`、`spring.cloud.nacos.discovery.enabled=false`
    - 关闭 JDBC/MyBatis 自动装配（如也在启动参数中传入亦可）
    - `server.port=4010`；`spring.main.allow-bean-definition-overriding=true`
  - 安全放行（仅 openapi）：定义 `SecurityWebFilterChain`，`authorizeExchange.anyExchange().permitAll()`，`csrf().disable()`
  - Nacos Stub（仅 openapi）：提供 `NacosDiscoveryProperties` Bean（空实现即可）
  - DataSource Stub（仅 openapi）：提供一个“无依赖”的 `DataSource` Bean（直接抛出 `SQLException`），避免引入 `spring-jdbc/h2`
    - 注意：不要依赖 `EmbeddedDatabaseBuilder`，否则需再引入额外依赖
- 启动参数（由脚本传入）示例：
  - 排除自动装配：`-Dspring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration,com.alibaba.cloud.nacos.NacosDiscoveryAutoConfiguration,com.alibaba.cloud.nacos.registry.NacosServiceRegistryAutoConfiguration`
  - Profile：`--spring.profiles.active=openapi`
  - JVM 内存：`JAVA_TOOL_OPTIONS="-XX:+UseContainerSupport -XX:MaxRAMFraction=2"`

二）导出脚本（仓库内，建议 `scripts/export_openapi.sh`）
  ```bash
#!/usr/bin/env bash
set -euo pipefail
mvn -B -DskipTests=true -DskipITs=true clean package
JAR=$(ls -1 target/*.jar | head -n1)
mkdir -p build/swagger
nohup bash -c "JAVA_TOOL_OPTIONS='-XX:+UseContainerSupport -XX:MaxRAMFraction=2' \
  SPRING_PROFILES_ACTIVE=openapi \
  SPRING_CLOUD_NACOS_CONFIG_ENABLED=false \
  SPRING_CLOUD_NACOS_DISCOVERY_ENABLED=false \
  SPRING_MAIN_ALLOW_BEAN_DEFINITION_OVERRIDING=true \
  java -Dspring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration,com.alibaba.cloud.nacos.NacosDiscoveryAutoConfiguration,com.alibaba.cloud.nacos.registry.NacosServiceRegistryAutoConfiguration \
  -Dserver.port=4010 -jar \"$JAR\" --spring.profiles.active=openapi > build/app.log 2>&1 &"
for i in $(seq 1 180); do
  if curl -fsS http://127.0.0.1:4010/v3/api-docs -o build/swagger/swagger.json; then exit 0; fi
  sleep 2
done
echo 'export failed'; tail -n 400 build/app.log || true; exit 1
```
- 可选 Makefile（统一 `make openapi`）：
```makefile
openapi:
	@echo "Generating OpenAPI spec..."
	mkdir -p build/swagger
	mvn -B -DskipTests=true -DskipITs=true -Popenapi springdoc-openapi:generate
	mv target/swagger.json build/swagger/swagger.json
```

三）CI 工作流（业务仓 `.github/workflows/openapi.yml`）
```yaml
name: openapi
on:
  push: { branches: [ main, dev, staging ] }
  workflow_dispatch:
jobs:
  prod:
    if: ${{ github.ref_name == 'main' }}
    uses: chekdata/.github/.github/workflows/openapi-file-sync.yml@main
    with:
      export_cmd: bash scripts/export_openapi.sh
      artifact_path: build/swagger/swagger.json
      service: backend-gateway-saas
      env: prod
      fail_on_lint: true
      checkout_ref: main
    secrets: inherit
  stg:
    if: ${{ github.ref_name == 'staging' }}
    uses: chekdata/.github/.github/workflows/openapi-file-sync.yml@main
    with:
      export_cmd: bash scripts/export_openapi.sh
      artifact_path: build/swagger/swagger.json
      service: backend-gateway-saas
      env: staging
      fail_on_lint: true
      checkout_ref: main
    secrets: inherit
  dev:
    if: ${{ github.ref_name == 'dev' }}
    uses: chekdata/.github/.github/workflows/openapi-file-sync.yml@main
    with:
      export_cmd: bash scripts/export_openapi.sh
      artifact_path: build/swagger/swagger.json
      service: backend-gateway-saas
      env: dev
      fail_on_lint: true
      checkout_ref: main
    secrets: inherit
```
- 必需 Secrets（仓库或组织层）：
  - `YAPI_BASE=https://yapi.chekkk.com`
  - `YAPI_TOKEN_DEV / YAPI_TOKEN_STG / YAPI_TOKEN_PROD`
- 可选（AI 富化已内置于 `yapi-sync-reusable.yml`，未提供 `LLM_API_KEY` 时自动跳过）：
  - `LLM_API_KEY`（可选）/（如需自定义）`LLM_ENDPOINT`、`LLM_MODEL`

四）YApi 导入实现（复用工作流已内置，了解机制便于排障）
- 请求体（application/json）：`{ "type": "swagger", "merge": "merge", "json": "<完整 OpenAPI JSON 字符串>" }`
- 关键点：使用 `jq --rawfile` 读取 `swagger.json` 为字符串字段，避免被解析为对象导致 400
  - 成功响应：`{"errcode":0,"errmsg":"成功导入接口 X 个, 已存在的接口 Y 个","data":null}`
- 组织工作流已内置容错：
  - 首发 application/json（rawfile）；
  - 若失败会尝试 form-urlencoded 兜底（`--data-urlencode "data=<payload json>"`，接口要求字段名为 `data`，内部包含 `type/merge/json`）。

五）常见错误与处理（本次实战沉淀）
- Maven 插件或版本不兼容（Spring Boot 2.3）：
  - `springdoc-openapi-maven-plugin` 高版本不兼容 → 改为运行服务+curl 或使用 1.6.x；推荐脚本方式启动后 `curl /v3/api-docs`
- 编译期缺包：
  - `OpenApiDataSourceStubConfig` 依赖 `spring-jdbc`/H2 → 切换为“无依赖 DataSource Stub”（直接抛异常的内联实现）
- 启动期 Bean 冲突：
  - `springSecurityFilterChain 已存在` → `spring.main.allow-bean-definition-overriding=true` 且将“放行”仅限定在 `openapi` profile
- 启动期外部依赖偶发 403/连接拒绝：
  - Nacos 相关 Bean 缺失 → 加 Stub 与 `@Profile("openapi")` 的 `@ComponentScan` 排除
  - Zipkin/外部依赖报错 → 允许抑制，不影响导出 `/v3/api-docs`
- CI 日志显示 Spectral `No ruleset has been found`：
  - 非阻塞（仅提示）；如需校验请在仓库根添加 `.spectral.yaml` 并在调用中指定 `lint_rules`
- YApi 400：
  - `"请求参数 data.json 应当是 string 类型"` → 使用 `jq --rawfile` 写入字符串字段或切换到 JSON 直传
  - `"请求参数 data 应当有必需属性 type"` → form 模式请传 `data=`（内含 `type/merge/json`），或改回 JSON 模式
- 已验证仓库（示例）：`backend-gateway-saas`（dev/staging 已成功导入，errcode=0）

六）最佳实践
- 文档质量：按“OpenAPI 内容与注释规范（必须）”补齐 `summary/description/examples` 与错误响应
- 产物路径：统一 `build/swagger/swagger.json`；避免仓库内多处路径漂移
- 触发策略：`push: [main, dev, staging] + workflow_dispatch`；首次可临时关闭 lint 严格失败，待补齐后再开启
- 变更可观测：工作流产出 `openapi-file-artifacts`（包含 `swagger.json`、YApi 响应、lint 输出），便于审计与回溯


## 网络与镜像治理实践（DNS/CoreDNS、出站、镜像仓库）

### 结论概览
- **CoreDNS**：dev/staging/prod 已稳定（prod 切换国内镜像并补齐 RBAC）。
- **出站策略**：为各集群节点安全组放通 TCP 80/443 与 TCP/UDP 53 后，镜像拉取、DNS 转发稳定。
- **镜像拉取**：
  - 公共镜像建议统一改用国内镜像代理（见下文映射表与脚本）。
  - 私有仓库需确保 kubelet 能解析 `cr.volces.com` 与 `chek-images-cn-beijing.cr.volces.com`（见“私有域名解析”）。
- **Argo CD**：组件 DNS 统一回归 `ClusterFirst`，移除历史 `hostAliases` 临时措施；Git 走 SSH:443 已验证（保留 keepalive 与超时配置）。

### CoreDNS 长期配置
- **镜像**：`m.daocloud.io/docker.io/coredns/coredns:1.11.1`（国内可达）。
- **RBAC**：补充 `discovery.k8s.io/v1 endpointslices` 的只读权限（ClusterRole/ClusterRoleBinding：`coredns-endpointslices`）。
- **Corefile（上游转发）**：
  ```
  .:53 {
    errors
    health { lameduck 5s }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 }
    prometheus :9153
    forward . 1.1.1.1 8.8.8.8 114.114.114.114 { max_concurrent 10000 }
    cache 30
    loop
    reload
    loadbalance
  }
  ```
- **私有域名解析（需平台侧提供权威 DNS）**：
  - 若 `cr.volces.com`/`chek-images-cn-beijing.cr.volces.com` 在公共 DNS 上为 NXDOMAIN，请提供 VPC 私网的权威 DNS IP；在 CoreDNS 增加条件转发（示例）：
    ```
    cr.volces.com:53 {
      forward . <INTERNAL_DNS_1> <INTERNAL_DNS_2>
    }
    chek-images-cn-beijing.cr.volces.com:53 {
      forward . <INTERNAL_DNS_1> <INTERNAL_DNS_2>
    }
    ```
  - 已确认（dev）：`100.96.0.2`、`100.96.0.3` 为私网权威 DNS，可用于上述条件转发。
  - 已确认（prod）：`100.96.0.2`、`100.96.0.3` 为私网权威 DNS（与 dev 一致），已在 CoreDNS 条件转发中使用。

### 节点出站（火山引擎安全组）
- **长期最小口径**：允许 TCP 443、TCP/UDP 53；如镜像源偶发 80 跳转，可临时放通 TCP 80。
- **自动化脚本**：`tmp/ve_fix_staging_egress.py`
  ```bash
  export VOLC_ACCESS_KEY_ID="<AK>" VOLC_SECRET_ACCESS_KEY="<SK>"
  python3 tmp/ve_fix_staging_egress.py <instanceId...>
  # 脚本会发现实例所属安全组，并为其授权 443/80/53 出站
  ```

### 私有镜像仓库凭据与绑定
- **目标**：将包含 `cr.volces.com/chek-images-cn-beijing.cr.volces.com` 的 dockerconfigjson Secret 复制到出现拉取失败的命名空间，并绑定默认 ServiceAccount。
- **脚本**：`tmp/auto_bind_private_registry.sh`
  ```bash
  KUBECONFIG=<.../kube.conf> tmp/auto_bind_private_registry.sh
  # 会自动寻找包含私有域名的 dockerconfigjson，创建目标 ns 的 registry-pull-secret 并 patch sa/default
  ```
- **ARC 控制器**：其 Deployment 使用 `serviceAccountName=arc-controller`，需为该 SA 绑定 `vecr-auth/registry-pull-secret`。

### 公共镜像国内代理映射与批量重写
- **推荐映射**：
  - `busybox:<tag>` → `docker.m.daocloud.io/library/busybox:<tag>`
  - `python:<tag>` → `docker.m.daocloud.io/library/python:<tag>`
  - `curlimages/curl:<tag>` → `docker.m.daocloud.io/curlimages/curl:<tag>`
  - `gcr.io/*` → `gcr.m.daocloud.io/*`
  - `ghcr.io/*` → `ghcr.m.daocloud.io/*`（若匿名 403，改国内镜像或同步至 VECR）
  - `registry.k8s.io/*` → `k8s.m.daocloud.io/*`
- **脚本**：`tmp/rewrite_public_images.sh`（仅作用于 Deployment/StatefulSet/DaemonSet）
  ```bash
  KUBECONFIG=<.../kube.conf> tmp/rewrite_public_images.sh
  ```

### ImagePullBackOff 统一体检
- **脚本**：`tmp/scan_imagepullbackoff.sh`
      ```bash
  KUBECONFIG=<.../kube.conf> CLUSTER_NAME=<NAME> tmp/scan_imagepullbackoff.sh
  ```

### Argo CD（稳定配置要点）
- **DNS 策略**：`repo-server`/`application-controller` 使用 `dnsPolicy: ClusterFirst`；避免 `hostAliases` 作为长期手段。
- **Git over SSH:443**：
  - `ARGOCD_EXEC_TIMEOUT=600s`
  - `ARGOCD_GIT_SHALLOW_FETCH=true`
  - `GIT_SSH_COMMAND="ssh -o ServerAliveInterval=15 -o ServerAliveCountMax=8 -o ConnectTimeout=30 -p 443"`
- **argocd-cm 最小配置**：
  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: argocd-cm
    namespace: argocd
  data:
    timeout.reconciliation: "180s"
  ```

### 已知问题与后续动作
- **staging**：
  - `frontend-app` 使用占位 tag `sha-REPLACEME` → 需更正为实际构建 tag。
  - `backend-saas` 某 tag 返回 404 → 修正为存在的 tag。
- **dev/prod**：
- `cr.volces.com/chek-images-cn-beijing.cr.volces.com` 在公共 DNS 为 NXDOMAIN → 需平台提供权威 DNS 并在 CoreDNS 配置条件转发（见“私有域名解析”）。
- **ghcr**：部分路径匿名 token 403 → 改国内镜像代理或将镜像同步到 VECR。

### 诊断手册（常用命令）
      ```bash
# CoreDNS 基本检查
kubectl -n kube-system get deploy coredns -o wide
kubectl -n kube-system logs deploy/coredns --tail=200

# Pod 内 DNS 验证（busybox）
kubectl -n <ns> run dns-test --image=docker.m.daocloud.io/library/busybox:1.36.1 -- sh -c 'sleep 3600'
kubectl -n <ns> exec dns-test -- nslookup kubernetes.default.svc.cluster.local
kubectl -n <ns> exec dns-test -- nslookup <private-registry-domain>

# ImagePullBackOff 扫描
KUBECONFIG=<.../kube.conf> CLUSTER_NAME=DEV tmp/scan_imagepullbackoff.sh

# 批量将公共镜像改为国内代理
KUBECONFIG=<.../kube.conf> tmp/rewrite_public_images.sh

# 自动为出现拉取失败的命名空间绑定私有仓库凭据
KUBECONFIG=<.../kube.conf> tmp/auto_bind_private_registry.sh
```

## 托管组件使用与迁移注意（统一口径）

- dns-autoscaler：
  - 作用：按节点/Pod 数动态调整 CoreDNS 副本。
  - 迁移：安装托管后，删除自建 `dns-autoscaler` 资源即可；无需业务改动。

- csi-nas + snapshot-controller：
  - 作用：提供 RWX 共享存储与卷快照。
  - 最小使用：创建 `StorageClass=nas-standard` → 创建 PVC（`accessModes=ReadWriteMany`，`storageClassName=nas-standard`）→ 工作负载挂载。
  - 迁移注意：先创建 PVC 并同步数据（Job/rsync），切换 Deployment 的 `volumeMounts` 至 PVC，验证后再下线旧卷。

- pod-identity-webhook（IRSA）：
  - 作用：为 Pod 注入 OIDC 凭证，最小权限访问云资源（如 TOS）。
  - 最小使用：创建带 `role-arn` 注解的 `ServiceAccount`，工作负载引用该 SA；应用使用官方 SDK 读取临时凭证。

- prometheus-agent（VMP 托管，prod 装）：
  - 作用：向托管工作区推送指标；支持 ServiceMonitor/PodMonitor/Probe。
  - 起步参数：vm-agent 0.5/2 Core，1/4 Gi；kube-state-metrics 0.2/0.8 Core，512Mi/2Gi；分片 1，开启扩缩容；安装 node-exporter。
  - 开始采集：在需采集对象上加 `metadata.labels.volcengine.vmp="true"`；端口名用 `http`，路径 `/metrics` 或 `/actuator/prometheus`。

- prometheus-adapter（prod 装）：
  - 作用：将 Prometheus 指标暴露为 HPA 可用的自定义/外部指标。
  - 使用：为目标指标加 `aggregated-metrics` 规则后，HPA 引用 `metrics: type: Pods/External`。

- keda/karpenter：
  - 作用：事件驱动伸缩/按需扩容节点。
  - 建议：若工作负载依赖队列/定时扩缩，用 KEDA；需要更快/更经济的节点扩缩，用 Karpenter 并评估 Spot 策略。

- node-problem-detector：
  - 作用：将节点硬件/内核问题上报为事件/条件，便于告警与自动化。

 - Ingress 迁移到 ALB（统一流程）：
  1) 服务 `Service` 端口命名为 `http`，`targetPort` 指向容器端口。
  2) Ingress `backend.service.port.number: 80`，`spec.ingressClassName: chek-<env>-alb`。
  3) 若有 NGINX 专有注解（regex/rewrite），改为前缀路由或在上游网关实现。
  4) 验证无误后删除手动 `ingress-nginx` 释放 CLB/EIP。

### 各环境托管组件现状（以控制台为准）
- dev：
  - 已装：alb-ingress-controller、metrics-server、dns-autoscaler、csi-nas、snapshot-controller、pod-identity-webhook、karpenter、node-problem-detector
  - 待办：按计划统一切换 Ingress 到 ALB（端口名 http → ingressClassName=chek-<env>-alb）
- staging：
  - 已装：alb-ingress-controller、metrics-server、dns-autoscaler、csi-nas、snapshot-controller、pod-identity-webhook、karpenter、node-problem-detector
  - 待办：同 dev
- prod：
  - 已装：alb-ingress-controller、metrics-server、dns-autoscaler、csi-nas、snapshot-controller、pod-identity-webhook、karpenter、node-problem-detector、prometheus-agent、prometheus-adapter
  - 待办：完成剩余服务从 NGINX → ALB 的切换；按需为 OSM 等工作负载绑定 NAS PVC；为需要访问 TOS 的服务完成 IRSA 绑定


### dev 集群落地实战（frontend-app）操作清单（含控制台联动）
- 场景背景
  - 网络模型：Flannel（暂不迁移 VPC‑CNI），采用 NodePort + ALB target‑type=instance。
  - 域名：`app-dev.chekkk.com`（DNS 使用 CNAME 指向 ALB 域名）。
  - ALB 实例：`alb-xp0pn039hxq854ov5ex0477i`（当前公网 IP `101.126.134.44`，仅作参考，可能变化）。
  - VPC 私网权威 DNS（用于私有域转发）：`100.96.0.2`、`100.96.0.3`。

- 镜像拉取（强制遵循）
  - 不改节点 containerd 配置；禁止下发任何修改 CRI 的 DaemonSet/SSH 操作。
  - 启用并绑定“容器镜像免密组件（cr‑credential‑controller）”，在 VECR 实例「集群免密拉取/绑定集群」中：
    - 绑定 dev 集群，授权项目 `dev/staging/prod`（pull）。
    - 命名空间可用 `*`，ServiceAccount 用 `*`（或应用实际 SA）。
  - Helm/Workload 不要硬编码 `imagePullSecrets`。如已写死，务必移除，交由 cr‑credential‑controller 注入。否则会导致 401（实际遇到过）。若需兼容无免密组件的集群，亦通过 Kyverno 统一注入 `vecr-regcred`，禁止手写。
  - 如私有域名在公网不可解析，CoreDNS 加条件转发到 VPC 权威 DNS（示例见“CoreDNS 长期配置/私有域名解析”小节）。

- Service/Ingress（GitOps）
  - Service
    - `type: NodePort`，端口：`name: http`、`port: 80`、`targetPort: 3000`，NodePort 例：`32433`。
  - Ingress
    - `spec.ingressClassName: chek-<env>-alb`
    - 注解最小集（Flannel/instance 模式）：
      - `ingress.vke.volcengine.com/target-type: instance`
      - `ingress.vke.volcengine.com/address-type: internet`
      - `ingress.vke.volcengine.com/backend-protocol: HTTP`
      - 可选绑定已有实例：`ingress.vke.volcengine.com/alb-instance-id: alb-xp0pn039hxq854ov5ex0477i`
    - 后端端口使用数值：`backend.service.port.number: 80`（Admission 校验对数字更友好）。
  - Argo CD 应用参数：确保 `ingress.className=chek-<env>-alb`，避免误写为其它自定义类名。

### MySQL / RDS 接入规范（统一说明）
- 总体原则：
  - 优先使用托管 RDS（如 `mysql-1c2f69d18fde.rds.ivolces.com`），应用代码只关心 **环境变量名**，不直接写死账号密码；
  - 各环境（dev/staging/prod）可以共享逻辑库（如 `core_user`），但生产库必须单独做备份与变更审计。
- 标准做法：
  - 每个微服务一个专用 DB Secret，命名推荐：`<service>-db`，例如 `backend-auth-saas-db`；
  - Secret 中至少包含：
    - `DB_URL`：如 `jdbc:mysql://mysql-1c2f69d18fde.rds.ivolces.com:3306/core_user?useUnicode=true&characterEncoding=utf-8&createDatabaseIfNotExist=true&useSSL=false&allowPublicKeyRetrieval=true&allowMultiQueries=true`；
    - `DB_UID`：数据库账号（当前为共享的 `developer`，后续建议逐步收敛为每服务专用账号，如 `auth_saas_app`）；
    - `DB_PWD`：对应密码；
  - 应用侧配置只保留占位默认值，例如：
    - `url: ${DB_URL:jdbc:mysql://localhost:3306/core_user?useUnicode=true&characterEncoding=utf-8&createDatabaseIfNotExist=true&useSSL=false&allowPublicKeyRetrieval=true&allowMultiQueries=true}`；
    - `username: ${DB_UID:CHANGE_ME_DB_USER}`；
    - `password: ${DB_PWD:CHANGE_ME_DB_PASSWORD}`；
  - Helm values 通过 `envFromSecrets` 挂载 Secret，例如：
    - `envFromSecrets: [nacos-cred, redis-cred, backend-auth-saas-db]`。
- `backend-auth-saas` 专用规范：
  - 命名空间：`saas-<env>`，当前使用库：`core_user`；
  - Secret 名称：`backend-auth-saas-db`，键固定为 `DB_URL/DB_UID/DB_PWD`；
  - 业务仓 `application.yml` 已改为只引用 `${DB_URL/DB_UID/DB_PWD}`，不再保留明文密码，后续如需更换 RDS 或拆分库，只需调整各环境的 Secret，无需改代码。

- 控制台联动操作（ALB）
  1) 验证 ALB 实例
     - 实例已绑定公网 EIP，入站安全组放通 TCP 80/443。
  2) 新建监听器（HTTP:80）
     - 监听协议/端口：HTTP / 80。
     - 默认后端服务器组：选择“实例模式（Instance）”的目标组（见下一步）。
  3) 新建目标组（实例模式）
     - 名称：`tg-frontend-app-dev-32433`
     - 通信协议：HTTP
     - 健康检查：自定义
       - 协议：HTTP，方法：GET
       - 域名：`app-dev.chekkk.com`
       - 路径：`/`
       - 成功状态码：勾选 2XX、3XX
       - 超时：2s，间隔：2s，健康阈值：3，不健康阈值：3
     - 注册后端：
       - 选择集群三台节点（如 `172.31.120.15/172.31.121.103/172.31.121.107`）
       - 端口：`32433`，权重：`100`
     - 将该目标组设置为监听器 :80 的“默认后端”。
  4) 安全组
     - ALB 安全组：入站放通 TCP 80（如需 443 同理）。
     - 节点安全组：入站允许“来源=ALB 安全组”的 TCP `32433`（或放通 NodePort 范围 30000‑32767，按基线收敛）。
  5) DNS 解析
     - 解析 `app-dev.chekkk.com` → CNAME 到 ALB 域名（如 `alb-...cn-beijing.volcenginealb.com`）。不建议直接 A 到 IP。

- 验证（内外一致）
  - 集群内（已验证返回 200）：
    - `kubectl -n app-dev run curl-once --image=docker.m.daocloud.io/curlimages/curl:8.10.1 --restart=Never -- curl -s -o /dev/null -w '%{http_code}\n' -H 'Host: app-dev.chekkk.com' http://<ALB_IP>/`
  - 外部：
    - `curl -H 'Host: app-dev.chekkk.com' http://<ALB_IP>/` 或直接访问 `http://app-dev.chekkk.com/`
  - E2E 脚本：`scripts/e2e/dev_e2e_check.sh`（包含 DNS 解析、镜像拉取、NodePort 与 Ingress 连通）。
  - 注：VKE 现网偶见 Ingress `.status.loadBalancer` 不回填，但只要 ALB 监听/目标组健康，转发即生效。

- 常见坑与结论（本次实战）
  - IngressClass 误写为自定义类名，控制器不接管 → 统一用 `alb`。
  - Ingress 后端端口使用名字在 Admission 校验失败 → 改为 `number: 80`。
  - Workload 写死 `imagePullSecrets` 覆盖了 cr‑credential‑controller 注入 → 导致 401，移除后恢复。
  - ALB `<pending>` 多为“节点 SG 未放通 NodePort 或监听器/目标组未建” → 按上文控制台步骤补齐。
  - 严格禁止修改 containerd；排障顺序：安全组/NAT → CoreDNS → Helm/Ingress/ALB → 不碰节点 CRI。

## 微服务接入网关（Spring Cloud Gateway）规范

- 路由约定
  - 统一前缀：`/api/<service>/...`。示例：`/api/vms/**`、`/api/offline/**`。
  - SCG 路由（通过环境变量注入）：
    - `SPRING_CLOUD_GATEWAY_ROUTES_0_ID=<service>`
    - `SPRING_CLOUD_GATEWAY_ROUTES_0_URI=http://<k8s-svc>.<ns>.svc.cluster.local:80`
    - `SPRING_CLOUD_GATEWAY_ROUTES_0_PREDICATES_0=Path=/api/<service>/**`
    - `SPRING_CLOUD_GATEWAY_ROUTES_0_FILTERS_0=StripPrefix=2`（保留 `/api/<service>` 后的真实路径）
  - OpenAPI 直达（可选）：如果需要将 `/api/<service>/openapi.json` 映射为下游 `/openapi.json`：
    - 额外加一条 Route：`Path=/api/<service>/openapi.json` + `SetPath=/openapi.json`。

- Ingress 规范（ALB）
  - `spec.ingressClassName: chek-<env>-alb`，target-type 使用实例模式（`ingress.vke.volcengine.com/target-type: instance`）。
  - 业务入口仅保留一份规则，避免相同 path 在不同 Ingress 重复导致冲突（例如 `/osm-gateway/healthz`）。
  - 后端统一指向 `backend-gateway-saas` 的 Service:80，路径前缀使用 `Prefix`：
    - `/api/<service>/` → `backend-gateway-saas:80`
  - 健康检查路径建议：`/actuator/health`（由网关对外暴露，后端服务自检不直接暴露公网）。

- 安全与白名单
  - 网关使用白名单优先放行。配置来源优先使用 `SPRING_APPLICATION_JSON`：
    - `{"secure":{"ignore":{"urls":["/api/<service>/healthz","/api/<service>/actuator/**","/api/offline/**","/openapi.json","/openapi","/v3/api-docs","/v3/api-docs/**","/actuator/**"]}}}`
  - 建议最小放行：`/healthz`、`/actuator/**`、`/openapi.json`；其他接口需要鉴权。
  - 显式路径放行兜底（避免配置未生效时误拦截）：网关代码已对 `/api/vms/healthz`、`/api/vms/actuator/**`、`/actuator/**`、`/openapi.json` 做 permitAll。

- 网关全局过滤器
  - ResponseBodyFilter：默认关闭，仅按需开启；避免对大/复杂 JSON（如 OpenAPI）进行重序列化。
  - CacheBodyGlobalFilter：默认关闭，仅按需开启。
  - RequestLogFilter：支持全局开关与路径绕过（openapi、actuator、离线接口），避免日志影响链路。

- 超时与内存
  - `SPRING_CODEC_MAX_IN_MEMORY_SIZE=64MB`（视业务 JSON 体积调整）。
  - `SPRING_CLOUD_GATEWAY_HTTPCLIENT_CONNECT_TIMEOUT=8000`（毫秒）。
  - `SPRING_CLOUD_GATEWAY_HTTPCLIENT_RESPONSE_TIMEOUT=120s`（按上游响应特性设定）。

- Nacos 与服务发现
  - 生产仅启用 Nacos Discovery（`SPRING_CLOUD_NACOS_DISCOVERY_ENABLED=true`），Config 默认关闭。
  - 需配置：`SPRING_CLOUD_NACOS_DISCOVERY_SERVER_ADDR`、`NAMESPACE`、`USERNAME/PASSWORD`。
  - 服务注册名建议：`<team>-<service>-service`，端口按容器端口暴露。
  - 若不启用发现，路由 `URI` 使用 K8s SVC FQDN。

- CI/CD 与环境
  - CI：main 分支推送 prod+dev 镜像，其他分支推 dev。镜像 tag：`sha-<short>`。
  - CD：每个环境一个 Argo 应用（dev/staging/prod），推荐启用 Argo Image Updater 自动跟随 tag。
  - Helm 参数注入路由与白名单，避免写死在应用配置中。

- 观测与排障
  - 临时打开 TRACE：`logging.level.org.springframework.cloud.gateway=TRACE`、`reactor.netty.*=TRACE`。
  - 快速验证（均加超时）：`curl --max-time 20 -i https://api.<domain>/api/<service>/healthz`、`openapi.json`。
  - Pod 侧验证：`kubectl -n <ns> port-forward deploy/backend-gateway-saas 12000:12000` 后本地访问。

- 资源与配额
  - 建议 requests：`cpu: 200m`、`memory: 256Mi`；limits 依据场景（至少 1Gi）。
  - 发布前检查命名空间 `ResourceQuota`，避免因总量耗尽导致调度失败。



