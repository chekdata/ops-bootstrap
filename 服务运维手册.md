服务运维手册（frontend-miker｜dev/staging/prod）

一、概览
- 架构：Next.js（端口 3000）→ Service NodePort:80→3000 → Ingress（ALB，target-type=instance）。
- 环境与域名：
  - dev：`miker-dev.chekkk.com`
  - staging：`miker-staging.chekkk.com`
  - prod：`miker.chekkk.com`（80=301→443，业务只在 443）

二、日常发布（PR 合并自动发布）
1) 合并到 dev/staging/main → Actions 触发 `ci`：构建并推 VECR（tags：`sha-<short>`，dev/staging 追加 `*-latest`）。
2) `deploy` 通过 `workflow_run`（ci 成功后）自动部署目标环境：
   - 等待镜像可用（manifest inspect 重试）
   - 命名空间自动创建（`miker-<env>`）
   - 应用 Service/Deployment/Ingress（NodePort/instance 模式，端口 3000）
3) 验证：
   - `kubectl -n miker-<env> rollout status deploy/frontend-miker`
   - `curl -I http(s)://<域名>/`（dev 可能 307，staging/prod 200）

三、回滚
1) 找到上一个稳定镜像：Actions → `ci` 运行 → 复制 `sha-<short>`。
2) 临时回滚（kubectl）：
   ```bash
   kubectl -n miker-<env> set image deploy/frontend-miker app=chek-images-cn-beijing.cr.volces.com/prod/frontend-miker:sha-<short>
   kubectl -n miker-<env> rollout status deploy/frontend-miker
   ```
3) 持久化：将该 sha 提交到部署清单/工作流参数（或通过 CI 重新发布）保持一致性。

四、故障排查（快捷清单）
- 503/超时
  - `kubectl -n miker-<env> get po,svc,ing` → Deployment Ready=1/1？
  - `kubectl -n miker-<env> get endpoints frontend-miker -o wide` → 容器端口=3000？
  - `kubectl -n miker-<env> describe ing frontend-miker` → 注解为 `target-type: instance`/`address-type: internet`；ALB 事件无健康检查错误？
  - 节点安全组：是否放通“ALB 安全组 → NodePort 段（30000–32767/TCP 或实际端口）”
- 容器探针失败
  - `kubectl -n miker-<env> logs deploy/frontend-miker --tail=200`
  - 探针路径/端口对齐 3000，镜像是否对应最新 sha
- 域名异常
  - `nslookup <域名>` → 指向 ALB FQDN（CNAME），非 A 到 EIP
  - `curl -H 'Host: <域名>' http://<ALB-IP>/ -I` → 验证 80/443 路由是否命中预期后端

五、飞书与 YApi（自动链路）
- 飞书拉群+通知（组织级）
  - 条件：业务仓 Secrets 配置 `FEISHU_CHAT_ID_PROD`、`GITOPS_PAT`
  - 业务仓 `notify-dispatch.yml` 在 CI/Deploy 成功后自动 `repository_dispatch` 到组织仓；组织仓入口工作流完成“成员同步入群 + 卡片通知”
  - 验证：组织仓 `.github` → Actions → Org Feishu Sync Members（`sync=success`、`notify=success`；Artifacts 三份诊断文件）
- YApi 同步
  - 建议接入组织级工作流：CI 成功后自动 dispatch 导入 OpenAPI
  - 验证：`https://yapi.chekkk.com/project/<id>/interface/api` 能看到最新接口；Actions 日志包含 “YApi synchronization successful.”

六、镜像仓库（VECR）治理
- 仓库：`chek-images-cn-beijing.cr.volces.com/prod/frontend-miker`
- 保留策略：`dev-latest`、`stg-latest` 与最近 15 个 `sha-*`；更旧 `sha-*` 定期清理（不触碰自定义标签）
- 审计脚本：只读列出与清理计划，默认不删除；需要时逐条删除（保守执行）

七、常用命令（速查）
```bash
# 发布状态
kubectl -n miker-<env> get deploy,po,svc,ingress
kubectl -n miker-<env> rollout status deploy/frontend-miker

# Ingress/ALB 诊断
kubectl -n miker-<env> describe ing frontend-miker | sed -n '1,160p'

# Service/Endpoints
kubectl -n miker-<env> get svc frontend-miker -o wide
kubectl -n miker-<env> get endpoints frontend-miker -o yaml | sed -n '1,80p'

# 覆盖镜像（临时回滚/前推）
kubectl -n miker-<env> set image deploy/frontend-miker app=chek-images-cn-beijing.cr.volces.com/prod/frontend-miker:sha-<short>

# 外部验证
curl -I -L --max-time 10 http://miker-<env>.chekkk.com/
```

八、SLO 与告警（建议）
- 可用性：月度 ≥ 99.9%（业务 200/3xx）；探针/页面失败率阈值告警
- 部署：rollout 超时/失败自动告警；自动回滚开关按业务重要度评估
- 入站：ALB 目标组不健康数、5xx/4xx（维度：Listener/Rule/ServerGroup）

---

服务运维手册（backend-gateway-saas｜Dev/Staging/Prod）

一、概览与职责
- 架构：Spring Cloud Gateway（容器端口 12000）→ Service `backend-gateway-saas:80→12000` → Ingress/ALB → 下游微服务（`vehicle-model-service`、`osm-gateway`、`backend-miker-offline` 等）。
- 域名：
  - dev：`api-dev.chekkk.com` → 命名空间 `dev`，Release `backend-gateway-saas`。
  - staging：`api-staging.chekkk.com`（预留）→ 命名空间 `staging`。
  - prod：`api.chekkk.com` → 命名空间 `miker-prod`。
- 职责：统一入口（路由/鉴权/限流），统一暴露健康检查与 OpenAPI，并注册到 Nacos。

二、标准工作流
- `ci.yml`（主 CI）
  - 触发：`push` 到 `main/dev/staging`，或手动 `workflow_dispatch`。
  - 行为：`mvn clean package` → 使用 Buildx 推镜像到 VECR（`dev/staging/prod/backend-gateway-saas:sha-<short>`）→ 可选按分支推 OpenAPI 到 YApi。
  - 运维要点：发布前确认对应 sha 镜像已存在于 VECR，否则容易出现 Pod `ImagePullBackOff`、Service 无 Endpoints。
- `deploy-dev.yml` / `deploy-staging.yml` / `deploy-prod.yml`
  - 触发：对应分支 push + 手动 `workflow_dispatch`。
  - 行为：使用 `kubectl + helm` 对 `dev/staging/miker-prod` 命名空间执行 `helm upgrade --install`，values 中将镜像 tag 固定为当前 `sha-<short>`。
  - 运维要点：
    - **当前生产环境实际以 ArgoCD Application `backend-gateway-saas-prod` 为真源**，Actions 中的 `deploy-prod.yml` 主要用于排障/验收 Chart，不是唯一发布路径；
    - 若 `deploy-prod.yml` 在 Helm 安装阶段因 NetworkPolicy 所有权冲突报错（例如 `NetworkPolicy "backend-gateway-saas-default-deny" exists and cannot be imported into the current release`），说明该资源已由 ArgoCD 管理，此类错误不影响线上流量，**禁止通过删除现有 NetworkPolicy 来“修绿”流水线**；
    - 无论通过哪条路径发布，若发布后出现 503/超时，仍应优先检查目标命名空间中该 Release 的 Deployment Ready 状态与 Service Endpoints 是否为 0。
- `yapi-sync-build.yml`
  - 触发：`push` 到 `main/dev/staging` 或手动。
  - 行为：本地启动应用（禁用 Nacos/DB 自动配置），在 4010 端口导出 OpenAPI，然后调用 YApi 的 `api/open/import_data` 导入。
  - 运维要点：失败多为 `YAPI_BASE/YAPI_TOKEN_*` 未配置或 token 与 YApi 项目不匹配。
- `yapi-sync-incluster.yml`（仅运维）
  - 触发：手动 `workflow_dispatch`。
  - 行为：使用生产 kubeconfig 在 `platform-prod` 内创建临时 Job，从 `backend-gateway-saas.miker-prod.svc.cluster.local` 拉取 OpenAPI 并直接调用 `yapi.platform-prod` Service 导入。
  - 适用场景：希望 YApi 文档与线上网关的实际路由一键对齐时使用。

三、常见故障与排查
- ErrImagePull / 无 Endpoints
  - 现象：Deployment `backend-gateway-saas` Pod 状态 `ImagePullBackOff`，Service `backend-gateway-saas` `ENDPOINTS <none>`，通过网关访问任意路径均连接失败（curl 000）。
  - 排查：`kubectl -n <ns> describe pod backend-gateway-saas-...` 检查镜像 tag；在 VECR 中确认该 tag 是否存在；如刚合并 main，确认 `ci.yml` 已成功完成。
- 健康检查 403 / OpenAPI 503
  - 现象：`/api/vms/healthz`、`/api/osm-gateway/healthz` 或 `/api/offline/openapi.json` 通过网关访问返回 403/503。
  - 排查：确认 `SecurityConfiguration` 放行 `/actuator/**`、`/api/**/healthz`、`/**/openapi.json`；查看网关路由是否正确使用 `StripPrefix/SetPath` 指向下游 `/actuator/health` 或 `/openapi.json`。
- HTTP→HTTPS 行为异常
  - 现象：`http://api.chekkk.com` 返回 404 或直接命中业务服务。
  - 规范：同一域名的 80 Ingress 只允许一个“全域 301 重定向入口”，业务 Ingress 仅在 443 上暴露带前缀的业务路径（如 `/api/vms/**`、`/api/osm-gateway/**`）。
  - 排查：若发现多个命名空间声明同一 host+80，则以网关所在命名空间的 redirect Ingress 为准，其他服务只保留 443 规则。

四、Nacos 与配置
- 地址：通过 `SPRING_CLOUD_NACOS_DISCOVERY_SERVER_ADDR` 指向平台 Nacos（如 `172.31.120.5:8848`），各环境以命名空间隔离：`NACOS_NAMESPACE=miker-<env>`。
- 账号：显式注入 `SPRING_CLOUD_NACOS_DISCOVERY_USERNAME/PASSWORD`，生产环境禁止匿名。出现 403 `username and pwd is blank!` 时，先检查这两项是否正确配置。
- 健康与实例：遇到注册异常时，优先查看 Nacos 控制台中对应服务的实例列表与心跳状态，再对照网关 Pod 日志。

五、日常运维小结
- CI 视角：优先关注 `ci.yml` 与对应环境的 `deploy-*.yml`；YApi 相关工作流失败通常不会影响转发，但会导致文档不同步。
- 集群视角：发布异常时，按顺序检查 Deployment → Service Endpoints → Ingress；出现 `no endpoints available for backend-gateway-saas` 多为 Pod 未 Ready 或 selector 不匹配。
- 文档视角：YApi 项目名统一为 `backend-gateway-saas-<env>`；如接口或描述过旧，可通过 `yapi-sync-build.yml` 或 `yapi-sync-incluster.yml` 做一次全量刷新。
- 分支与工作流治理：线上仅保留 `ci.yml`、`deploy-dev.yml`、`deploy-staging.yml`、`deploy-prod.yml`、`yapi-sync-build.yml`、`yapi-sync-incluster.yml` 六条与网关相关的工作流；历史调试用工作流/分支（如 `ci/yapi-timeouts-*`、`ci/yapi-fallback-*` 等）已统一清理。运维在排查 CI/CD 问题时，**只需要关注上述六条工作流与 `main/dev/staging` 三个长期分支**，其它短期实验分支应在使用结束后尽快删除，避免后续混淆“真正的发布路径`。

---

服务运维手册（backend-saas｜Dev/Staging/Prod）

一、概览
- 技术栈：Spring Boot（容器端口 `12000`），应用名 `chek-benchmark-service`。
- 命名空间与资源：
  - dev：`saas-dev`，Deployment/Service `backend-saas`。
  - staging：`saas-staging`。
  - prod：`saas-prod`。
- 对外访问：
  - 通过 Ingress/ALB 暴露到 `api-<env>.chekkk.com` / `api.chekkk.com`，健康检查路径统一为 `/actuator/health`（容器内直连 `http://127.0.0.1:12000/actuator/health` 应返回 200）。
  - 业务流量通常通过 `backend-gateway-saas` 转发；如需直接访问服务，可在集群内使用 Service FQDN：`http://backend-saas.saas-<env>.svc.cluster.local:12000/`。

二、CI/CD 与版本策略
- 工作流概览（GitHub Actions）：
  - `ci-build-and-push`（`.github/workflows/ci.yml`）：
    - 触发：`push` 到 `main/dev/staging`，或手动 `workflow_dispatch`。
    - 行为：构建 jar，使用 Jib 将镜像推送到 VECR（为适配自托管 Runner 上 Jib 首次冷启动，job 级 `timeout-minutes` 已提升到 `40` 分钟，本轮最差冷启动构建约 33 分钟完成）：
      - `dev` 分支 → `chek-images-cn-beijing.cr.volces.com/dev/backend-saas:dev`；
      - `staging` 分支 → `chek-images-cn-beijing.cr.volces.com/dev/backend-saas:staging`；
      - `main` 上的 `vX.Y.Z` tag → `chek-images-cn-beijing.cr.volces.com/prod/backend-saas:vX.Y.Z` 与别名 `:prod`；
      - 未打 tag 的 `main` 只构建不推镜像（避免产生不可追溯的临时 tag）。
    - 容错：针对 Daocloud 镜像代理偶发 `UnexpectedBlobDigestException`，工作流对 Jib 构建增加一次有限重试，减少因上游 flakiness 造成的假红灯。
  - `deploy-helm`（`.github/workflows/deploy.yml`）：
    - 触发：`push` 到 `dev` / `staging`，或手动 `workflow_dispatch`。
    - 行为：根据分支选择 `KUBECONFIG_DEV` / `KUBECONFIG_STG`，在 `saas-dev/saas-staging` 命名空间执行 `helm upgrade --install backend-saas charts/backend-saas`，镜像始终指向当前环境固定标签（`dev` 或 `staging`），并在部署后执行 `kubectl rollout status deploy/backend-saas`。
  - `deploy-helm-prod`（`.github/workflows/deploy-prod.yml`）：
    - 触发：仅在 `main` 上打 `vX.Y.Z` tag 时触发。
    - 行为：使用 `KUBECONFIG_PROD` 在 `saas-prod` 命名空间执行 prod 部署，镜像标签为该 tag（`vX.Y.Z`），Ingress host 固定 `api.chekkk.com`；在执行 Helm 之前，先以带认证的 `curl` 轮询 Registry v2 manifest（`/v2/prod/backend-saas/manifests/vX.Y.Z`），最长等待 45 分钟，确认镜像真实存在后再 `helm upgrade --install`，从根本上消除 “CD 抢在 CI 推完镜像前触发” 导致的 `image not found/ImagePullBackOff` 竞态。
  - `openapi`（`.github/workflows/openapi.yml`）：
    - 触发：`push` 到 `main/dev/staging`，或手动 `workflow_dispatch`。
    - 行为：调用仓库内 `scripts/export_openapi.sh` 生成 `build/swagger/swagger.json`，使用 Spectral 进行 lint，并上传为 artifact `openapi-file-artifacts`；随后复用组织级 `yapi-sync-reusable.yml`，根据分支将 OpenAPI 导入对应环境的 YApi 项目（默认开启 LLM 富化）。

- 调试/排障型工作流（仅手工使用）：
  - `ci-openapi-main`：历史兼容工作流，只在需要针对某个 main commit 手工导出 OpenAPI 时使用；触发时必须在输入参数中显式填入 `confirm=yes` 才会执行，避免误点。
  - `openapi-inline`：一体化“导出+导入 YApi”的排障工具，仅当 `openapi.yml` + 组织级复用工作流链路难以排查时使用；仅允许 `jasonhong` 或 `chekkk-bot` 手工触发。
  - `diag-min`：简单的 runner 自检（打印时间与内核信息），用于排查 Runner 环境问题；同样限制触发人。
  - `ops-dns-check`：在指定环境命名空间中运行临时 Pod，检查集群内对 VECR/DockerHub 以及公共 DNS 的连通性（`nslookup` + `wget`）；仅在需要排查网络/DNS 问题时由运维手动触发。
- 运维建议：
  - 日常排查 CI/CD 问题时，优先关注上面四条“主链路工作流”（`ci-build-and-push` / `deploy-helm` / `deploy-helm-prod` / `openapi`）以及 `main/dev/staging` 三个长期分支。
  - 与 CI/部署相关的临时实验，一律通过在现有工作流中增加参数/条件来实现；短期调试分支在合并验证稳定后应在 1–2 周内清理，避免日后排查时出现“同名工作流但行为不同”的情况。

三、健康检查与常见故障
- 集群内健康检查：
  - 查看发布状态：
    ```bash
    kubectl -n saas-<env> get deploy,po,svc backend-saas
    kubectl -n saas-<env> rollout status deploy/backend-saas
    ```
  - 容器内检查健康：
    ```bash
    kubectl -n saas-<env> exec deploy/backend-saas -- \
      curl -sS -o /dev/null -w '%{http_code}\n' http://127.0.0.1:12000/actuator/health
    ```
- 只读根文件系统与日志目录：
  - backend-saas 的 Pod 默认启用 `readOnlyRootFilesystem`，Chart 中为 `/tmp` 与 `/logs` 挂载了 `emptyDir` 卷，以避免 Spring Boot / Log4j2 因无法创建临时目录或日志文件而启动失败。
  - 若在日志中看到 “Unable to create tempDir. java.io.tmpdir is set to /tmp” 或 “Could not create directory /logs” 一类报错，优先确认 Deployment 的 `volumeMounts` 与 `volumes` 是否按模板存在，或是否被手工改动。
- Redis 健康检查：
  - 集群内访问 Redis 主机（尤其是跨 VPC 或托管 Redis）时可能出现 DNS 解析失败或超时，导致 readiness probe 一直失败、Pod 无法 Ready。
  - 处理方式：在配置中关闭 Redis 健康检查（`management.health.redis.enabled=false`），或通过环境变量 `MANAGEMENT_HEALTH_REDIS_ENABLED=false` 注入（例如在 `backend-saas-env` Secret 中添加该键），让探针只依赖核心组件健康；Redis 本身的状态通过指标与业务错误率监控。
- Nacos 配置与认证：
  - backend-saas 通过 `bootstrap.yml` 将 `NACOS_USERNAME/NACOS_PASSWORD` 环境变量映射到 `spring.cloud.nacos.config.username/password` 与 `spring.cloud.nacos.discovery.username/password`，避免出现 403 `username and pwd is blank!`。
  - 运维验证步骤：
    1) 在对应环境的 Nacos 控制台中，查看 `chek-benchmark-service` 服务是否在预期命名空间下有实例且心跳正常；
    2) 若实例缺失或不健康，查看 Pod 日志中与 Nacos 相关的错误（认证失败、连通性问题等），优先检查 `NACOS_SERVER_ADDR/NACOS_NAMESPACE/NACOS_USERNAME/NACOS_PASSWORD` 的配置是否正确。

四、VECR 标签与清理
- 镜像仓库与标签策略：
  - dev/staging：镜像统一存放在 `chek-images-cn-beijing.cr.volces.com/dev/backend-saas` 仓库，仅保留 `:dev` 与 `:staging` 标签，对应环境分支；不再额外推 `sha-*` 或 `*-latest`。
  - prod：镜像存放在 `chek-images-cn-beijing.cr.volces.com/prod/backend-saas` 仓库，仅在 `main` 上打 `vX.Y.Z` tag 时推送 `:vX.Y.Z` 与别名 `:prod`；其它 tag 禁止占用该仓库。
- 后台清理脚本（按需执行）：
  - 在 `ops-bootstrap` 仓中提供了针对 backend-saas 的标签清理脚本 `scripts/vecr_prune_backend_saas.py`，用于避免 VECR tag 数量爆表：
```bash
    cd /Users/jasonhong/Desktop/CICD/ops-bootstrap
    export REGISTRY_USERNAME='<VECR 用户名>'
    export REGISTRY_PASSWORD='<VECR 密码>'
    python3 scripts/vecr_prune_backend_saas.py
    ```
  - 行为说明：
    - 仅访问 `dev/backend-saas` 与 `prod/backend-saas` 两个仓库；
    - 列出当前所有 tag，打印 `[INFO] 仓库 tags` 列表；
    - 对于 `dev/staging/prod` 和 `vX.Y.Z` 标签打印 `[KEEP]` 并保留；
    - 对于其它标签（如历史 `sha-*` 或临时测试 tag）调用 VECR 的 manifest 删除接口，打印 `[DEL]`；
    - 若仓库不存在（例如 prod 尚未创建），打印 `[WARN] skip ...` 并跳过。
  - 运维建议：执行前先确认当前 Deployment/Helm values/Nacos 配置中引用的镜像 tag，确保不会删除正在使用或最近一次可回滚的版本；如需针对其它服务做类似清理，可参考该脚本实现，避免写“全局删除 sha-*”的危险脚本。

五、本轮验证结果速记
- **Dev（`saas-dev`）**：
  - 通过 `ci-build-and-push` → `deploy-helm` 最新一轮完整跑通，Deployment `backend-saas` 为 `1/1 Running`，容器内 `http://127.0.0.1:12000/actuator/health` 返回 200；
  - 从公网 `https://api-dev.chekkk.com/api/backend-saas/healthz` 访问因 ALB/网络出口限制出现超时，属于基础设施层问题，不影响集群内与网关内部路由的联通。
- **Staging（`saas-staging`）**：
  - 同样由 `ci-build-and-push` → `deploy-helm` 链路完成部署，Deployment `1/1 Running`，日志中可见 Nacos 注册成功、实例心跳正常；
  - 公网 `https://api-staging.chekkk.com/api/backend-saas/healthz` 返回 301，说明域名/TLS/ALB 链路正常，仅存在额外重定向逻辑，后续可在网关/Ingress 收口。
- **Prod（`saas-prod`）**：
  - 当前 `saas-prod` 命名空间内已有 `deployment/backend-saas` 与对应 Service，Pod `1/1 Running`，日志中 Nacos 显示 `register finished` 与实例列表更新；
  - 已通过 `v1.0.35` tag 完成一次端到端发布演练：第一次运行时曾因 `deploy-helm-prod` 抢先触发而对应镜像尚未在 VECR 出现，导致 `context deadline exceeded` / `image not found`，随后确认为 CI/CD 竞态并通过“Registry manifest 轮询等待”修复；当前在相同 tag 上重跑时，`deploy-helm-prod` 能稳定绿灯并正确滚动到 `prod/backend-saas:v1.0.35`。
- **OpenAPI / YApi / LLM**：
  - 运行中进程不直接对外暴露 `/openapi.json`（该路径仅在 `openapi` Profile 下用于导出），因此 `https://api.chekkk.com/api/backend-saas/openapi.json` 返回 404 属预期行为；
  - 文档同步链路通过 `openapi.yml` + 组织级 `yapi-sync-reusable.yml` 完成，已多次成功将 `backend-saas` OpenAPI 导入 YApi，对应项目中可看到 LLM 富化过的接口描述。

六、排障速查
- CI/CD 相关：
  - `ci-build-and-push` 红灯且报 `UnexpectedBlobDigestException`：工作流已内置一次重试；若仍失败，多为上游镜像代理问题，可稍后重试或联系平台检查 Daocloud 镜像代理。
  - `deploy-helm` / `deploy-helm-prod` 失败且 Helm 报错：优先查看 `helm status backend-saas -n saas-<env>` 与 `kubectl -n saas-<env> get po,svc,ing`，确认 Release 状态与 Pod/Service 正常。
- 业务与健康相关：
  - Pod Running 但 ALB 4xx/5xx：在 Pod 内部访问 `http://127.0.0.1:12000/actuator/health` 与关键业务接口，确认是服务自身问题还是 Ingress/网关路由问题；
  - Nacos 中无实例：检查 env Secret（如 `backend-saas-env`）是否包含正确的 Nacos 配置与账号密码。

---

服务运维手册（backend-app｜Dev/Staging/Prod）

一、概览
- 技术栈：Django（容器端口 `8000`）。
- 命名空间：
  - dev：`app-dev`，Deployment/Service `backend-app`。
  - staging：`app-staging`。
  - prod：`app-prod`。
- 网关与入口（目标形态）：
  - 通过 `backend-gateway-saas` 统一暴露 `/api/backend-app/**`；
  - OpenAPI：`/api/backend-app/openapi.json`（dev/staging 已返回 200，prod 需按该路径对齐网关路由与后端配置）。

二、CI/CD 与版本策略
- 主工作流：`.github/workflows/build-and-deploy.yml`
  - 触发：
    - `dev` / `staging` 分支 `push`：构建并部署到 `app-dev` / `app-staging`；
    - `main` 上打 `vX.Y.Z` tag 并 push：构建并部署到 `app-prod`；
    - `workflow_dispatch`：运维可指定 `ref` 手动触发一次构建+部署。
  - 镜像仓库与标签：
    - dev：`chek-images-cn-beijing.cr.volces.com/dev/backend-app:dev`；
    - staging：`chek-images-cn-beijing.cr.volces.com/staging/backend-app:staging`；
    - prod：`chek-images-cn-beijing.cr.volces.com/prod/backend-app:vX.Y.Z` 与别名 `:prod`（仅 `v*` tag 触发），不再自动推 `sha-*`。
  - 部署逻辑：
    - 通过 Helm Chart `charts/backend-app` 执行 `helm upgrade --install backend-app`；
    - 部署前自动确保命名空间存在、镜像凭据 `vecr-auth` 与配置 Secret `backend-app-config` 就绪；
    - 在集群内临时起 curl Pod 调用 Nacos API，自动创建 `app-dev/app-staging/app-prod` 三个命名空间（若已存在则忽略）。

三、YApi 与文档
- `yapi-sync.yml`
  - 触发：`push` 到 `main/dev/staging`、定时 `0 2 * * *`、手动 `workflow_dispatch`。
  - 行为：本地生成 OpenAPI (`drf-spectacular` 优先) 到 `build/swagger/openapi.json` → 上传 artifact `swagger` → 调用组织级 `yapi-sync-reusable.yml`，根据分支选择 `YAPI_TOKEN_DEV/STG/PROD` 导入对应 YApi 项目。
  - 当前设置 `enable_llm: "false"`，不启用 LLM 自动描述，后续如需开启需先确认 schema 为合法 JSON。
- `yapi-only.yml`
  - 手动工具型工作流：在 CI 外需要重新导入文档时使用，直接在 job 内生成 swagger 并导入 YApi。

四、健康检查与联通性
- 集群内检查：
  - 部署：`kubectl -n app-<env> rollout status deploy/backend-app`;
  - Service：`kubectl -n app-<env> get svc backend-app -o wide`（`port: 80 → targetPort: 8000`）。
- In-cluster smoke（与 CI 相同逻辑）：
  - 访问 Service：
    ```bash
    kubectl -n app-<env> run svc-smoke --image=docker.m.daocloud.io/curlimages/curl:8.10.1 --restart=Never -- \
      sh -lc 'echo svc check; curl -sS http://backend-app.app-<env>.svc.cluster.local/ -m 10 -o /dev/null -w "%{http_code}\n"' || true
    ```
  - 访问 Ingress（通过网关域名）：
    ```bash
    kubectl -n app-<env> run ing-smoke --image=docker.m.daocloud.io/curlimages/curl:8.10.1 --restart=Never -- \
      sh -lc 'echo ingress check; curl -sS http://app-<env>.chekkk.com/ -m 10 -o /dev/null -w "%{http_code}\n"' || true
    ```
- 外部检查（dev/staging）：
  - `curl -sk -o /dev/null -w '%{http_code}\n' http://app-dev.chekkk.com/api/backend-app/openapi.json` → 200；
  - `curl -sk -o /dev/null -w '%{http_code}\n' http://app-staging.chekkk.com/api/backend-app/openapi.json` → 200；
  - prod：对外入口统一通过 API 网关域名暴露，期望 `curl -sk -o /dev/null -w '%{http_code}\n' https://api.chekkk.com/api/backend-app/openapi.json` → 200；如为 404/403，优先检查 `backend-gateway-saas` 的路由配置与 `SECURE_IGNORE_URLS` 白名单，以及 Ingress 是否仍存在多条规则抢占 `api.chekkk.com`。

五、网关路由与 Ingress（prod）
- 网关路由来源：
  - 生产环境下，`backend-app` 相关路由（`/api/backend-app/**` 与 `/api/backend-app/openapi.json`）仅由 `backend-gateway-saas` 进程内的 `SPRING_APPLICATION_JSON` 提供，配置源头是 ops-bootstrap 中的 `templates/gateway/backend-app-route.yml` + Helm values；
  - 禁止在 K8s 环境继续使用 `env.SPRING_CLOUD_GATEWAY_ROUTES_*` 或业务仓 `application.yml` 作为 route 来源；历史上如仍有 env 方式注入的 route，迁移至 JSON 后应统一删除。
- ALB / Ingress 形态（api.chekkk.com）：
  - `miker-prod` 命名空间下，`api-gateway-https-biz` 是 `api.chekkk.com:443` 的唯一业务入口，负责 `/api/backend-app/**` 与其它业务 API 的转发；
  - `api-gateway-http-osm` 仅占用 `api.chekkk.com:80`，将所有 HTTP 请求重定向至 HTTPS；
  - backend-app 自身在 `app-prod` 命名空间不再暴露 `api.chekkk.com` 域名的 Ingress，避免与 API 网关的规则冲突。

六、Nacos 与配置
- 后端通过 `backend-app-config` Secret 注入：
  - `NACOS_SERVER`、`NACOS_NAMESPACE`、`NACOS_USERNAME`、`NACOS_PASSWORD` 等关键字段；
  - CI 会在每次部署前通过 curl Pod 尝试创建对应 namespace。
- 运维验证：
  - 登录平台 Nacos 控制台，在 `app-dev/app-staging/app-prod` 命名空间下查找 `backend-app` 服务，确认实例列表与健康状态；
  - 若 CI 中 Nacos smoke 日志只显示 `{"count":0,"doms":[]}`，说明连通性正常但不一定有实例，仍以控制台信息为准。

七、飞书通知
- 当 `build-and-deploy` 在 prod 成功完成部署和 smoke 后，会通过 Feishu IM API 向 `FEISHU_CHAT_ID_PROD` 发送文本消息，包含：
  - 仓库名、分支/tag、Commit SHA、Actions 运行链接；
  - 消息结构已按最新接口要求封装为标准 JSON，出现 `code=9499` 时可拷贝 `log_id` 到 Feishu 控制台查看详情。

八、分支与工作流治理
- 长期分支：`main`（prod）、`staging`、`dev`；其它 feature 分支在合并后应清理。
- 工作流：
  - 主链路：`build-and-deploy.yml`、`yapi-sync.yml`；
  - 工具与排障：`yapi-only.yml`、`nacos-and-smoke.yml`、`verify-alb-mapping.yml`；
  - 历史调试工作流 `smoke-manual.yml`、`staging-smoke.yml` 已从各分支删除，避免误触发无效路径。

---

服务运维手册（osm-gateway｜Prod）

一、概览
- 技术栈：FastAPI + 自定义 Nacos 客户端，容器端口 `8090`。
- K8s（prod）：
  - Helm Release：`osm-gateway`（命名空间 `miker-prod`，Chart 位于业务仓 `helm/osm-gateway`，由 GitHub Actions 直接 `helm upgrade --install`）。
  - Service：由 Helm 生成 `osm-gateway`，`type: ClusterIP`，`port: 80 → targetPort: 8090`（历史手写的 `k8s/prod/service.yaml` 已删除，避免双轨配置）。
- 网关与入口：
  - `backend-gateway-saas` 将 `/api/osm-gateway/**` 前缀路由到 `osm-gateway.miker-prod.svc.cluster.local:80`；
  - 对外健康检查路径：`https://api.chekkk.com/api/osm-gateway/healthz`；
  - 对外 OpenAPI 路径：`https://api.chekkk.com/api/osm-gateway/openapi.json`（网关内部 `SetPath=/openapi.json`）。

二、标准工作流
- `ci.yml`（主 CI）
  - 触发：`push` 到 `main/dev/staging`、PR、手动 `workflow_dispatch`。
  - 行为：运行 gitleaks + Trivy 扫描 → 使用 Buildx 构建镜像并推送到 `chek-images-cn-beijing.cr.volces.com/prod/osm-gateway:sha-<short>`（必要时附加 `dev-latest/stg-latest` 别名）→ main 成功后自动回写 Helm `values.yaml` 中的 `image.tag`。
- `deploy.yml`（prod 部署）
  - 触发：`push main` 或手动 `workflow_dispatch`。
  - 行为：使用组织级 PAT 拉取 `ops-bootstrap`，加载 kubeconfig 后，对 `miker-prod` 命名空间执行 `helm upgrade --install osm-gateway ops-bootstrap/templates/helm`，并按当前 commit 号拼出 `sha-<short>` 镜像 tag。
  - 容错策略：若 `helm upgrade --install` 返回 `release: already exists` 等错误，会自动调用 `helm status osm-gateway -n miker-prod` 检查 Release：
    - 若 Release 状态为 `deployed`，本次部署视为成功，仅在日志中记录；
    - 若 Release 不存在或状态异常，则保持流水线失败，方便运维介入排查。
- `yapi-sync.yml`（YApi + LLM 富化）
  - 触发：`push main`、手动 `workflow_dispatch`、定时 `0 2 * * *`。
  - 行为：将仓库根目录的 `openapi.json` 拷贝到 `build/swagger/swagger.json`，作为 artifact `swagger-json` 上传；随后复用组织级 `yapi-sync-reusable.yml`，从 artifact 读取 swagger，并根据 `YAPI_BASE` + `YAPI_TOKEN_PROD` 将接口导入 `osm-gateway` 的 YApi 项目，默认开启 LLM description 富化。

三、日常发布与回滚
- 正常发布（prod）：
  1) 合并到 `main`；
  2) 等待 `ci.yml` → `deploy.yml` → `yapi-sync.yml` 三条工作流全部成功；
  3) 在集群中确认：`kubectl -n miker-prod rollout status deploy/osm-gateway`，以及 Service/Endpoints 正常。
- 紧急回滚：
  - 在 Actions 中找到上一次绿色 `ci.yml`，复制对应 `sha-<short>`；
  - 使用 `kubectl` 临时覆盖镜像：
    ```bash
    kubectl -n miker-prod set image deploy/osm-gateway app=chek-images-cn-beijing.cr.volces.com/prod/osm-gateway:sha-<short>
    kubectl -n miker-prod rollout status deploy/osm-gateway
    ```
  - 事后应将该 tag 写回 Helm `values.yaml`（或重新跑一次带该 tag 的 CI），保证 Git/Helm 与线上状态一致。

四、健康检查与排障
- 直连服务：
  - `kubectl -n miker-prod port-forward svc/osm-gateway 18090:80`
  - `curl -i http://127.0.0.1:18090/healthz`
  - `curl -i http://127.0.0.1:18090/openapi.json`
- 通过网关：
  - `curl --max-time 20 -i https://api.chekkk.com/api/osm-gateway/healthz`
  - `curl --max-time 20 -i https://api.chekkk.com/api/osm-gateway/openapi.json`
- 常见问题：
  - **503 / 无 Endpoints**：多见于镜像 tag 不存在或 CI 失败未更新 Helm。检查：
    - `kubectl -n miker-prod get deploy,svc,endpoints osm-gateway`
    - `kubectl -n miker-prod describe pod -l app.kubernetes.io/name=osm-gateway` 是否有 `ImagePullBackOff`
    - 对照 VECR 中是否存在对应 `sha-<short>` 镜像。
  - **Nacos 未注册或实例不健康**：
    - Pod 日志中是否有 `nacos_init_error` / `nacos_register_error` 类事件；
    - 确认运行时环境变量 `NACOS_SERVER_ADDR/NACOS_NAMESPACE/NACOS_USERNAME/NACOS_PASSWORD` 由 `nacos-cred` Secret 正确注入；
    - 在 Nacos 控制台中检查 `miker-prod` 命名空间下 `osm-gateway` 服务实例的心跳状态。
  - **OpenAPI 或健康检查经网关访问异常**：
    - 先在服务侧验证 `/healthz`、`/openapi.json` 是否 200；
    - 再确认 `backend-gateway-saas` 的路由已为 `/api/osm-gateway/**` 使用 `StripPrefix`，并为 `/api/osm-gateway/openapi.json` 添加 `SetPath=/openapi.json`，且相关路径在安全白名单中。

五、分支与工作流治理（针对 osm-gateway）
- 自动路径：只认 `ci.yml` / `deploy.yml` / `yapi-sync.yml` 三条工作流，其它工作流（`ci-build.yml`、`build-kaniko.yml`、`build-rsync.yml`）仅保留为手动工具，不会因 push 自动触发。
- 分支：日常只长期保留 `main` 及少量功能分支（`feat/*`、`fix/*`）。针对 CI/YApi 调试的纯基础设施分支（如 `chore/use-org-workflows`、`chore/yapi-sync-*`、`infra/ci-standard` 等）在对应改动合入 `main` 且验证稳定后，应在 1–2 周内统一删除，避免以后运维排查时误判“真实发布分支”。

---

服务运维手册（vehicle-model-service｜Dev/Prod）

一、概览
- 技术栈：FastAPI，容器端口 `4010`。
- K8s：
  - Deployment：`vehicle-model-service`（dev / miker-prod）。
  - Service：`vehicle-model-service`，`port: 80 → targetPort: 4010`，供网关与内部直连使用。
- 网关路径：
  - 业务：`/api/vms/**` → StripPrefix=2 → 下游真实路径。
  - 健康检查：`/api/vms/healthz` → `/actuator/health`。
  - OpenAPI：`/api/vms/openapi.json` → `/openapi.json`。

二、CI/CD 与工作流
- `ci.yml`
  - 负责构建镜像并推送 VECR（`prod/dev` 仓库，标签统一 `sha-<short>`，必要时附加 `dev-latest` 等别名）。
- `deploy.yml`
  - 调用 `chekdata/ops-bootstrap/.github/workflows/deploy-helm-call.yml@main`，参数：
    - `service: vehicle-model-service`、`namespace: miker-prod`、
      `container_port: 4010`、`svc_port: 80`、`target_port: 4010`、
      `extra_values_file: helm/prod.values.yaml`。
  - 仅当改动命中 `.github/workflows/deploy.yml` 或 `helm/**` 时自动触发 prod 发布；原来仓内的 `k8s/prod/service.yaml` 已删除，**prod 的 Deployment/Service 仅以 Helm Chart 为真源**。
- `deploy-dev.yml`
  - 触发：`dev` / `dev/**` 分支的 `push` / `pull_request` / `workflow_dispatch`。
  - 步骤：写入 kubeconfig → 创建 `dev` 命名空间 → 确保镜像凭据 `vecr-auth` 和 TOS 凭据 `tos-creds` →
    渲染并 apply `k8s/dev/deployment.yaml` / `service.yaml` → `rollout status` →
    在 Pod 内跑一次 TOS 只读自检（`boto3.list_objects_v2`）。
- `openapi-generate.yml`
  - 安装 Python 依赖，设置 `PYTHONPATH=.`、`MONGO_URI=mongodb://localhost:27017/dummy` 后运行 `scripts/gen_openapi.py`，生成 `openapi.json` 并用 `create-pull-request` 创建更新 PR。
- `yapi-sync.yml`
  - 不再从外部 `https://api.chekkk.com/api/vms/openapi.json` 拉取文档，而是：
    - 在 CI 中本地生成 OpenAPI（同样设置 `PYTHONPATH=.` 和 dummy `MONGO_URI`），写入 `swagger.json`；
    - 作为 artifact 上传；
    - job 级调用组织级 `chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main`，通过 `artifact-name: swagger-artifact` 读取并导入 YApi；
    - 根据分支选择 `YAPI_TOKEN / YAPI_TOKEN_STG / YAPI_TOKEN_DEV`，默认 `enable_llm: true` 通过组织级 `LLM_API_KEY` 开启 LLM 富化。

三、健康检查与排障
- 直连 Service：
  - `curl -sS -o /dev/null -w '%{http_code}\n' http://vehicle-model-service:80/healthz`
  - `curl -sS -o /dev/null -w '%{http_code}\n' http://vehicle-model-service:80/actuator/health`
  - `curl -sS -o /dev/null -w '%{http_code}\n' http://vehicle-model-service:80/api/vms/healthz`
- 通过网关：
  - `curl --max-time 10 -i https://api.chekkk.com/api/vms/healthz`
  - `curl --max-time 20 -i https://api.chekkk.com/api/vms/openapi.json`（目前返回 401，属网关鉴权策略预期行为，**不再作为 CI/YApi 的获取入口**）
- 常见问题：
  - 网关 503/404：多数是服务本身未暴露对应健康接口或 OpenAPI 路径。`vehicle-model-service` 已内置 `/healthz`、`/actuator/health`、`/api/vms/healthz`，后续新服务必须在代码层显式补齐。
  - YApi 同步失败：优先检查 `yapi-sync.yml` 是否成功生成并上传 `swagger.json` artifact，以及 job 级 `uses` 调用组织工作流时传入了正确的 `YAPI_BASE/YAPI_TOKEN*`。

---

服务运维手册（backend-miker-offline｜Prod）

一、概览
- 技术栈：FastAPI，容器端口（当前）`8088`。
- K8s：
  - Deployment / Service：`backend-miker-offline`（命名空间 `miker-prod`），prod 已由 Helm Chart + ArgoCD Application `backend-miker-offline-prod` 管理，原 `k8s/prod/*.yaml` 已删除。
- 网关路径：
  - 业务：`/api/offline/**`。
  - OpenAPI：`/api/offline/openapi.json` → `/openapi.json`（单一路由，绕过复杂过滤器）。

二、CI/CD 与工作流（当前形态）
- `ci.yml`
  - 构建 `backend-miker-offline` 镜像并推送 VECR（dev/prod 项目，标签 `sha-<short>`）。
- `deploy.yml`
  - 使用 `backend-miker-offline` 仓库内的 Helm Chart（`charts/backend-miker-offline`）与 ops-bootstrap 中的 ArgoCD Application：
    - App：`ops-bootstrap/apps/argocd/backend-miker-offline-prod.yaml` 指向 `repo=chekdata/backend-miker-offline.git`、`path=charts/backend-miker-offline`、`namespace=miker-prod`；
    - `deploy.yml` 负责在 prod 变更时触发 `helm upgrade --install` 并由 ArgoCD 接管后续同步；
    - 历史手写 `k8s/prod/*.yaml` 已在仓库中移除，**避免出现“双真源”（手写 YAML 与 Helm 同时存在）**。
- `openapi-generate.yml`
  - 在 CI 中安装依赖，设置 `PYTHONPATH=.` 后运行 `scripts/gen_openapi.py` 生成 `openapi.json`，并通过 `create-pull-request` 更新仓库。
- `yapi-sync.yml`
  - 采用与 `vehicle-model-service` 相同的“本地生成 + artifact + 组织级工作流”方案：
    - `build-openapi` job 内设置 `PYTHONUNBUFFERED=1`、`PYTHONPATH=.`，运行 `scripts/gen_openapi.py`（或等价脚本）生成 `openapi.json`；
    - 上传为名为 `openapi-build` 的 artifact；
    - 下游 job 调用 `chekdata/.github/.github/workflows/yapi-sync-reusable.yml@main`，以 `artifact-name` 方式加载并导入 YApi，支持 LLM 富化；
    - 全程不依赖网关外网访问 `https://api.chekkk.com/api/offline/openapi.json`，排除了网关 4xx/5xx 对文档同步的影响。

三、健康检查与 OpenAPI 排障
- 直连后端：
  - `kubectl -n miker-prod port-forward svc/backend-miker-offline 18080:80`
  - `curl -i http://127.0.0.1:18080/healthz`
  - `curl -i http://127.0.0.1:18080/openapi.json`
- 通过网关：
  - `curl --max-time 20 -i https://api.chekkk.com/api/offline/openapi.json`
- 常见问题：
  - 403/503：历史问题主要出在网关 ResponseBodyFilter 对大体积 OpenAPI JSON 的处理；现在通过专门路由和白名单绕过。后续新服务暴露 OpenAPI 时，统一使用 `/api/<svc>/openapi.json` + `SetPath=/openapi.json`，并在 `SECURE_IGNORE_URLS` 中放行。

服务运维手册（ALB / Ingress / 前端与 YApi 运行手册）

一、目标与范围
- 统一外部入口通过 ALB 承载，按环境（dev/staging/prod）独立实例、独立 IngressClass。
- 80 仅用作 HTTP→HTTPS 301，业务路径集中在 443。
- GitOps 为单一真源：Ingress/Service/证书/工作流全部入库，禁止控制台手工改规则（仅应急）。

二、常用路径与凭据位置
- Kubeconfig（只读/运维）：`tools/kube-conf/chek-<env>-k8s-public-kube.conf`
- GitOps 仓：`ops-bootstrap`（当前仓库）
- 预签名校验与清理脚本：`tmp/alb_verify_hosts.py`、`tmp/sg_scan.py`、`tmp/sg_delete_empty.py`

三、日常巡检（每日）
1) 预签名校验 ALB（80/443）：
   ```bash
   source tmp/venv/bin/activate
   export VOLC_AK=<RO_AK> VOLC_SK=<RO_SK> REGION=cn-beijing
   export ALB_PROD=<alb-prod-id> ALB_STAGING=<alb-stg-id> ALB_DEV=<alb-dev-id>
   export HOSTS=yapi.chekkk.com,app.chekkk.com,www.chekkk.com,app-staging.chekkk.com,www-staging.chekkk.com,app-dev.chekkk.com,www-dev.chekkk.com
   python tmp/alb_verify_hosts.py > /tmp/alb_verify.json
   ```
   - 期望：80 下无业务 ServerGroup（仅 redirect）；443 命中业务目标组且健康>0。
2) Ingress/Service 健康：
   ```bash
   KUBECONFIG=tools/kube-conf/chek-<env>-k8s-public-kube.conf \
     kubectl -n <ns> get ingress,svc,endpoints
   ```
   - 期望：Endpoints Ready 数量>0，Ingress 不处于 Deleting。

四、问题定位与修复
A. 80 返回 200 非 301
- 现象：HTTP 未经 ALB，或 80 存在业务规则。
- 排查：
  - DNS：确认域名为 CNAME → 对应环境 ALB 域名（不要 A 到 EIP）。
  - Ingress：80 侧 Ingress 仅有 `/ -> ssl-redirect:80`，注解包含 `ingress.vke.volcengine.com/actions.ssl-redirect` 与 `listen-ports: [{"HTTP":80}]`。
  - 预签名：确认 80 下无业务 ServerGroup。
- 修复：更新 Ingress（GitOps），合并后等待控制器收敛；必要时临时删除 80 下的业务规则（仅应急，随后回归 GitOps）。

B. 443 规则竞争/落入默认后端
- 现象：返回控制台默认页/404。
- 排查：是否缺少 `group.name/group.order` 或未显式 `listen-ports: [{"HTTPS":443}]`。
- 修复：在 443 的 Ingress 上补齐分组与优先级；单域尽量单 Ingress，确保 host-header 明确命中。

C. Admission 报错 “svc [...] has no port [0]”
- 原因：Ingress backend 使用 `port.name` 或 `use-annotation`。
- 修复：将后端端口改为数值 `number: 80`；`ssl-redirect` Service 使用 `port:80/targetPort:80`。

D. 503/502
- 现象：目标组无健康实例或后端拒绝。
- 排查：
  - Service selector → Endpoints；
  - Service `targetPort` 与容器端口一致；
  - 节点安全组是否放通 ALB 安全组到 NodePort 段；
  - 控制器日志是否提示 “no endpoints available”。
- 修复：修正 selector/端口，放通 SG；必要时重建 Ingress 以刷新目标组。

E. yapi 登录/302 到 `/feishu-sso/`
- 解释：应用内 SSO 插件行为（非 ALB）；`/user/login` 302 到 SSO 回调属预期。
- 健康检查：可用 `/user/login`（成功码 200,301,302）。
- 根路径：如监听 3000，使用 `yapi-web`（NodePort 80→3000）承载 `/`。

五、例行清理（每周）
1) 扫描空/不健康 ServerGroups（仅报告）：
   ```bash
   source tmp/venv/bin/activate
   export VE_REGION=cn-beijing VE_AK=<AK> VE_SK=<SK> ALB_PROD=<id>
   python tmp/sg_scan.py > /tmp/sg_candidates.json
   ```
2) 删除空目标组（谨慎，仅当未被规则引用）：
   ```bash
   export CANDIDATES_JSON="$(cat /tmp/sg_candidates.json)"
   python tmp/sg_delete_empty.py
   ```
3) 准则：被规则引用/健康>0 的目标组禁止删除；如需替换，先在 GitOps 中切换 Ingress/Service，待控制器生成替代组后再清理。

六、CI/CD 与 GitOps（前端）
- 应用仓工作流：构建并推 VECR → `repository_dispatch` 通知 GitOps 仓。
- GitOps 仓工作流：接收事件 → 运行 `tmp/update_manifests.py` 更新各环境镜像 → 创建带 `automerge` 标签的 PR → 自动合并 → Argo CD 同步。
- 必需 Secrets：
  - 应用仓：`VE_REGISTRY/VE_USERNAME/VE_PASSWORD/GITOPS_PAT`
  - GitOps 仓：`GITOPS_PAT`（用于创建/合并 PR）
- 标签与版本策略（必须遵守）
  - 版本来源：前端仓以 `package.json.version` 为唯一版本源；生产发布以 Git tag `vX.Y.Z` 驱动。
  - 标签规则：`dev → :dev`、`staging → :staging`；`main` 不推镜像；当推送 `v*` 标签时产出 `:vX.Y.Z`，且仅当该 tag 对应提交属于 `main` 时额外产出 `:prod`（流水线内已做 ancestry 校验，非 `main` 的 tag 不会产生 `:prod`）。
  - 开发者发版（Node 推荐）：`git checkout main && git pull && npm version {patch|minor|major} && git push --follow-tags`；或手动维护 `package.json.version` 后 `git tag vX.Y.Z && git push origin vX.Y.Z`。

七、DNS 规范（外部 DNS 如腾讯云）
- 建议使用 CNAME 指向 ALB 域名；TTL 60–300。
- 禁止 A 到 EIP（EIP 可能漂移；CNAME 更稳）。
- 分环境域名：`<svc>-dev|staging.chekkk.com`、生产 `app/www/yapi.chekkk.com`。

八、应急操作（尽量 24–48 小时内回归 GitOps）
- 临时在控制台删除 80 下的业务转发或修正 443 的 host 规则，以恢复服务；完成后立即将变更写回 Ingress，并清理手工规则。

九、附录：命令速查
```bash
# 应用最新 Ingress/Service（dev）
KUBECONFIG=tools/kube-conf/chek-dev-k8s-public-kube.conf \
kubectl apply -f frontend-app-dev -f frontend-saas-dev

# 预签名校验（prod/app/www/yapi）
source tmp/venv/bin/activate
export VOLC_AK=<RO_AK> VOLC_SK=<RO_SK> REGION=cn-beijing ALB_PROD=<alb-id>
export HOSTS=app.chekkk.com,www.chekkk.com,yapi.chekkk.com
python tmp/alb_verify_hosts.py | tee /tmp/alb_verify_prod.json
```

十、飞书通知与自动拉群（组织级复用）
- 入口工作流：组织仓 `.github/workflows/org-feishu-notify-sync.yml`
  - 触发：`repository_dispatch`（type: `feishu-ci-notify`）或手动 `workflow_dispatch`
  - Jobs：
    - `sync`：部门枚举 → `nickname == GitHub 登录名（小写）` 命中 → 以 `open_id` 将成员加入 `chat_id`
      - 产物：`feishu_diag.json`、`feishu_nicknames.json`、`feishu_sync_summary.json`
    - `notify`：调用 `reusable-feishu-ci-notify.yml` 发送卡片
- 仓库侧接入：在应用仓配置 `FEISHU_CHAT_ID_PROD`、`GITOPS_PAT`，并添加 `notify-dispatch.yml`（监听主 CI/CD 成功后派发）
- 常见故障：
  - 未入群但成功：多为 `chat_id` 为空 → 检查应用仓 `FEISHU_CHAT_ID_PROD`
  - 命中人数少：成员飞书昵称与 GitHub 登录名不一致 → 统一昵称为登录名（小写）
  - 未收到通知：确认组织级存在 `notify` 作业，且应用仓已派发到 `.github`

十一、GitHub 清理 SOP（保守）
- 工作流（Workflows）
  - 保留：主发布（如 `cd-on-merge`）、组织派发（`notify-dispatch`）、飞书通知（`feishu-notify`）
  - 候选禁用：90–120 天无运行或已被替换的工作流；先“禁用”观察，再考虑删除 YAML
- 分支（Branches）
  - 保留：`main/master`、`develop`、`dev/staging` 等主分支
  - 候选删除：前缀 `fix-/feat-/ci-/deploy-/temp-/test-/hotfix-/chore-/revert-`，且
    - 非开放 PR 的 head；且最近提交超过 7–14 天；且已不在发布计划
  - 删除顺序：先在仓库“分支保护”放开限制 → 删除 → 恢复保护
- PR（Pull Requests）
  - 仅关闭部署脚手架/临时性 PR 且无后续计划的；保留功能/修复类
  - 关闭前在描述中补上原因与链接，便于审计

- 实战记录（vehicle-model-service / backend-miker-offline）：
  - 两个微服务仓库已按本 SOP 完成一次清理：
    - `vehicle-model-service`：仅保留标准 CI/CD 与安全相关工作流（`ci.yml/deploy.yml/deploy-dev.yml/openapi-generate.yml/yapi-sync.yml/vms-*.yml/release-sbom.yml/security-gitleaks-bootstrap.yml/vms-release-please.yml`），删除早期调试用的自定义 build/YApi 手工同步工作流；远端仅保留 `main` 与 `release-please--branches--main` 两个长期分支；
    - `backend-miker-offline`：仅保留 `ci.yml/openapi-generate.yml/yapi-sync.yml/bmo-*.yml/bmo-release-please.yml/release-sbom.yml/security-gitleaks-bootstrap.yml`，删除 `ci-build.yml/yapi-sync-manual.yml/update-k8s-image.yml` 等历史工作流；远端仅保留 `main/dev`、业务功能分支和 `release-please--branches--main`，历史 `chore/*`、`fix/ci-*`、`infra/ci-*`、`release/image-sha-*` 分支已删除；
  - 对于未来新服务，推荐参照这两个仓的做法：**所有部署相关实验尽量在现有工作流内通过参数开关实现，临时分支用完后在一两周内删除，避免“真正发布路径”被大量历史分支/工作流淹没。**

十二、VECR 镜像清理 SOP（前置条件）
- 当前镜像“免密拉取”不等于“匿名可列举/删除”，列举/删除需具备权限：
  - 任选其一：机器人账号 token（含 delete）/ OpenAPI AKSK（镜像仓库读删）/ 开启匿名列表权限
- 清理策略（具备权限后）：
  - 标签匹配：`tmp|temp|test|ci|pr|dev|staging` 等临时/环境标签
  - 过期策略：非活跃 > 30/60 天；同一基线仅保留最近 N 个版本
  - 执行方式：先生成计划（dry-run 报表）→ 人工确认 → 批量删除
- 审计与回溯：输出“仓库/标签/创建时间/最后拉取时间/删除人/时间”到报表归档

- 当前限制与临时做法：
  - 实战中发现，`chek-images-cn-beijing.cr.volces.com` 在当前账号下无法通过 Harbor 风格 API（`/api/v2.0/projects/...`）可靠列举项目/标签（返回 404 数字）；这说明接口权限或产品形态受限，**暂不具备安全自动化删除镜像的条件**；
  - 在这种情况下，禁止在脚本中直接调用 Registry v2 的 `DELETE /v2/<name>/manifests/<digest>` 等接口批量删镜像，以免误删仍被 K8s/ArgoCD 引用的 tag；
  - 建议做法：
    - 先从集群与 GitOps 侧盘点“在用/可回滚” tag：包括各环境 Deployment、Helm values、ArgoCD Application 中引用的 `sha-<short>`；
    - 再在 VECR 控制台分别打开 `prod/dev/<service>` 仓库（如 `prod/vehicle-model-service`、`dev/backend-miker-offline`），**只删除那些确认未被任何 Deployment/ArgoCD/values 引用的旧 `sha-*` 标签**，并保留最近若干个 sha 以及所有版本标签（如 `v1.0.0`）；
    - 删除动作请参照本 SOP 的审计要求，至少记录“仓库/标签/删除时间/执行人”，必要时可在 `tmp/` 下保留一次性清单或截图归档。

十三、DNS 运维规范
- 权威 DNS：放在“腾讯云”，火山引擎侧不再新建 A 记录
- 外部解析：一律 CNAME 到 ALB 域名；TTL 建议 300（60–300 视情况）
- 变更窗口：在低峰变更；变更后执行外网与集群内双路径验证

十四、夜间校验与每周清理（作业落地）
- Nightly 验证（组织级）
  - 校验监听器 80/443、规则 host-header 命中、ServerGroup 健康
  - 产物作为告警输入；失败需创建问题单并在 24h 内处理
- Weekly 清理
  - 空/不健康 ServerGroup 定位与清理（遵循“未被规则引用/无健康实例”原则）
  - 配合 GitOps 变更：先切流再删组，避免流量中断

